{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c1121-4e1a-47a9-88e6-d3630be2a16a",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f1a4b7-28eb-41f2-ab32-5cd4ad731bed",
   "metadata": {},
   "source": [
    "Transductive - you have a single graph (like Cora) you split some nodes (and not graphs) into train/val/test training sets. While you're training you'll be using only the labels from your training nodes. BUT. During the forward prop, by the nature of how spatial GNNs work, you'll be aggregating the feature vectors from your neighbors and some of them may belong to val or even test sets! The main point is - you ARE NOT using their label information but you ARE using the structural information and their features.\n",
    "\n",
    "Inductive - you're probably much more familiar with this one if you come from the computer vision or NLP background. You have a set of training graphs, a separate set of val graphs and of course a separate set of test graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1841ec8-a984-49d7-a978-78892f98e8b6",
   "metadata": {},
   "source": [
    "## 1. https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c239446-da79-4727-8bee-60e375ed0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "#import seaborn as sns\n",
    "#sns.reset_orig()\n",
    "#sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf2a69-6616-4dad-b1f5-943a16ee60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The \n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha - Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        \n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "        \n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        \n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # One per head\n",
    "        \n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "        \n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "        \n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "        \n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix \n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        \n",
    "        edges = adj_matrix.nonzero(as_tuple=False) # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
    "        \n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "        \n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a) \n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "        \n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "        \n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "        \n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "        \n",
    "        return node_feats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec9ace-6edb-4f1c-8135-32fd896fdf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e6d71-8577-4643-b2bd-51a98692d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = GATLayer(2, 2, num_heads=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43733e2b-8285-4790-8301-3283c22762e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4defc3b6-6fb6-4652-b5c2-1cdfdfbd07b3",
   "metadata": {},
   "source": [
    "## 2 Basic GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9924da8-9a70-4d39-9959-3a2ddbf97600",
   "metadata": {},
   "source": [
    "### 2.1 https://towardsdatascience.com/graph-attention-networks-under-the-hood-3bd70dc7a87 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2c9a0-c123-48bc-bfdc-f0b0c085f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z):\n",
    "    return np.where(z > 0, z, z * 0.01)\n",
    "\n",
    "def softmax(z):\n",
    "    if len(z.shape) > 1:\n",
    "        # Softmax for matrix\n",
    "        max_matrix = np.max(z, axis=0)\n",
    "        stable_z = z - max_matrix\n",
    "        e = np.exp(stable_z)\n",
    "        a = e / np.sum(e, axis=0, keepdims=True)\n",
    "    else:\n",
    "        # Softmax for vector\n",
    "        vector_max_value = np.max(z)\n",
    "        a = (np.exp(z - vector_max_value)) / sum(np.exp(z - vector_max_value))\n",
    "\n",
    "    assert a.shape == z.shape\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f59fa-61aa-47ac-bdee-fde634a3e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n----- One-hot vector representation of nodes. Shape(n,n)\\n')\n",
    "X = np.eye(5, 5)\n",
    "n = X.shape[0]\n",
    "np.random.shuffle(X)\n",
    "print(X)\n",
    "\n",
    "print('\\n\\n----- Embedding dimension\\n')\n",
    "emb = 3\n",
    "print(emb)\n",
    "\n",
    "print('\\n\\n----- Weight Matrix. Shape(emb, n)\\n')\n",
    "W = np.random.uniform(-np.sqrt(1. / emb), np.sqrt(1. / emb), (emb, n))\n",
    "print(W)\n",
    "\n",
    "print('\\n\\n----- Adjacency Matrix (undirected graph). Shape(n,n)\\n')\n",
    "A = np.random.randint(2, size=(n, n))\n",
    "np.fill_diagonal(A, 1)  \n",
    "A = (A + A.T)\n",
    "A[A > 1] = 1\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a6d55-7c68-4ac8-b15f-441bf4e6e374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d989ca-60b0-4722-aec6-fc585093c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equation (1)\n",
    "print('\\n\\n----- Linear Transformation. Shape(n, emb)\\n')\n",
    "z1 = X.dot(W.T)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8838221-f2c3-4b77-a148-54ce49f52283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equation (2) - First part\n",
    "print('\\n\\n----- Concat hidden features to represent edges. Shape(len(emb.concat(emb)), number of edges)\\n')\n",
    "edge_coords = np.where(A==1)\n",
    "h_src_nodes = z1[edge_coords[0]]\n",
    "h_dst_nodes = z1[edge_coords[1]]\n",
    "z2 = np.concatenate((h_src_nodes, h_dst_nodes), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6711d6ca-f1a6-4763-90d9-c16dc779d106",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# equation (2) - Second part\n",
    "print('\\n\\n----- Attention coefficients. Shape(1, len(emb.concat(emb)))\\n')\n",
    "att = np.random.rand(1, z2.shape[1])\n",
    "print(att)\n",
    "\n",
    "print('\\n\\n----- Edge representations combined with the attention coefficients. Shape(1, number of edges)\\n')\n",
    "z2_att = z2.dot(att.T)\n",
    "print(z2_att)\n",
    "\n",
    "print('\\n\\n----- Leaky Relu. Shape(1, number of edges)')\n",
    "e = leaky_relu(z2_att)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414b74c-b8cc-42c7-8402-7eae50fceb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21e540-6113-4acd-b389-b57679566f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# equation (3)\n",
    "print('\\n\\n----- Edge scores as matrix. Shape(n,n)\\n')\n",
    "e_matr = np.zeros(A.shape)\n",
    "e_matr[edge_coords[0], edge_coords[1]] = e.reshape(-1,)\n",
    "print(e_matr)\n",
    "\n",
    "print('\\n\\n----- For each node, normalize the edge (or neighbor) contributions using softmax\\n')\n",
    "alpha0 = softmax(e_matr[:,0][e_matr[:,0] != 0]) \n",
    "alpha1 = softmax(e_matr[:,1][e_matr[:,1] != 0])\n",
    "alpha2 = softmax(e_matr[:,2][e_matr[:,2] != 0])\n",
    "alpha3 = softmax(e_matr[:,3][e_matr[:,3] != 0])\n",
    "alpha4 = softmax(e_matr[:,4][e_matr[:,4] != 0])\n",
    "alpha = np.concatenate((alpha0, alpha1, alpha2, alpha3, alpha4))\n",
    "print(alpha)\n",
    "\n",
    "print('\\n\\n----- Normalized edge score matrix. Shape(n,n)\\n')\n",
    "A_scaled = np.zeros(A.shape)\n",
    "A_scaled[edge_coords[0], edge_coords[1]] = alpha.reshape(-1,)\n",
    "print(A_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a68105-817e-4a89-a7a8-7aafeae27568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equation (4)\n",
    "print('\\n\\nNeighborhood aggregation (GCN) scaled with attention scores (GAT). Shape(n, emb)\\n')\n",
    "ND_GAT = A_scaled.dot(z1)\n",
    "print(ND_GAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2ed04-a0ef-44b8-b6c6-9a4221d31463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14547718-6291-4c3c-a4be-593e88e89707",
   "metadata": {},
   "source": [
    "### 2.2 https://github.com/johncava/pytorch-GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba19a0-baee-471c-8066-fa4e45507eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0e264-a301-477b-9ddf-2fdde43b14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(graph):\n",
    "    neighbors = []\n",
    "    for node in graph:\n",
    "        node_neighbors = []\n",
    "        for index, val in enumerate(node):\n",
    "            if val == 1:\n",
    "                node_neighbors.append(index)\n",
    "        neighbors.append(node_neighbors)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420bf43-d03d-41b0-afb0-a649e5528a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy graph structure as a symmetric graph\n",
    "graph = [[1,0,0,0,0,0,0,0,0,0],\n",
    "         [0,1,0,0,0,0,0,0,0,0],\n",
    "         [1,0,1,0,0,0,0,0,0,0],\n",
    "         [0,1,1,1,0,0,0,0,0,0],\n",
    "         [1,1,0,0,1,0,0,0,0,0],\n",
    "         [0,0,1,0,0,1,0,0,0,0],\n",
    "         [1,1,0,0,0,0,1,0,0,0],\n",
    "         [0,0,1,0,1,1,0,1,0,0],\n",
    "         [1,0,0,1,0,1,0,0,1,0],\n",
    "         [0,0,0,1,0,0,1,0,0,1]]\n",
    "\n",
    "# Toy label\n",
    "label = [[1,1],[1,1],[1,1],[1,1],[1,1],[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
    "\n",
    "# Turn array into numpy array\n",
    "graph = np.array(graph)\n",
    "\n",
    "# Turn symmetric graph into an adjacency graph\n",
    "graph = graph + graph.T - np.eye(10)\n",
    "\n",
    "# Random feature matrix for the graph\n",
    "features = np.random.rand(10,10) * 10\n",
    "\n",
    "# Turn features into pytorch Variable\n",
    "features = Variable(torch.Tensor(features))\n",
    "\n",
    "# Turn label into pytorch Variable\n",
    "label = Variable(torch.Tensor(label))\n",
    "\n",
    "# Get neighbors for attention model\n",
    "neighbors = get_neighbors(graph)\n",
    "\n",
    "# Define W_out which would be equal in this case to the number of features of the label dataset => 2\n",
    "W_out = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a016396-77c9-44ae-9d62-ef3245e47e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Attention Model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        # Linear Function that takes the features (h_i) and turns it into new features (new_h_i)\n",
    "        self.W = nn.Linear(10,W_out)\n",
    "        # Note: Attention Mechanism takes twice the output of Linear Function (W) because of the concatentation of Wh_i and Wh_j (Wh_i || Wh_j)\n",
    "        self.a = nn.Linear(2*W_out,1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # List to hold the new h_i values calculated from the attention mechanism\n",
    "        new_h_list = []\n",
    "        # Go through each node and perform attention in respect to its neighbors (which has been computed previously)\n",
    "        for primary_index,primary_node in enumerate(neighbors):\n",
    "            h = []\n",
    "            W_hjs = []\n",
    "            e = torch.Tensor([])\n",
    "            # Reference Equation (1),(3) : e_ij = a(Wh_i, Wh_j) = Leaky_Relu(attention(Wh_i, Wh_j)) => Neural_Network( Wh_i || Wh_j )\n",
    "            for neighbor in primary_node:\n",
    "                # Neighbor node features matrix multiplied with W. Also stored for future use when multiplying against alphas in line 75\n",
    "                W_hj = self.W(features[neighbor])\n",
    "                # Note: concatenation of e_ij into a single torch tensor such that there is one line to do F.softmax(e) in line 70\n",
    "                e = torch.cat((e,self.leaky_relu(self.a(torch.cat((self.W(x[primary_index]),W_hj))))))\n",
    "                W_hjs.append(W_hj)\n",
    "            # Softmax(e_ij) Reference: Equation (2)\n",
    "            a = F.softmax(e)\n",
    "            # Reference: Equation (4)\n",
    "            new_h = torch.Tensor([0.0]*W_out)\n",
    "            for a_ij, w_hj in zip(a,W_hjs):\n",
    "                new_h += a_ij * w_hj\n",
    "            new_h_list.append(F.leaky_relu(new_h))\n",
    "            ######################################\n",
    "        return torch.stack(new_h_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abce8ea-3fba-4eb7-b1f4-bf0093b7984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Attention Model\n",
    "attention = Attention()\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(attention.parameters(), lr=1e-3)\n",
    "max_iterations = 10\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    prediction = attention(features)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(prediction,label)\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90015579-714a-45ab-872a-57657b531d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff7ed268-398b-447b-bb74-5ab32c569f9c",
   "metadata": {},
   "source": [
    "## 3. https://dsgiitr.com/blogs/gat/\n",
    "### Implementing GAT Layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed0bc40-8a1a-4b50-9f49-642e21818e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x211f54821b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(2020) # seed for reproducible numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bae8efe-64e0-4f46-b4f8-e4c73204ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple PyTorch Implementation of the Graph Attention layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.dropout       = dropout        # drop prob = 0.6\n",
    "        self.in_features   = in_features    # \n",
    "        self.out_features  = out_features   # \n",
    "        self.alpha         = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n",
    "        self.concat        = concat         # conacat = True for all layers except the output layer.\n",
    "\n",
    "        # Xavier Initialization of Weights\n",
    "        # Alternatively use weights_init to apply weights of choice \n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # Linear Transformation\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        # Attention Mechanism\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        # Masked Attention\n",
    "        zero_vec  = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        \n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime   = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed87ed97-138a-4692-96e2-775bbca0aaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes in Cora: 7\n",
      "Number of Node Features in Cora: 1433\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "name_data = 'Cora'\n",
    "dataset = Planetoid(root= 'data/' + name_data, name = name_data)\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "\n",
    "print(f\"Number of Classes in {name_data}:\", dataset.num_classes)\n",
    "print(f\"Number of Node Features in {name_data}:\", dataset.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755e5be8-42f1-4838-ac32-b45efc068e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "        \n",
    "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,\n",
    "                             heads=self.out_head, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Dropout before the GAT layer is used to avoid overfitting in small datasets like Cora.\n",
    "        # One can skip them if the dataset is sufficiently large.\n",
    "        \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "818626d9-000f-4315-b8ab-dec732569790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GAT().to(device)\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    if epoch%200 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c541fa-27a6-4c70-86e6-7c75aafa2a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8150\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a735651-d9b6-4fce-ad2f-2806ad35305b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fbcf5-a33c-4e79-a472-664b9a038382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "471c984e-24ee-4dd9-9b1d-0647818d7419",
   "metadata": {},
   "source": [
    "## 4. https://github.com/raunakkmr/Graph-Attention-Networks\n",
    "\n",
    "### Used batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92a8bab-b3f8-44a9-9474-f1e9c424da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "from math import ceil\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75490371-de29-4d08-a248-d41e600062b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_heads, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.fcs = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_heads)])\n",
    "        self.a = nn.ModuleList([nn.Linear(2*output_dim, 1) for _ in range(num_heads)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, features, nodes, mapping, rows):\n",
    " \n",
    "\n",
    "        nprime = features.shape[0]\n",
    "        \n",
    "        rows = [np.array([mapping[v] for v in row], dtype=np.int64) for row in rows]\n",
    "        sum_degs = np.hstack(([0], np.cumsum([len(row) for row in rows])))\n",
    "        \n",
    "        mapped_nodes = [mapping[v] for v in nodes]\n",
    "        indices = torch.LongTensor([[v, c] for (v, row) in zip(mapped_nodes, rows) for c in row]).t()\n",
    "\n",
    "\n",
    "        out = []\n",
    "        for k in range(self.num_heads):\n",
    "            h = self.fcs[k](features)\n",
    "\n",
    "            nbr_h = torch.cat(tuple([h[row] for row in rows]), dim=0) # Neighbour\n",
    "            self_h = torch.cat(tuple([h[mapping[nodes[i]]].repeat(len(row), 1) for (i, row) in enumerate(rows)]), dim=0)\n",
    "            \n",
    "            cat_h = torch.cat((self_h, nbr_h), dim=1)\n",
    "\n",
    "            e = self.leakyrelu(self.a[k](cat_h))\n",
    "\n",
    "            alpha = [self.softmax(e[lo : hi]) for (lo, hi) in zip(sum_degs, sum_degs[1:])]\n",
    "            alpha = torch.cat(tuple(alpha), dim=0)\n",
    "            \n",
    "            alpha = alpha.squeeze(1)\n",
    "            alpha = self.dropout(alpha)\n",
    "\n",
    "            adj = torch.sparse.FloatTensor(indices, alpha, torch.Size([nprime, nprime]))\n",
    "            out.append(torch.sparse.mm(adj, h)[mapped_nodes])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b33c2c6-9060-4b8b-9e1c-7d449cc5d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, num_heads,\n",
    "                 dropout=0.5, device='cpu'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.num_layers = len(hidden_dims) + 1\n",
    "\n",
    "        dims = [input_dim] + [d*nh for (d, nh) in zip(hidden_dims, num_heads[:-1])] + [output_dim*num_heads[-1]]\n",
    "        in_dims = dims[:-1]\n",
    "        out_dims = [d // nh for (d, nh) in zip(dims[1:], num_heads)]\n",
    "\n",
    "        self.attn = nn.ModuleList([GraphAttention(i, o, nh, dropout) for (i, o, nh) in zip(in_dims, out_dims, num_heads)])\n",
    "\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(dim) for dim in dims[1:-1]])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, features, node_layers, mappings, rows):\n",
    "\n",
    "        out = features\n",
    "        for k in range(self.num_layers):\n",
    "            nodes = node_layers[k+1]\n",
    "            mapping = mappings[k]\n",
    "            \n",
    "            init_mapped_nodes = np.array([mappings[0][v] for v in nodes], dtype=np.int64)\n",
    "            \n",
    "            cur_rows = rows[init_mapped_nodes]\n",
    "            out = self.dropout(out)\n",
    "            \n",
    "            out = self.attn[k](out, nodes, mapping, cur_rows)\n",
    "            \n",
    "            if k+1 < self.num_layers:\n",
    "                out = [self.elu(o) for o in out]\n",
    "                out = torch.cat(tuple(out), dim=1)\n",
    "                out = self.bns[k](out)\n",
    "                \n",
    "            else:\n",
    "                out = torch.cat(tuple([x.flatten().unsqueeze(0) for x in out]), dim=0)\n",
    "                out = out.mean(dim=0).reshape(len(nodes), self.output_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d781975-64bc-4cd6-9748-6bef3f3de79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dict = {\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"dataset\" : \"cora\",\n",
    "    \"dataset_path\" : \"data/cora/\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"task\" : \"node_classification\",\n",
    "    \"cuda\" : \"True\",\n",
    "    \"hidden_dims\" : [8],\n",
    "    \"num_heads\" : [8, 1],\n",
    "    \"dropout\" : 0.6,\n",
    "    \"batch_size\" : 140,\n",
    "    \"epochs\" : 200,\n",
    "    \"lr\" : 5e-2,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"transductive\" : \"True\",\n",
    "    \"self_loop\" : \"True\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205a2c87-e1cf-4996-ad28-5c6dc13dfbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cora(Dataset):\n",
    "\n",
    "    def __init__(self, path, mode, num_layers,\n",
    "                 self_loop=False, normalize_adj=False, transductive=False):\n",
    "\n",
    "        super(Cora, self).__init__()\n",
    "\n",
    "        self.path = path\n",
    "        self.mode = mode\n",
    "        self.num_layers = num_layers\n",
    "        self.self_loop = self_loop\n",
    "        self.normalize_adj = normalize_adj\n",
    "        self.transductive = transductive\n",
    "        self.idx = {\n",
    "            'train' : np.array(range(140)),\n",
    "            'val' : np.array(range(200, 500)),\n",
    "            'test' : np.array(range(500, 1500))\n",
    "        }\n",
    "\n",
    "        print('--------------------------------')\n",
    "        print('Reading cora dataset from {}'.format(path))\n",
    "        citations = np.loadtxt(os.path.join(path, 'cora.cites'), dtype=np.int64)\n",
    "        content = np.loadtxt(os.path.join(path, 'cora.content'), dtype=str)\n",
    "        print('Finished reading data.')\n",
    "\n",
    "        print('Setting up data structures.')\n",
    "        if transductive:\n",
    "            idx = np.arange(content.shape[0])\n",
    "        else:\n",
    "            if mode == 'train':\n",
    "                idx = self.idx['train']\n",
    "                \n",
    "            elif mode == 'val':\n",
    "                idx = np.hstack((self.idx['train'], self.idx['val']))\n",
    "                \n",
    "            elif mode == 'test':\n",
    "                idx = np.hstack((self.idx['train'], self.idx['test']))\n",
    "                \n",
    "        features, labels = content[idx, 1:-1].astype(np.float32), content[idx, -1]\n",
    "        d = {j : i for (i,j) in enumerate(sorted(set(labels)))}\n",
    "        labels = np.array([d[l] for l in labels])\n",
    "\n",
    "        vertices = np.array(content[idx, 0], dtype=np.int64)\n",
    "        d = {j : i for (i,j) in enumerate(vertices)}\n",
    "        \n",
    "        edges = np.array([e for e in citations if e[0] in d.keys() and e[1] in d.keys()])\n",
    "        edges = np.array([d[v] for v in edges.flatten()]).reshape(edges.shape)\n",
    "        \n",
    "        n, m = labels.shape[0], edges.shape[0]\n",
    "        u, v = edges[:, 0], edges[:, 1]\n",
    "        \n",
    "        adj = sp.coo_matrix((np.ones(m), (u, v)),\n",
    "                            shape=(n, n),\n",
    "                            dtype=np.float32)\n",
    "        \n",
    "        adj += adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj) # Building a symmetric adjacency matrix of a given sparse matrix\n",
    "        \n",
    "        if self_loop:\n",
    "            adj += sp.eye(n)\n",
    "            \n",
    "        if normalize_adj:\n",
    "            degrees = np.power(np.array(np.sum(adj, axis=1)), -0.5).flatten()\n",
    "            degrees = sp.diags(degrees)\n",
    "            adj = (degrees.dot(adj.dot(degrees)))\n",
    "        print('Finished setting up data structures.')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.adj = adj.tolil()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx[self.mode])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.transductive:\n",
    "            idx += int(self.idx[self.mode][0])\n",
    "        else:\n",
    "            if self.mode != 'train':\n",
    "                idx += len(self.idx['train'])\n",
    "                \n",
    "                \n",
    "        node_layers, mappings = self._form_computation_graph(idx)\n",
    "        rows = self.adj.rows[node_layers[0]]\n",
    "        \n",
    "        features = self.features[node_layers[0], :]\n",
    "        labels = self.labels[node_layers[-1]]\n",
    "        \n",
    "        features = torch.FloatTensor(features)\n",
    "        labels = torch.LongTensor(labels)\n",
    "\n",
    "        return features, node_layers, mappings, rows, labels\n",
    "\n",
    "    def collate_wrapper(self, batch):\n",
    "\n",
    "        idx = [node_layers[-1][0] for node_layers in [sample[1] for sample in batch]]\n",
    "\n",
    "        \n",
    "        node_layers, mappings = self._form_computation_graph(idx)\n",
    "        \n",
    "        rows = self.adj.rows[node_layers[0]]       \n",
    "        features = self.features[node_layers[0], :]\n",
    "        labels = self.labels[node_layers[-1]]\n",
    "        \n",
    "        features = torch.FloatTensor(features)\n",
    "        labels = torch.LongTensor(labels)\n",
    "\n",
    "        return features, node_layers, mappings, rows, labels\n",
    "\n",
    "    def get_dims(self):\n",
    "\n",
    "        return self.features.shape[1], len(set(self.labels))\n",
    "\n",
    "    def _form_computation_graph(self, idx):\n",
    "\n",
    "        _list, _set = list, set\n",
    "        rows = self.adj.rows\n",
    "        \n",
    "        if type(idx) is int:\n",
    "            node_layers = [np.array([idx], dtype=np.int64)]\n",
    "        elif type(idx) is list:\n",
    "            node_layers = [np.array(idx, dtype=np.int64)]\n",
    "            \n",
    "        for _ in range(self.num_layers):\n",
    "            prev = node_layers[-1]\n",
    "            arr = [node for node in prev]\n",
    "            arr.extend([v for node in arr for v in rows[node]])\n",
    "            arr = np.array(_list(_set(arr)), dtype=np.int64)\n",
    "            node_layers.append(arr)\n",
    "        node_layers.reverse()\n",
    "\n",
    "        mappings = [{j : i for (i,j) in enumerate(arr)} for arr in node_layers]\n",
    "\n",
    "        return node_layers, mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e58500-9832-4ef6-b5a7-47d69009a6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f00df7ff-698e-4a96-8c09-7691f557ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import node_classification\n",
    "\n",
    "def get_criterion(task):\n",
    "\n",
    "    if task == 'node_classification':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return criterion\n",
    "\n",
    "def get_dataset(args):\n",
    "\n",
    "    task, dataset_name, *dataset_args = args\n",
    "    if task == 'node_classification':\n",
    "        if dataset_name == 'cora':\n",
    "            dataset = Cora(*dataset_args)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_fname(config):\n",
    "\n",
    "    hidden_dims_str = '_'.join([str(x) for x in config['hidden_dims']])\n",
    "    num_heads_str = '_'.join([str(x) for x in config['num_heads']])\n",
    "    batch_size = config['batch_size']\n",
    "    epochs = config['epochs']\n",
    "    lr = config['lr']\n",
    "    weight_decay = config['weight_decay']\n",
    "    dropout = config['dropout']\n",
    "    transductive = str(config['transductive'])\n",
    "    fname = 'gat_hidden_dims_{}_num_heads_{}_batch_size_{}_epochs_{}_lr_{}_weight_decay_{}_dropout_{}_transductive_{}.pth'.format(\n",
    "        hidden_dims_str, num_heads_str, batch_size, epochs, lr,\n",
    "        weight_decay, dropout, transductive)\n",
    "\n",
    "    return fname\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--json', type=str, default='config.json',\n",
    "                        help='path to json file with arguments, default: config.json')\n",
    "\n",
    "    parser.add_argument('--print_every', type=int, default=16,\n",
    "                        help='print loss and accuracy after how many batches, default: 16')\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, choices=['cora'], default='cora',\n",
    "                        help='name of the dataset, default=cora')\n",
    "    parser.add_argument('--dataset_path', type=str,\n",
    "                        default='/Users/raunak/Documents/Datasets/Cora', \n",
    "                        help='path to dataset')\n",
    "    parser.add_argument('--self_loop', action='store_true',\n",
    "                        help='whether to add self loops to adjacency matrix, default=False')\n",
    "    parser.add_argument('--normalize_adj', action='store_true',\n",
    "                        help='whether to normalize adj like in gcn, default=False')\n",
    "    parser.add_argument('--transductive', action='store_true',\n",
    "                        help='whether to use all nodes while training, default=False')\n",
    "\n",
    "    parser.add_argument('--task', type=str,\n",
    "                        choices=['unsupervised', 'node_classification'],\n",
    "                        default='node_classification',\n",
    "                        help='type of task, default=node_classification')\n",
    "\n",
    "    parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                        help='dropout parameter, default=0.5.')\n",
    "    parser.add_argument('--cuda', action='store_true',\n",
    "                        help='whether to use GPU, default: False')\n",
    "    parser.add_argument('--hidden_dims', type=int, nargs=\"*\",\n",
    "                        help='dimensions of hidden layers, specify through config.json')\n",
    "    parser.add_argument('--num_heads', type=int, nargs=\"*\",\n",
    "                        help='number of attention heads in each layer, length should be equal to len(hidden_dims)+1, specify through config.json')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=8,\n",
    "                        help='training batch size, default=8')\n",
    "    parser.add_argument('--epochs', type=int, default=10,\n",
    "                        help='number of training epochs, default=10')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        help='learning rate, default=1e-3')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                        help='weight decay, default=5e-4')\n",
    "\n",
    "    parser.add_argument('--save', action='store_true',\n",
    "                        help='whether to save model in trained_models/ directory, default: False')\n",
    "    parser.add_argument('--load', action='store_true',\n",
    "                        help='whether to load model in trained_models/ directory')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    config = vars(args)\n",
    "\n",
    "    config.update(json_dict)\n",
    "\n",
    "    config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "    print('--------------------------------')\n",
    "    print('Config:')\n",
    "    for (k, v) in config.items():\n",
    "        print(\"    '{}': '{}'\".format(k, v))\n",
    "    print('--------------------------------')\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98154a-0bf6-4c79-a658-af3538809ca8",
   "metadata": {},
   "source": [
    "###### https://pytorch.org/docs/stable/data.html#dataloader-collate-fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59bce7b-4e79-42d1-b6eb-df119ea0a67d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = parse_args()\n",
    "\n",
    "    #if config['cuda'] and torch.cuda.is_available():\n",
    "    #    device = 'cuda:0'\n",
    "    #else:\n",
    "    device = 'cpu'\n",
    "\n",
    "    dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                    'train', config['num_layers'], config['self_loop'],\n",
    "                    config['normalize_adj'], config['transductive'])\n",
    "    \n",
    "    dataset = get_dataset(dataset_args)\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                        shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "    \n",
    "    input_dim, output_dim = dataset.get_dims()\n",
    "\n",
    "    model = GAT(input_dim, config['hidden_dims'], output_dim,\n",
    "                       config['num_heads'], config['dropout'], device)\n",
    "    model.to(device)\n",
    "\n",
    "    if not config['load']:\n",
    "        criterion = get_criterion(config['task'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                               weight_decay=config['weight_decay'])\n",
    "        \n",
    "        epochs = config['epochs']\n",
    "        stats_per_batch = config['stats_per_batch']\n",
    "        num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "        \n",
    "        model.train()\n",
    "        print('--------------------------------')\n",
    "        print('Training.')\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "            running_loss = 0.0\n",
    "            num_correct, num_examples = 0, 0\n",
    "            \n",
    "            for (idx, batch) in enumerate(loader):\n",
    "                features, node_layers, mappings, rows, labels = batch\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(features, node_layers, mappings, rows)\n",
    "                loss = criterion(out, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    running_loss += loss.item()\n",
    "                    predictions = torch.max(out, dim=1)[1]\n",
    "                    num_correct += torch.sum(predictions == labels).item()\n",
    "                    num_examples += len(labels)\n",
    "                    \n",
    "                if (idx + 1) % stats_per_batch == 0:\n",
    "                    running_loss /= stats_per_batch\n",
    "                    accuracy = num_correct / num_examples\n",
    "                    print('    Batch {} / {}: loss {}, accuracy {}'.format(\n",
    "                        idx+1, num_batches, running_loss, accuracy))\n",
    "                    running_loss = 0.0\n",
    "                    num_correct, num_examples = 0, 0\n",
    "        print('Finished training.')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        if config['save']:\n",
    "            print('--------------------------------')\n",
    "            directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                    'trained_models')\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            fname = get_fname(config)\n",
    "            path = os.path.join(directory, fname)\n",
    "            print('Saving model at {}'.format(path))\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print('Finished saving model.')\n",
    "            print('--------------------------------')\n",
    "        \n",
    "        \n",
    "    dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                    'test', config['num_layers'], config['self_loop'],\n",
    "                    config['normalize_adj'], config['transductive'])\n",
    "    \n",
    "    dataset = get_dataset(dataset_args)\n",
    "    \n",
    "    \n",
    "    loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                        shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "    criterion = get_criterion(config['task'])\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    \n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    model.eval()\n",
    "    print('--------------------------------')\n",
    "    print('Testing.')\n",
    "    \n",
    "    \n",
    "    running_loss, total_loss = 0.0, 0.0\n",
    "    num_correct, num_examples = 0, 0\n",
    "    total_correct, total_examples = 0, 0\n",
    "    \n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions = torch.max(out, dim=1)[1]\n",
    "        \n",
    "        num_correct += torch.sum(predictions == labels).item()\n",
    "        total_correct += torch.sum(predictions == labels).item()\n",
    "        \n",
    "        num_examples += len(labels)\n",
    "        total_examples += len(labels)\n",
    "        \n",
    "        if (idx + 1) % stats_per_batch == 0:\n",
    "            running_loss /= stats_per_batch\n",
    "            accuracy = num_correct / num_examples\n",
    "            print('    Batch {} / {}: loss {}, accuracy {}'.format(\n",
    "                idx+1, num_batches, running_loss, accuracy))\n",
    "            running_loss = 0.0\n",
    "            num_correct, num_examples = 0, 0\n",
    "            \n",
    "    total_loss /= num_batches\n",
    "    total_accuracy = total_correct / total_examples\n",
    "    print('Loss {}, accuracy {}'.format(total_loss, total_accuracy))\n",
    "    print('Finished testing.')\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10456312-7f2e-4dc5-a523-db06c3d8666d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Config:\n",
      "    'json': 'config.json'\n",
      "    'print_every': '16'\n",
      "    'dataset': 'cora'\n",
      "    'dataset_path': 'data/cora/'\n",
      "    'self_loop': 'True'\n",
      "    'normalize_adj': 'False'\n",
      "    'transductive': 'True'\n",
      "    'task': 'node_classification'\n",
      "    'dropout': '0.6'\n",
      "    'cuda': 'True'\n",
      "    'hidden_dims': '[8]'\n",
      "    'num_heads': '[8, 1]'\n",
      "    'batch_size': '140'\n",
      "    'epochs': '200'\n",
      "    'lr': '0.05'\n",
      "    'weight_decay': '0.0005'\n",
      "    'save': 'False'\n",
      "    'load': 'False'\n",
      "    'stats_per_batch': '3'\n",
      "    'mode': 'train'\n",
      "    'num_layers': '2'\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Reading cora dataset from data/cora/\n",
      "Finished reading data.\n",
      "Setting up data structures.\n",
      "Finished setting up data structures.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Training.\n",
      "Epoch 1 / 200\n",
      "Epoch 2 / 200\n",
      "Epoch 3 / 200\n",
      "Epoch 4 / 200\n",
      "Epoch 5 / 200\n",
      "Epoch 6 / 200\n",
      "Epoch 7 / 200\n",
      "Epoch 8 / 200\n",
      "Epoch 9 / 200\n",
      "Epoch 10 / 200\n",
      "Epoch 11 / 200\n",
      "Epoch 12 / 200\n",
      "Epoch 13 / 200\n",
      "Epoch 14 / 200\n",
      "Epoch 15 / 200\n",
      "Epoch 16 / 200\n",
      "Epoch 17 / 200\n",
      "Epoch 18 / 200\n",
      "Epoch 19 / 200\n",
      "Epoch 20 / 200\n",
      "Epoch 21 / 200\n",
      "Epoch 22 / 200\n",
      "Epoch 23 / 200\n",
      "Epoch 24 / 200\n",
      "Epoch 25 / 200\n",
      "Epoch 26 / 200\n",
      "Epoch 27 / 200\n",
      "Epoch 28 / 200\n",
      "Epoch 29 / 200\n",
      "Epoch 30 / 200\n",
      "Epoch 31 / 200\n",
      "Epoch 32 / 200\n",
      "Epoch 33 / 200\n",
      "Epoch 34 / 200\n",
      "Epoch 35 / 200\n",
      "Epoch 36 / 200\n",
      "Epoch 37 / 200\n",
      "Epoch 38 / 200\n",
      "Epoch 39 / 200\n",
      "Epoch 40 / 200\n",
      "Epoch 41 / 200\n",
      "Epoch 42 / 200\n",
      "Epoch 43 / 200\n",
      "Epoch 44 / 200\n",
      "Epoch 45 / 200\n",
      "Epoch 46 / 200\n",
      "Epoch 47 / 200\n",
      "Epoch 48 / 200\n",
      "Epoch 49 / 200\n",
      "Epoch 50 / 200\n",
      "Epoch 51 / 200\n",
      "Epoch 52 / 200\n",
      "Epoch 53 / 200\n",
      "Epoch 54 / 200\n",
      "Epoch 55 / 200\n",
      "Epoch 56 / 200\n",
      "Epoch 57 / 200\n",
      "Epoch 58 / 200\n",
      "Epoch 59 / 200\n",
      "Epoch 60 / 200\n",
      "Epoch 61 / 200\n",
      "Epoch 62 / 200\n",
      "Epoch 63 / 200\n",
      "Epoch 64 / 200\n",
      "Epoch 65 / 200\n",
      "Epoch 66 / 200\n",
      "Epoch 67 / 200\n",
      "Epoch 68 / 200\n",
      "Epoch 69 / 200\n",
      "Epoch 70 / 200\n",
      "Epoch 71 / 200\n",
      "Epoch 72 / 200\n",
      "Epoch 73 / 200\n",
      "Epoch 74 / 200\n",
      "Epoch 75 / 200\n",
      "Epoch 76 / 200\n",
      "Epoch 77 / 200\n",
      "Epoch 78 / 200\n",
      "Epoch 79 / 200\n",
      "Epoch 80 / 200\n",
      "Epoch 81 / 200\n",
      "Epoch 82 / 200\n",
      "Epoch 83 / 200\n",
      "Epoch 84 / 200\n",
      "Epoch 85 / 200\n",
      "Epoch 86 / 200\n",
      "Epoch 87 / 200\n",
      "Epoch 88 / 200\n",
      "Epoch 89 / 200\n",
      "Epoch 90 / 200\n",
      "Epoch 91 / 200\n",
      "Epoch 92 / 200\n",
      "Epoch 93 / 200\n",
      "Epoch 94 / 200\n",
      "Epoch 95 / 200\n",
      "Epoch 96 / 200\n",
      "Epoch 97 / 200\n",
      "Epoch 98 / 200\n",
      "Epoch 99 / 200\n",
      "Epoch 100 / 200\n",
      "Epoch 101 / 200\n",
      "Epoch 102 / 200\n",
      "Epoch 103 / 200\n",
      "Epoch 104 / 200\n",
      "Epoch 105 / 200\n",
      "Epoch 106 / 200\n",
      "Epoch 107 / 200\n",
      "Epoch 108 / 200\n",
      "Epoch 109 / 200\n",
      "Epoch 110 / 200\n",
      "Epoch 111 / 200\n",
      "Epoch 112 / 200\n",
      "Epoch 113 / 200\n",
      "Epoch 114 / 200\n",
      "Epoch 115 / 200\n",
      "Epoch 116 / 200\n",
      "Epoch 117 / 200\n",
      "Epoch 118 / 200\n",
      "Epoch 119 / 200\n",
      "Epoch 120 / 200\n",
      "Epoch 121 / 200\n",
      "Epoch 122 / 200\n",
      "Epoch 123 / 200\n",
      "Epoch 124 / 200\n",
      "Epoch 125 / 200\n",
      "Epoch 126 / 200\n",
      "Epoch 127 / 200\n",
      "Epoch 128 / 200\n",
      "Epoch 129 / 200\n",
      "Epoch 130 / 200\n",
      "Epoch 131 / 200\n",
      "Epoch 132 / 200\n",
      "Epoch 133 / 200\n",
      "Epoch 134 / 200\n",
      "Epoch 135 / 200\n",
      "Epoch 136 / 200\n",
      "Epoch 137 / 200\n",
      "Epoch 138 / 200\n",
      "Epoch 139 / 200\n",
      "Epoch 140 / 200\n",
      "Epoch 141 / 200\n",
      "Epoch 142 / 200\n",
      "Epoch 143 / 200\n",
      "Epoch 144 / 200\n",
      "Epoch 145 / 200\n",
      "Epoch 146 / 200\n",
      "Epoch 147 / 200\n",
      "Epoch 148 / 200\n",
      "Epoch 149 / 200\n",
      "Epoch 150 / 200\n",
      "Epoch 151 / 200\n",
      "Epoch 152 / 200\n",
      "Epoch 153 / 200\n",
      "Epoch 154 / 200\n",
      "Epoch 155 / 200\n",
      "Epoch 156 / 200\n",
      "Epoch 157 / 200\n",
      "Epoch 158 / 200\n",
      "Epoch 159 / 200\n",
      "Epoch 160 / 200\n",
      "Epoch 161 / 200\n",
      "Epoch 162 / 200\n",
      "Epoch 163 / 200\n",
      "Epoch 164 / 200\n",
      "Epoch 165 / 200\n",
      "Epoch 166 / 200\n",
      "Epoch 167 / 200\n",
      "Epoch 168 / 200\n",
      "Epoch 169 / 200\n",
      "Epoch 170 / 200\n",
      "Epoch 171 / 200\n",
      "Epoch 172 / 200\n",
      "Epoch 173 / 200\n",
      "Epoch 174 / 200\n",
      "Epoch 175 / 200\n",
      "Epoch 176 / 200\n",
      "Epoch 177 / 200\n",
      "Epoch 178 / 200\n",
      "Epoch 179 / 200\n",
      "Epoch 180 / 200\n",
      "Epoch 181 / 200\n",
      "Epoch 182 / 200\n",
      "Epoch 183 / 200\n",
      "Epoch 184 / 200\n",
      "Epoch 185 / 200\n",
      "Epoch 186 / 200\n",
      "Epoch 187 / 200\n",
      "Epoch 188 / 200\n",
      "Epoch 189 / 200\n",
      "Epoch 190 / 200\n",
      "Epoch 191 / 200\n",
      "Epoch 192 / 200\n",
      "Epoch 193 / 200\n",
      "Epoch 194 / 200\n",
      "Epoch 195 / 200\n",
      "Epoch 196 / 200\n",
      "Epoch 197 / 200\n",
      "Epoch 198 / 200\n",
      "Epoch 199 / 200\n",
      "Epoch 200 / 200\n",
      "Finished training.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Reading cora dataset from data/cora/\n",
      "Finished reading data.\n",
      "Setting up data structures.\n",
      "Finished setting up data structures.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Testing.\n",
      "    Batch 3 / 8: loss 0.5464577476183573, accuracy 0.8428571428571429\n",
      "    Batch 6 / 8: loss 0.5920681754748026, accuracy 0.8261904761904761\n",
      "Loss 0.6126781329512596, accuracy 0.827\n",
      "Finished testing.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1b43a-4394-4166-b6a5-b94c96a78893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8f65b-0227-4008-9365-cf7ec3358f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa00e49c-f32d-4057-ba78-47bd3b6053d0",
   "metadata": {},
   "source": [
    "## 5.1 https://github.com/PetarV-/GAT\n",
    "#### pytorch implementatuon of GAT https://arxiv.org/abs/1710.10903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36285b62-aed9-4045-a339-43a8e244f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d0343-e401-42a7-8a6d-2eaf7ed900c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # Wh.shape (N, out_feature)\n",
    "        # self.a.shape (2 * out_feature, 1)\n",
    "        # Wh1&2.shape (N, 1)\n",
    "        # e.shape (N, N)\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.T\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "    \n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37d25b-deab-4963-bff7-2925cb01a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Sparse version of GAT.\"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
    "                                             nclass, \n",
    "                                             dropout=dropout, \n",
    "                                             alpha=alpha, \n",
    "                                             concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d1336-5d53-4e90-a3b3-5400bc69b3f5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b928667-ef4a-4445-8554-03e049ccc783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    # The classes must be sorted before encoding to enable static class encoding.\n",
    "    # In other words, make sure the first class always maps to index 0.\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize_features(features)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e2336-88c4-4e78-bb6c-ba3b5b93b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
    "parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
    "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=10000, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
    "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
    "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab330c8-e4cc-49d8-98cb-c7a0086a08dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcacfb-d015-4573-a879-373d396fe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "if args.sparse:\n",
    "    model = SpGAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "else:\n",
    "    model = GAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a8f67-60f8-443c-94df-54c977c3395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3380f83-b4eb-4a34-bb4e-c1e8a5af8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f88a2a-a69a-497f-9938-a312429a45ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "\n",
    "# Testing\n",
    "compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25e762-c4e9-4015-ab4a-2f075f173d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42635ab1-008a-49d7-be1e-1d671cb52cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f518d-246f-430b-9ad1-b68e6c7f7ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9656de9b-7d38-4508-bf8a-2f9f8a97d7a3",
   "metadata": {},
   "source": [
    "## 5.2 https://github.com/psh150204/GAT *** Can be referred for understanding\n",
    "#### PyTorch implementation of the paper \"Graph Attention Networks\" (ICLR 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f99d37-82da-49c6-a941-a8e75910c8b2",
   "metadata": {},
   "source": [
    "#### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c768e938-d8ab-4283-9e8b-9223dcb46ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import os \n",
    "import time \n",
    "import random \n",
    "import argparse\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim \n",
    "from torch.autograd import Variable\n",
    "import scipy.sparse as sp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc943fc0-8b10-46af-9961-45c6a62be0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolutionLayer,self).__init__()\n",
    "        self.W = nn.Parameter(torch.zeros(in_features, out_features, dtype = torch.float32))\n",
    "        nn.init.xavier_uniform_(self.W) # initialize as described in Glorot & Bengio (2010)\n",
    "    \n",
    "    def forward(self, input, adj):\n",
    "        # input (= X) : a tensor with size [N, F]\n",
    "        # adj (= A_hat) : a tensor with size [N, N]\n",
    "\n",
    "        return torch.mm(adj, torch.mm(input, self.W))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    # single head attention\n",
    "    def __init__(self, in_features, out_features, alpha):\n",
    "        super(Attention, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W = nn.Linear(in_features, out_features, bias = False)\n",
    "        self.a_T = nn.Linear(2 * out_features, 1, bias = False)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.xavier_uniform_(self.a_T.weight)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        # h : a tensor with size [N, F] where N be a number of nodes and F be a number of features\n",
    "        N = h.size(0)\n",
    "        Wh = self.W(h) # h -> Wh : [N, F] -> [N, F']\n",
    "        \n",
    "        # H1 : [N, N, F'], H2 : [N, N, F'], attn_input = [N, N, 2F']\n",
    "\n",
    "        # H1 = [[h1 h1 ... h1]   |  H2 = [[h1 h2 ... hN]   |   attn_input = [[h1||h1 h1||h2 ... h1||hN]\n",
    "        #       [h2 h2 ... h2]   |        [h1 h2 ... hN]   |                 [h2||h1 h2||h2 ... h2||hN]\n",
    "        #            ...         |             ...         |                         ...\n",
    "        #       [hN hN ... hN]]  |        [h1 h2 ... hN]]  |                 [hN||h1 hN||h2 ... hN||hN]]\n",
    "        \n",
    "        H1 = Wh.unsqueeze(1).repeat(1,N,1)\n",
    "        H2 = Wh.unsqueeze(0).repeat(N,1,1)\n",
    "        \n",
    "        attn_input = torch.cat([H1, H2], dim = -1)\n",
    "\n",
    "        e = F.leaky_relu(self.a_T(attn_input).squeeze(-1), negative_slope = self.alpha) # [N, N]\n",
    "        \n",
    "        attn_mask = -1e18*torch.ones_like(e)\n",
    "        masked_e = torch.where(adj > 0, e, attn_mask)\n",
    "        attn_scores = F.softmax(masked_e, dim = -1) # [N, N]\n",
    "\n",
    "        h_prime = torch.mm(attn_scores, Wh) # [N, F']\n",
    "\n",
    "        return F.elu(h_prime) # [N, F']\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    # multi head attention\n",
    "    def __init__(self, in_features, out_features, num_heads, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        \n",
    "        self.concat = concat\n",
    "        self.attentions = nn.ModuleList([Attention(in_features, out_features, alpha) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        # input (= X) : a tensor with size [N, F]\n",
    "\n",
    "        if self.concat :\n",
    "            # concatenate\n",
    "            outputs = []\n",
    "            for attention in self.attentions:\n",
    "                outputs.append(attention(input, adj))\n",
    "            \n",
    "            return torch.cat(outputs, dim = -1) # [N, KF']\n",
    "\n",
    "        else :\n",
    "            # average\n",
    "            output = None\n",
    "            for attention in self.attentions:\n",
    "                if output == None:\n",
    "                    output = attention(input, adj)\n",
    "                else:\n",
    "                    output += attention(input, adj)\n",
    "            \n",
    "            return output/len(self.attentions) # [N, F']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd6dc8-8374-4c1d-b89c-6b0810e3c6be",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9139b284-7e54-4de1-9efc-97f3d7ff0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, F, H, C, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layer1 = GraphConvolutionLayer(F, H)\n",
    "        self.layer2 = GraphConvolutionLayer(H, C)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # X : a tensor with size [N, F]\n",
    "        \n",
    "        x = self.dropout(F.relu(self.layer1(x, adj))) # [N, H]\n",
    "        return self.layer2(x, adj) # [N, C]\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, F, H, C, dropout, alpha, K):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = GraphAttentionLayer(F, H, K, alpha)\n",
    "        self.layer2 = GraphAttentionLayer(K * H, C, 1, alpha, concat = False)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # x : a tensor with size [N, F]\n",
    "\n",
    "        x = self.dropout(F.relu(self.layer1(x, adj))) # [N, KH]\n",
    "        return self.layer2(x, adj) # [N, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761a54a-0825-470a-8c8a-310abe00721c",
   "metadata": {},
   "source": [
    "#### Load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d39812-149e-46dc-b3a0-6cf9ee202483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset=\"cora\"):\n",
    "    \n",
    "    print(\"loading {} dataset ... \". format(dataset))\n",
    "\n",
    "    path=\"./data/\"+dataset+\"/\" \n",
    "\n",
    "    if dataset == 'cora':\n",
    "        idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path,dataset), dtype=np.dtype(str))\n",
    "        features = sp.csr_matrix(idx_features_labels[:,1:-1], dtype=np.float32)\n",
    "        labels = encode_onehot(idx_features_labels[:,-1])\n",
    "\n",
    "        idx = np.array(idx_features_labels[:,0],dtype=np.int32)\n",
    "        idx_map = {j: i for i,j in enumerate(idx)}\n",
    "        edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path,dataset), dtype=np.int32)\n",
    "        edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "        adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        \n",
    "    elif dataset == 'citeseer':\n",
    "        idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path,dataset), dtype=np.dtype(str))\n",
    "        features = sp.csr_matrix(idx_features_labels[:,1:-1], dtype=np.float32)\n",
    "        labels = encode_onehot(idx_features_labels[:,-1])\n",
    "\n",
    "        idx = np.array(idx_features_labels[:,0],dtype=np.dtype(str))\n",
    "        idx_map = {j: i for i,j in enumerate(idx)}\n",
    "        \n",
    "        edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path,dataset), dtype=np.dtype(str))\n",
    "        edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.dtype(str)).reshape(edges_unordered.shape)\n",
    "        \n",
    "        adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T>adj) - adj.multiply(adj.T>adj)\n",
    "    features = normalize_features(features)\n",
    "    adj = normalize_adj(adj+sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200,500)\n",
    "    idx_test = range(500,1500)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test \n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "\n",
    "    return correct / len(labels)\n",
    "\n",
    "def normalize_adj(mx): # A_hat = DAD\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    mx_to =  mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "    return mx_to\n",
    "\n",
    "def normalize_features(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx_to =  r_mat_inv.dot(mx) \n",
    "    return mx_to \n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i,:] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def mkdir_p(mypath):\n",
    "    '''Creates a directory. equivalent to using mkdir -p on the command line'''\n",
    "\n",
    "    from errno import EEXIST\n",
    "    from os import makedirs,path\n",
    "\n",
    "    try:\n",
    "        makedirs(mypath)\n",
    "    except OSError as exc: # Python >2.5\n",
    "        if exc.errno == EEXIST and path.isdir(mypath):\n",
    "            pass\n",
    "        else: raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6cfb41-d778-4096-87b1-85d924341c44",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "loading cora dataset ... \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.75 GiB (GPU 0; 4.00 GiB total capacity; 1.84 GiB already allocated; 815.20 MiB free; 1.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m args\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# not args.no_cuda and torch.cuda.is_available()\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(args\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[1;32m--> 118\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 50\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     47\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     48\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 50\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [N, F]\u001b[39;00m\n\u001b[0;32m     51\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(preds[idx_train], labels[idx_train])\n\u001b[0;32m     52\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m accuracy(preds[idx_train], labels[idx_train])\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Optimization-Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m, in \u001b[0;36mGAT.forward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# x : a tensor with size [N, F]\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m)) \u001b[38;5;66;03m# [N, KH]\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x, adj)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Optimization-Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[7], line 67\u001b[0m, in \u001b[0;36mGraphAttentionLayer.forward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     65\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attention \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentions:\n\u001b[1;32m---> 67\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [N, KF']\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# average\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Optimization-Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, h, adj)\u001b[0m\n\u001b[0;32m     28\u001b[0m Wh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW(h) \u001b[38;5;66;03m# h -> Wh : [N, F] -> [N, F']\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# H1 : [N, N, F'], H2 : [N, N, F'], attn_input = [N, N, 2F']\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# H1 = [[h1 h1 ... h1]   |  H2 = [[h1 h2 ... hN]   |   attn_input = [[h1||h1 h1||h2 ... h1||hN]\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#       [h2 h2 ... h2]   |        [h1 h2 ... hN]   |                 [h2||h1 h2||h2 ... h2||hN]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#            ...         |             ...         |                         ...\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#       [hN hN ... hN]]  |        [h1 h2 ... hN]]  |                 [hN||h1 hN||h2 ... hN||hN]]\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m H1 \u001b[38;5;241m=\u001b[39m \u001b[43mWh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m H2 \u001b[38;5;241m=\u001b[39m Wh\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(N,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m attn_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([H1, H2], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.75 GiB (GPU 0; 4.00 GiB total capacity; 1.84 GiB already allocated; 815.20 MiB free; 1.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    # meta settings\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device('cuda' if(torch.cuda.is_available()) else 'cpu')\n",
    "\n",
    "    # load the data\n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_data(args.dataset)\n",
    "    features = features.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # parameter intialization\n",
    "    N = features.size(0) # num_of_nodes\n",
    "    F = features.size(1) # num_of_features\n",
    "    H = args.hidden # hidden nodes\n",
    "    C = labels.max().item() + 1 # num_classes\n",
    "    \n",
    "    # for validation\n",
    "    epochs_since_improvement = 0\n",
    "    best_loss = 10.\n",
    "\n",
    "    # init training object\n",
    "    if args.model == 'GCN':\n",
    "        network = GCN(F, H, C, args.dropout).to(device)\n",
    "\n",
    "        # pre-processing\n",
    "        A_tilde = adj + torch.eye(N)\n",
    "        D_tilde_inv_sqrt = torch.diag(torch.sqrt(torch.sum(A_tilde, dim = 1)) ** -1)\n",
    "        adj = torch.mm(D_tilde_inv_sqrt, torch.mm(A_tilde, D_tilde_inv_sqrt)).to(device) # A_hat\n",
    "        \n",
    "    else:\n",
    "        network = GAT(F, H, C, args.dropout, args.alpha, args.n_heads).to(device)\n",
    "        adj = adj.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(network.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(args.epochs):\n",
    "        t = time.time()\n",
    "        network.train()\n",
    "\n",
    "        preds = network(features, adj) # [N, F]\n",
    "        train_loss = criterion(preds[idx_train], labels[idx_train])\n",
    "        train_acc = accuracy(preds[idx_train], labels[idx_train])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            preds_val = network(features, adj)\n",
    "            val_loss = criterion(preds_val[idx_val], labels[idx_val])\n",
    "            val_acc = accuracy(preds_val[idx_val], labels[idx_val])\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss < best_loss :\n",
    "                best_loss = val_loss\n",
    "                epochs_since_improvement = 0\n",
    "            else:\n",
    "                epochs_since_improvement += 1\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_accs.append(train_acc.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        val_accs.append(val_acc.item())\n",
    "\n",
    "        print('[%d/%d] train loss : %.4f | train acc %.2f%% | val loss %.4f | val acc %.2f%% | time %.3fs'\n",
    "                    %(epoch+1, args.epochs, train_loss.item(), train_acc.item() * 100, val_loss.item(), val_acc.item() * 100, time.time() - t))\n",
    "\n",
    "        if epochs_since_improvement > args.patience - 1 :\n",
    "            print(\"There's no improvements during %d epochs and so stop the training.\"%(args.patience))\n",
    "            break\n",
    "    \n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        preds = network(features, adj)\n",
    "        test_acc = accuracy(preds[idx_test], labels[idx_test])\n",
    "        print('Test Accuracy : %.2f'%(test_acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "if __name__  == \"__main__\":\n",
    "    \n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "    parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
    "    parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
    "    parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train.')\n",
    "    parser.add_argument('--save_every', type=int, default=10, help='Save every n epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "    parser.add_argument('--hidden', type=int, default=64, help='Number of hidden units.')\n",
    "    parser.add_argument('--n_heads', type=int, default=8, help='Number of head attentions.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate (1 - keep probability).')\n",
    "    parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "    parser.add_argument('--patience', type=int, default=10, help='patience')\n",
    "    parser.add_argument('--dataset', type=str, default='cora', choices=['cora','citeseer'], help='Dataset to train.')\n",
    "    parser.add_argument('--model', type=str, default='GAT', choices=['GCN','GAT'], help='Model to train.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    args.cuda = False # not args.no_cuda and torch.cuda.is_available()\n",
    "    print(args.cuda)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58cdf66e-a46d-4424-9439-ed2c7383428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cora dataset ... \n"
     ]
    }
   ],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device('cuda' if(torch.cuda.is_available()) else 'cpu')\n",
    "\n",
    "# load the data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(args.dataset)\n",
    "features = features.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# parameter intialization\n",
    "N = features.size(0) # num_of_nodes\n",
    "F = features.size(1) # num_of_features\n",
    "H = args.hidden # hidden nodes\n",
    "C = labels.max().item() + 1 # num_classes\n",
    "\n",
    "# for validation\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 10.\n",
    "\n",
    "network = GAT(F, H, C, args.dropout, args.alpha, args.n_heads).to(device)\n",
    "adj = adj.to(device)\n",
    "    \n",
    "optimizer = optim.Adam(network.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5e767d-4b10-4726-b41e-c97b316f3f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2708, 1433]), torch.Size([2708, 2708]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.size(), adj.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f8f00-2f9a-4dab-a78a-594a196127c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Train\n",
    "for epoch in range(args.epochs):\n",
    "    t = time.time()\n",
    "    network.train()\n",
    "\n",
    "        preds = network(features, adj) # [N, F]\n",
    "        train_loss = criterion(preds[idx_train], labels[idx_train])\n",
    "        train_acc = accuracy(preds[idx_train], labels[idx_train])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            preds_val = network(features, adj)\n",
    "            val_loss = criterion(preds_val[idx_val], labels[idx_val])\n",
    "            val_acc = accuracy(preds_val[idx_val], labels[idx_val])\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss < best_loss :\n",
    "                best_loss = val_loss\n",
    "                epochs_since_improvement = 0\n",
    "            else:\n",
    "                epochs_since_improvement += 1\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_accs.append(train_acc.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        val_accs.append(val_acc.item())\n",
    "\n",
    "        print('[%d/%d] train loss : %.4f | train acc %.2f%% | val loss %.4f | val acc %.2f%% | time %.3fs'\n",
    "                    %(epoch+1, args.epochs, train_loss.item(), train_acc.item() * 100, val_loss.item(), val_acc.item() * 100, time.time() - t))\n",
    "\n",
    "        if epochs_since_improvement > args.patience - 1 :\n",
    "            print(\"There's no improvements during %d epochs and so stop the training.\"%(args.patience))\n",
    "            break\n",
    "    \n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        preds = network(features, adj)\n",
    "        test_acc = accuracy(preds[idx_test], labels[idx_test])\n",
    "        print('Test Accuracy : %.2f'%(test_acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "if __name__  == \"__main__\":\n",
    "    \n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "    parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
    "    parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
    "    parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train.')\n",
    "    parser.add_argument('--save_every', type=int, default=10, help='Save every n epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "    parser.add_argument('--hidden', type=int, default=64, help='Number of hidden units.')\n",
    "    parser.add_argument('--n_heads', type=int, default=8, help='Number of head attentions.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate (1 - keep probability).')\n",
    "    parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "    parser.add_argument('--patience', type=int, default=10, help='patience')\n",
    "    parser.add_argument('--dataset', type=str, default='cora', choices=['cora','citeseer'], help='Dataset to train.')\n",
    "    parser.add_argument('--model', type=str, default='GAT', choices=['GCN','GAT'], help='Model to train.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    args.cuda = False # not args.no_cuda and torch.cuda.is_available()\n",
    "    print(args.cuda)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa14a71-cfc9-4c0b-ba93-4e17d0943a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e34535cb-c5ad-4fdc-a934-a5451560aa96",
   "metadata": {},
   "source": [
    "## 5.3 https://github.com/marblet/gat-pytorch\n",
    "#### This is the pytorch inplementation of Graph Attention Networks.https://arxiv.org/abs/1710.10903"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0557724-8166-41a8-b850-8f9a5872446e",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a654b98-3948-4327-bdfc-8af359eecd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a5ab070-e23b-4f36-8809-e858c41bc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, data, nhid=8, nhead=8, nhead_out=1, alpha=0.2, dropout=0.6):\n",
    "        super(GAT, self).__init__()\n",
    "        nfeat, nclass = data.num_features, data.num_classes\n",
    "        \n",
    "        self.attentions = [GATConv(nfeat, nhid, dropout=dropout, alpha=alpha) for _ in range(nhead)]\n",
    "        self.out_atts = [GATConv(nhid * nhead, nclass, dropout=dropout, alpha=alpha) for _ in range(nhead_out)]\n",
    "        \n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "            \n",
    "        for i, attention in enumerate(self.out_atts):\n",
    "            self.add_module('out_att{}'.format(i), attention)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for att in self.attentions:\n",
    "            att.reset_parameters()\n",
    "        for att in self.out_atts:\n",
    "            att.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_list = data.features, data.edge_list\n",
    "        x = torch.cat([att(x, edge_list) for att in self.attentions], dim=1)\n",
    "        x = F.elu(x)\n",
    "        x = torch.sum(torch.stack([att(x, edge_list) for att in self.out_atts]), dim=0) / len(self.out_atts)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GATConv(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, bias=True):\n",
    "        super(GATConv, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.data, gain=1.414)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, x, edge_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        h = torch.matmul(x, self.weight)\n",
    "\n",
    "        source, target = edge_list\n",
    "        a_input = torch.cat([h[source], h[target]], dim=1)\n",
    "        e = F.leaky_relu(torch.matmul(a_input, self.a), negative_slope=self.alpha)\n",
    "\n",
    "        N = h.size(0)\n",
    "        attention = -1e20*torch.ones([N, N], device=device, requires_grad=True)\n",
    "        attention[source, target] = e[:, 0]\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        if self.bias is not None:\n",
    "            h_prime = h_prime + self.bias\n",
    "\n",
    "        return h_prime\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class SPGAT(nn.Module):\n",
    "    def __init__(self, data, nhid=8, nhead=8, nhead_out=1, alpha=0.2, dropout=0.6):\n",
    "        super(SPGAT, self).__init__()\n",
    "        nfeat, nclass = data.num_features, data.num_classes\n",
    "        self.attentions = [SPGATConv(nfeat, nhid, dropout=dropout, alpha=alpha) for _ in range(nhead)]\n",
    "        self.out_atts = [SPGATConv(nhid * nhead, nclass, dropout=dropout, alpha=alpha) for _ in range(nhead_out)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        for i, attention in enumerate(self.out_atts):\n",
    "            self.add_module('out_att{}'.format(i), attention)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for att in self.attentions:\n",
    "            att.reset_parameters()\n",
    "        for att in self.out_atts:\n",
    "            att.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_list = data.features, data.edge_list\n",
    "        x = torch.cat([att(x, edge_list) for att in self.attentions], dim=1)\n",
    "        x = F.elu(x)\n",
    "        x = torch.sum(torch.stack([att(x, edge_list) for att in self.out_atts]), dim=0) / len(self.out_atts)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def sp_softmax(indices, values, N):\n",
    "    source, _ = indices\n",
    "    v_max = values.max()\n",
    "    exp_v = torch.exp(values - v_max)\n",
    "    exp_sum = torch.zeros(N, 1, device=device)\n",
    "    exp_sum.scatter_add_(0, source.unsqueeze(1), exp_v)\n",
    "    exp_sum += 1e-10\n",
    "    softmax_v = exp_v / exp_sum[source]\n",
    "    return softmax_v\n",
    "\n",
    "\n",
    "def sp_matmul(indices, values, mat):\n",
    "    source, target = indices\n",
    "    out = torch.zeros_like(mat)\n",
    "    out.scatter_add_(0, source.expand(mat.size(1), -1).t(), values * mat[target])\n",
    "    return out\n",
    "\n",
    "\n",
    "class SPGATConv(GATConv):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, bias=True):\n",
    "        super(SPGATConv, self).__init__(in_features, out_features, dropout, alpha, bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        h = torch.matmul(x, self.weight)\n",
    "\n",
    "        source, target = edge_list\n",
    "        a_input = torch.cat([h[source], h[target]], dim=1)\n",
    "        e = F.leaky_relu(torch.matmul(a_input, self.a), negative_slope=self.alpha)\n",
    "\n",
    "        attention = sp_softmax(edge_list, e, h.size(0))\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        h_prime = sp_matmul(edge_list, attention, h)\n",
    "        if self.bias is not None:\n",
    "            h_prime = h_prime + self.bias\n",
    "\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c3196-b37d-46c3-9a9f-ed760f8b0341",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "389be3b6-2354-436b-ab0e-4ce3a877fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, adj, edge_list, features, labels, train_mask, val_mask, test_mask):\n",
    "        self.adj = adj\n",
    "        self.edge_list = edge_list\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.train_mask = train_mask\n",
    "        self.val_mask = val_mask\n",
    "        self.test_mask = test_mask\n",
    "        self.num_features = features.size(1)\n",
    "        self.num_classes = int(torch.max(labels)) + 1\n",
    "\n",
    "    def to(self, device):\n",
    "        self.adj = self.adj.to(device)\n",
    "        self.edge_list = self.edge_list.to(device)\n",
    "        self.features = self.features.to(device)\n",
    "        self.labels = self.labels.to(device)\n",
    "        self.train_mask = self.train_mask.to(device)\n",
    "        self.val_mask = self.val_mask.to(device)\n",
    "        self.test_mask = self.test_mask.to(device)\n",
    "\n",
    "\n",
    "def load_data(dataset_str, norm_feat=True):\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for name in names:\n",
    "        with open(\"data/planetoid/ind.{}.{}\".format(dataset_str, name), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                out = pkl.load(f, encoding='latin1')\n",
    "            else:\n",
    "                out = objects.append(pkl.load(f))\n",
    "\n",
    "            if name == 'graph':\n",
    "                objects.append(out)\n",
    "            else:\n",
    "                out = out.todense() if hasattr(out, 'todense') else out\n",
    "                objects.append(torch.Tensor(out))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx = parse_index_file(\"data/planetoid/ind.{}.test.index\".format(dataset_str))\n",
    "    train_idx = torch.arange(y.size(0), dtype=torch.long)\n",
    "    val_idx = torch.arange(y.size(0), y.size(0) + 500, dtype=torch.long)\n",
    "    sorted_test_idx = np.sort(test_idx)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        len_test_idx = max(test_idx) - min(test_idx) + 1\n",
    "        tx_ext = torch.zeros(len_test_idx, tx.size(1))\n",
    "        tx_ext[sorted_test_idx - min(test_idx), :] = tx\n",
    "        ty_ext = torch.zeros(len_test_idx, ty.size(1))\n",
    "        ty_ext[sorted_test_idx - min(test_idx), :] = ty\n",
    "\n",
    "        tx, ty = tx_ext, ty_ext\n",
    "\n",
    "    features = torch.cat([allx, tx], dim=0)\n",
    "    features[test_idx] = features[sorted_test_idx]\n",
    "    \n",
    "    if norm_feat:\n",
    "        features = preprocess_features(features)\n",
    "\n",
    "    labels = torch.cat([ally, ty], dim=0).max(dim=1)[1]\n",
    "    labels[test_idx] = labels[sorted_test_idx]\n",
    "\n",
    "    edge_list = adj_list_from_dict(graph)\n",
    "    edge_list = add_self_loops(edge_list, features.size(0))\n",
    "    adj = normalize_adj(edge_list)\n",
    "\n",
    "    train_mask = index_to_mask(train_idx, labels.shape[0])\n",
    "    val_mask = index_to_mask(val_idx, labels.shape[0])\n",
    "    test_mask = index_to_mask(test_idx, labels.shape[0])\n",
    "\n",
    "    data = Data(adj, edge_list, features, labels, train_mask, val_mask, test_mask)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def adj_list_from_dict(graph):\n",
    "    G = nx.from_dict_of_lists(graph)\n",
    "    coo_adj = nx.to_scipy_sparse_matrix(G).tocoo()\n",
    "    indices = torch.from_numpy(np.vstack((coo_adj.row, coo_adj.col)).astype(np.int64))\n",
    "    return indices\n",
    "\n",
    "\n",
    "def index_to_mask(index, size):\n",
    "    mask = torch.zeros((size, ), dtype=torch.bool)\n",
    "    mask[index] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def add_self_loops(edge_list, size):\n",
    "    i = torch.arange(size, dtype=torch.int64).view(1, -1)\n",
    "    self_loops = torch.cat((i, i), dim=0)\n",
    "    edge_list = torch.cat((edge_list, self_loops), dim=1)\n",
    "    return edge_list\n",
    "\n",
    "\n",
    "def get_degree(edge_list):\n",
    "    row, col = edge_list\n",
    "    deg = torch.bincount(row)\n",
    "    return deg\n",
    "\n",
    "\n",
    "def normalize_adj(edge_list):\n",
    "    deg = get_degree(edge_list)\n",
    "    row, col = edge_list\n",
    "    deg_inv_sqrt = torch.pow(deg.to(torch.float), -0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0.0\n",
    "    weight = torch.ones(edge_list.size(1))\n",
    "    v = deg_inv_sqrt[row] * weight * deg_inv_sqrt[col]\n",
    "    norm_adj = torch.sparse.FloatTensor(edge_list, v)\n",
    "    return norm_adj\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    rowsum = features.sum(dim=1, keepdim=True)\n",
    "    rowsum[rowsum == 0] = 1\n",
    "    features = features / rowsum\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc4817-e99d-404d-b3b6-8f7036c4090f",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a072fb2-0c13-4603-b037-f306426f018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from copy import deepcopy\n",
    "from numpy import mean, std\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose, use_loss, use_acc, save_model):\n",
    "        assert use_loss or use_acc, 'use loss or (and) acc'\n",
    "        self.patience = patience\n",
    "        self.use_loss = use_loss\n",
    "        self.use_acc = use_acc\n",
    "        self.save_model = save_model\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_val_acc = 0\n",
    "        self.state_dict = None\n",
    "\n",
    "    def check(self, evals, model, epoch):\n",
    "        if self.use_loss and self.use_acc:\n",
    "            # For GAT, based on https://github.com/PetarV-/GAT/blob/master/execute_cora.py\n",
    "            if evals['val_loss'] <= self.best_val_loss or evals['val_acc'] >= self.best_val_acc:\n",
    "                if evals['val_loss'] <= self.best_val_loss and evals['val_acc'] >= self.best_val_acc:\n",
    "                    if self.save_model:\n",
    "                        self.state_dict = deepcopy(model.state_dict())\n",
    "                self.best_val_loss = min(self.best_val_loss, evals['val_loss'])\n",
    "                self.best_val_acc = max(self.best_val_acc, evals['val_acc'])\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        elif self.use_loss:\n",
    "            if evals['val_loss'] < self.best_val_loss:\n",
    "                self.best_val_loss = evals['val_loss']\n",
    "                self.counter = 0\n",
    "                if self.save_model:\n",
    "                    self.state_dict = deepcopy(model.state_dict())\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        elif self.use_acc:\n",
    "            if evals['val_acc'] > self.best_val_acc:\n",
    "                self.best_val_acc = evals['val_acc']\n",
    "                self.counter = 0\n",
    "                if self.save_model:\n",
    "                    self.state_dict = deepcopy(model.state_dict())\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        stop = False\n",
    "        if self.counter >= self.patience:\n",
    "            stop = True\n",
    "            if self.verbose:\n",
    "                print(\"Stop training, epoch:\", epoch)\n",
    "            if self.save_model:\n",
    "                model.load_state_dict(self.state_dict)\n",
    "        return stop\n",
    "\n",
    "\n",
    "def train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output[data.train_mask], data.labels[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "\n",
    "    outputs = {}\n",
    "    for key in ['train', 'val', 'test']:\n",
    "        if key == 'train':\n",
    "            mask = data.train_mask\n",
    "        elif key == 'val':\n",
    "            mask = data.val_mask\n",
    "        else:\n",
    "            mask = data.test_mask\n",
    "        loss = F.nll_loss(output[mask], data.labels[mask]).item()\n",
    "        pred = output[mask].max(dim=1)[1]\n",
    "        acc = pred.eq(data.labels[mask]).sum().item() / mask.sum().item()\n",
    "\n",
    "        outputs['{}_loss'.format(key)] = loss\n",
    "        outputs['{}_acc'.format(key)] = acc\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def run(data, model, lr, weight_decay, epochs=100000, niter=100, early_stopping=True, patience=100,\n",
    "        use_loss=True, use_acc=True, save_model=True, verbose=False):\n",
    "    # for GPU\n",
    "    data.to(device)\n",
    "\n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for _ in tqdm(range(niter)):\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        model.to(device).reset_parameters()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # for early stopping\n",
    "        if early_stopping:\n",
    "            stop_checker = EarlyStopping(patience, verbose, use_loss, use_acc, save_model)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, optimizer, data)\n",
    "            evals = evaluate(model, data)\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch: {: 4d}'.format(epoch),\n",
    "                      'train loss: {:.5f}'.format(evals['train_loss']),\n",
    "                      'train acc: {:.5f}'.format(evals['train_acc']),\n",
    "                      'val loss: {:.5f}'.format(evals['val_loss']),\n",
    "                      'val acc: {:.5f}'.format(evals['val_acc']))\n",
    "\n",
    "            if early_stopping:\n",
    "                if stop_checker.check(evals, model, epoch):\n",
    "                    break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        evals = evaluate(model, data)\n",
    "        if verbose:\n",
    "            for met, val in evals.items():\n",
    "                print(met, val)\n",
    "\n",
    "        val_acc_list.append(evals['val_acc'])\n",
    "        test_acc_list.append(evals['test_acc'])\n",
    "\n",
    "    print(\"mean\", mean(test_acc_list))\n",
    "    print(\"std\", std(test_acc_list))\n",
    "    return {\n",
    "        'val_acc': mean(val_acc_list),\n",
    "        'test_acc': mean(test_acc_list),\n",
    "        'test_acc_std': std(test_acc_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757401d-16fb-41e8-871e-c5bbe8de9d3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90915c9a-29b0-43a4-a7d0-db2634a563c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suagrawa\\AppData\\Local\\Temp\\ipykernel_18100\\1063022227.py:29: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  out = pkl.load(f, encoding='latin1')\n",
      "C:\\Users\\suagrawa\\AppData\\Local\\Temp\\ipykernel_18100\\1063022227.py:78: DeprecationWarning: \n",
      "\n",
      "The scipy.sparse array containers will be used instead of matrices\n",
      "in Networkx 3.0. Use `to_scipy_sparse_array` instead.\n",
      "  coo_adj = nx.to_scipy_sparse_matrix(G).tocoo()\n",
      " 50%|                                         | 5/10 [06:23<07:49, 93.87s/it]"
     ]
    }
   ],
   "source": [
    "#from gat import GAT\n",
    "#from spgat import SPGAT\n",
    "#from train import run\n",
    "#from data import load_data\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # load a data according to input\n",
    "    data = load_data('cora')\n",
    "\n",
    "    # create GAT model\n",
    "    # You can use the sparse version of GAT, which reduces computational time and memory consumption.\n",
    "    model = SPGAT(data)\n",
    "    # You can also use the dense version of GAT\n",
    "    # model = GAT(data)\n",
    "\n",
    "    # run the model niter times\n",
    "    run(data, model, lr=0.005, weight_decay=5e-4, niter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d2eab-4956-4305-9925-ed1837da2948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e056d8-e8ad-4cda-9952-a810bc8f04c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111070a-827a-40f7-8db6-d546bcd1baee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bd743-578d-4ed2-a28e-0ee027929c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc94df1-7009-4598-91f8-302f349dc48c",
   "metadata": {},
   "source": [
    "## 6. https://github.com/QData/LaMP\n",
    "#### arxiv.org/pdf/1904.08049.pdf \n",
    "(NEURAL MESSAGE PASSING FOR MULTI-LABEL CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec67bb0-3cc5-4f7f-8f5e-b4f83fb9f591",
   "metadata": {},
   "source": [
    "## 7. https://github.com/taishan1994/pytorch_gat/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832a3ad-2264-4813-bd81-d3c944010925",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e9fd5d-0fe4-40b9-a6fd-421bb531e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "#from scipy.sparse.linalg.eigen.arpack import eigsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9967778-614f-403a-991f-b2d20cdf0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn_head(nn.Module):\n",
    "  def __init__(self, \n",
    "        in_channel, \n",
    "        out_sz, \n",
    "        bias_mat, \n",
    "        in_drop=0.0, \n",
    "        coef_drop=0.0, \n",
    "        activation=None,\n",
    "        residual=False):\n",
    "    super(Attn_head, self).__init__() \n",
    "    self.in_channel = in_channel\n",
    "    self.out_sz = out_sz \n",
    "    self.bias_mat = bias_mat\n",
    "    self.in_drop = in_drop\n",
    "    self.coef_drop = coef_drop\n",
    "    self.activation = activation\n",
    "    self.residual = residual\n",
    "    \n",
    "    self.conv1 = nn.Conv1d(self.in_channel, self.out_sz, 1)\n",
    "    self.conv2_1 = nn.Conv1d(self.out_sz, 1, 1)\n",
    "    self.conv2_2 = nn.Conv1d(self.out_sz, 1, 1)\n",
    "    self.leakyrelu = nn.LeakyReLU()\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "    #pytorchdropoutp\n",
    "    self.in_dropout = nn.Dropout()\n",
    "    self.coef_dropout = nn.Dropout()\n",
    "    self.res_conv = nn.Conv1d(self.in_channel, self.out_sz, 1)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    seq = x\n",
    "    if self.in_drop != 0.0:\n",
    "      seq = self.in_dropout(x)\n",
    "    seq_fts = self.conv1(seq)\n",
    "    f_1 = self.conv2_1(seq_fts)\n",
    "    f_2 = self.conv2_2(seq_fts)\n",
    "    logits = f_1 + torch.transpose(f_2, 2, 1)\n",
    "    logits = self.leakyrelu(logits)\n",
    "    coefs = self.softmax(logits + self.bias_mat)\n",
    "    if self.coef_drop !=0.0:\n",
    "      coefs = self.coef_dropout(coefs)\n",
    "    if self.in_dropout !=0.0:\n",
    "      seq_fts = self.in_dropout(seq_fts)\n",
    "    ret = torch.matmul(coefs, torch.transpose(seq_fts, 2, 1))\n",
    "    ret = torch.transpose(ret, 2, 1)\n",
    "    if self.residual:\n",
    "      if seq.shape[1] != ret.shape[1]:\n",
    "        ret = ret + self.res_conv(seq)\n",
    "      else:\n",
    "        ret = ret + seq\n",
    "    return self.activation(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9785a065-79f4-49da-8b09-2fc934e1ca1b",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ffe234c-73a1-4bca-af14-a19d6357a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "  def __init__(self,\n",
    "      nb_classes, \n",
    "      nb_nodes, \n",
    "      attn_drop, \n",
    "      ffd_drop, \n",
    "      bias_mat, \n",
    "      hid_units, \n",
    "      n_heads, \n",
    "      residual=False):\n",
    "    super(GAT, self).__init__()  \n",
    "    self.nb_classes = nb_classes\n",
    "    self.nb_nodes = nb_nodes\n",
    "    self.attn_drop = attn_drop\n",
    "    self.ffd_drop = ffd_drop\n",
    "    self.bias_mat = bias_mat\n",
    "    self.hid_units = hid_units\n",
    "    self.n_heads = n_heads\n",
    "    self.residual = residual\n",
    "\n",
    "    self.attn1 = Attn_head(in_channel=1433, out_sz=self.hid_units[0],\n",
    "                bias_mat=self.bias_mat, in_drop=self.ffd_drop,\n",
    "                coef_drop=self.attn_drop, activation=nn.ELU(),\n",
    "                residual=self.residual)\n",
    "    self.attn2 = Attn_head(in_channel=64, out_sz=self.nb_classes,\n",
    "                bias_mat=self.bias_mat, in_drop=self.ffd_drop,\n",
    "                coef_drop=self.attn_drop, activation=nn.ELU(),\n",
    "                residual=self.residual)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    attns = []\n",
    "    for _ in range(self.n_heads[0]):\n",
    "      attns.append(self.attn1(x))\n",
    "    h_1 = torch.cat(attns, dim=1)\n",
    "    out = self.attn2(h_1)\n",
    "    logits = torch.transpose(out.view(self.nb_classes,-1), 1, 0)\n",
    "    logits = self.softmax(logits)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ccea8-3f0a-4e6e-8363-b2582e4c30af",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfa6a7d-5be5-4ac8-836c-7c10f39d151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\Source\\\\GNN-Tutorials\\\\data\\\\planetoid\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78542b62-4349-4e34-aebf-0b47e6eb2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.todense(), sparse_to_tuple(features)\n",
    "\n",
    "def adj_to_bias(adj, sizes, nhood=1):\n",
    "    nb_graphs = adj.shape[0]\n",
    "    mt = np.empty(adj.shape)\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
    "        for i in range(sizes[g]):\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data(dataset_str): # {'pubmed', 'citeseer', 'cora'}\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(path + \"ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(path + \"ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    #pytorchone-hot\n",
    "    my_labels = np.where(labels==1)[1]\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_my_labels_mask = sample_mask(idx_train, my_labels.shape[0])\n",
    "    val_my_labels_mask = sample_mask(idx_val, my_labels.shape[0])\n",
    "    test_my_labels_mask = sample_mask(idx_test, my_labels.shape[0])\n",
    "    train_my_labels = my_labels[train_my_labels_mask]\n",
    "    val_my_labels = my_labels[val_my_labels_mask]\n",
    "    test_my_labels = my_labels[test_my_labels_mask]\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    print(adj.shape)\n",
    "    print(features.shape)\n",
    "    data_dict = {\n",
    "      'adj': adj,\n",
    "      'features': features,\n",
    "      'y_train': y_train,\n",
    "      'y_val': y_val,\n",
    "      'y_test': y_test,\n",
    "      'train_mask': train_mask,\n",
    "      'val_mask': val_mask,\n",
    "      'test_mask': test_mask,\n",
    "      'train_my_labels': train_my_labels,\n",
    "      'val_my_labels': val_my_labels,\n",
    "      'test_my_labels': test_my_labels,\n",
    "      'my_labels': my_labels\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f62e60-4fe5-466e-912e-7fc40c294ab2",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3e87c2-a5b6-4e73-88bd-0e568b10fb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18c9710-5c3c-4264-9d31-3a5b4567744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 2708)\n",
      "(2708, 1433)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suagrawa\\AppData\\Local\\Temp\\ipykernel_30372\\1298871606.py:61: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n",
      "C:\\Users\\suagrawa\\AppData\\Local\\Temp\\ipykernel_30372\\1298871606.py:82: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
      "C:\\Users\\suagrawa\\AppData\\Local\\Temp\\ipykernel_30372\\1298871606.py:52: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(mask, dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "data = load_data(\"cora\")\n",
    "adj = data['adj']\n",
    "features = data['features']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "train_mask = data['train_mask']\n",
    "val_mask = data['val_mask']\n",
    "test_mask = data['test_mask']\n",
    "train_my_labels = data['train_my_labels']\n",
    "val_my_labels = data['val_my_labels']\n",
    "test_my_labels = data['test_my_labels']\n",
    "my_labels = data['my_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c558e2c3-593d-406f-ae02-f086c0e036f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, spars = preprocess_features(features)\n",
    "nb_nodes = features.shape[0]\n",
    "ft_sizes = features.shape[1]\n",
    "nb_classes = my_labels.shape[0]\n",
    "adj = adj.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90fc2350-7ff3-4198-a0e1-94a73ee22a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj[np.newaxis]\n",
    "features = features[np.newaxis]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b6348-5ee5-43fd-8970-d7723f47d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = torch.from_numpy(adj_to_bias(adj, [nb_nodes], nhood=1)).float().to(device)\n",
    "features = torch.from_numpy(features)\n",
    "features = torch.transpose(features,2,1).to(device)\n",
    "hid_units=[8]\n",
    "n_heads=[8, 1]\n",
    "epochs = 2400\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b63fed-4eae-4c92-a0cf-e0e0969da768",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GAT(nb_classes=nb_classes,\n",
    "      nb_nodes=nb_nodes, \n",
    "      attn_drop=0.0, \n",
    "      ffd_drop=0.0, \n",
    "      bias_mat=biases, \n",
    "      hid_units=hid_units, \n",
    "      n_heads=n_heads, \n",
    "      residual=False).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=gat.parameters(),lr=lr,betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef8ff6-ef11-4322-a17d-b2e9ae55155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_my_labels = torch.from_numpy(train_my_labels).long().to(device)\n",
    "val_my_labels = torch.from_numpy(val_my_labels).long().to(device)\n",
    "test_my_labels = torch.from_numpy(test_my_labels).long().to(device)\n",
    "\n",
    "train_mask = np.where(train_mask == 1)[0]\n",
    "val_mask = np.where(val_mask == 1)[0]\n",
    "test_mask = np.where(test_mask == 1)[0]\n",
    "train_mask = torch.from_numpy(train_mask).to(device)\n",
    "val_mask = torch.from_numpy(val_mask).to(device)\n",
    "test_mask = torch.from_numpy(test_mask).to(device)\n",
    "\n",
    "print(\"\", len(train_my_labels))\n",
    "print(\"\", len(val_my_labels))\n",
    "print(\"\", len(test_my_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6836d4-d1e4-472c-b591-da14022c60de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a16789-1e4c-44b0-b4b1-b6928eba1ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c763da59-8581-4be2-8dfe-ecbf22fc7d38",
   "metadata": {},
   "source": [
    "## 8. https://github.com/gordicaleksa/pytorch-GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1638b5ac-9901-4c20-96c1-5d5eb3a9c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import enum\n",
    "\n",
    "# Visualization related imports\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import igraph as ig\n",
    "\n",
    "# Main computation libraries\n",
    "import numpy as np\n",
    "\n",
    "# Deep learning related imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20aaff0e-b622-42cf-bc86-492d7d056c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(enum.Enum):\n",
    "    PPI = 0\n",
    "\n",
    "    \n",
    "class GraphVisualizationTool(enum.Enum):\n",
    "    IGRAPH = 0\n",
    "\n",
    "\n",
    "# We'll be dumping and reading the data from this directory\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\n",
    "PPI_PATH = os.path.join(DATA_DIR_PATH, 'ppi')\n",
    "PPI_URL = 'https://data.dgl.ai/dataset/ppi.zip'  # preprocessed PPI data from Deep Graph Library\n",
    "\n",
    "#\n",
    "# PPI specific constants\n",
    "#\n",
    "\n",
    "PPI_NUM_INPUT_FEATURES = 50\n",
    "PPI_NUM_CLASSES = 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a822331-84d4-4324-b979-aca74bed5beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Source\\\\GNN-Tutorials\\\\data\\\\ppi'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPI_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9ee17c-c951-45d5-880e-561b67a0bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_read(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cea8fd-8d44-4eb3-9e34-9d08a67ff620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data(training_config, device):\n",
    "    dataset_name = training_config['dataset_name'].lower()\n",
    "    should_visualize = training_config['should_visualize']\n",
    "\n",
    "    if dataset_name == DatasetType.PPI.name.lower():  # Protein-Protein Interaction dataset\n",
    "        #if not os.path.exists(PPI_PATH):  # download the first time this is ran\n",
    "            #os.makedirs(PPI_PATH)\n",
    "            #zip_tmp_path = os.path.join(PPI_PATH, 'ppi.zip')\n",
    "            #download_url_to_file(PPI_URL, zip_tmp_path)\n",
    "\n",
    "            #with zipfile.ZipFile(zip_tmp_path) as zf:\n",
    "                #zf.extractall(path=PPI_PATH)\n",
    "            #print(f'Unzipping to: {PPI_PATH} finished.')\n",
    "\n",
    "            #os.remove(zip_tmp_path)\n",
    "            #print(f'Removing tmp file {zip_tmp_path}.')\n",
    "\n",
    "        # Collect train/val/test graphs here\n",
    "        edge_index_list = []\n",
    "        node_features_list = []\n",
    "        node_labels_list = []\n",
    "\n",
    "        # Dynamically determine how many graphs we have per split (avoid using constants when possible)\n",
    "        num_graphs_per_split_cumulative = [0]\n",
    "\n",
    "        # Small optimization \"trick\" since we only need test in the playground.py\n",
    "        splits = ['test'] if training_config['ppi_load_test_only'] else ['train', 'valid', 'test']\n",
    "\n",
    "        for split in splits:\n",
    "            node_features = np.load(os.path.join(PPI_PATH, f'{split}_feats.npy'))\n",
    "\n",
    "            node_labels = np.load(os.path.join(PPI_PATH, f'{split}_labels.npy'))\n",
    "            nodes_links_dict = json_read(os.path.join(PPI_PATH, f'{split}_graph.json'))\n",
    "            collection_of_graphs = nx.DiGraph(json_graph.node_link_graph(nodes_links_dict))\n",
    "            graph_ids = np.load(os.path.join(PPI_PATH, F'{split}_graph_id.npy'))\n",
    "            num_graphs_per_split_cumulative.append(num_graphs_per_split_cumulative[-1] + len(np.unique(graph_ids)))\n",
    "\n",
    "            for graph_id in range(np.min(graph_ids), np.max(graph_ids) + 1):\n",
    "                mask = graph_ids == graph_id \n",
    "                graph_node_ids = np.asarray(mask).nonzero()[0]\n",
    "                graph = collection_of_graphs.subgraph(graph_node_ids)  # returns the induced subgraph over these nodes\n",
    "                print(f'Loading {split} graph {graph_id} to CPU. '\n",
    "                      f'It has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.')\n",
    "\n",
    "                edge_index = torch.tensor(list(graph.edges), dtype=torch.long).transpose(0, 1).contiguous()\n",
    "                edge_index = edge_index - edge_index.min()\n",
    "                edge_index_list.append(edge_index)\n",
    "                node_features_list.append(torch.tensor(node_features[mask], dtype=torch.float))\n",
    "                node_labels_list.append(torch.tensor(node_labels[mask], dtype=torch.float))\n",
    "\n",
    "                if should_visualize:\n",
    "                    plot_in_out_degree_distributions(edge_index.numpy(), graph.number_of_nodes(), dataset_name)\n",
    "                    visualize_graph(edge_index.numpy(), node_labels[mask], dataset_name)\n",
    "\n",
    "        if training_config['ppi_load_test_only']:\n",
    "            data_loader_test = GraphDataLoader(\n",
    "                node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                batch_size=training_config['batch_size'],\n",
    "                shuffle=False\n",
    "            )\n",
    "            return data_loader_test\n",
    "        else:\n",
    "\n",
    "            data_loader_train = GraphDataLoader(\n",
    "                node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\n",
    "                batch_size=training_config['batch_size'],\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "            data_loader_val = GraphDataLoader(\n",
    "                node_features_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\n",
    "                node_labels_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\n",
    "                edge_index_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\n",
    "                batch_size=training_config['batch_size'],\n",
    "                shuffle=False  # no need to shuffle the validation and test graphs\n",
    "            )\n",
    "\n",
    "            data_loader_test = GraphDataLoader(\n",
    "                node_features_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\n",
    "                node_labels_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\n",
    "                edge_index_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\n",
    "                batch_size=training_config['batch_size'],\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "            return data_loader_train, data_loader_val, data_loader_test\n",
    "    else:\n",
    "        raise Exception(f'{dataset_name} not yet supported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eff1a72-54f1-4eea-98cb-e0f2266672b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "    When dealing with batches it's always a good idea to inherit from PyTorch's provided classes (Dataset/DataLoader).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, node_features_list, node_labels_list, edge_index_list, batch_size=1, shuffle=False):\n",
    "        graph_dataset = GraphDataset(node_features_list, node_labels_list, edge_index_list)\n",
    "        # We need to specify a custom collate function, it doesn't work with the default one\n",
    "        super().__init__(graph_dataset, batch_size, shuffle, collate_fn=graph_collate_fn)\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This one just fetches a single graph from the split when GraphDataLoader \"asks\" it\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, node_features_list, node_labels_list, edge_index_list):\n",
    "        self.node_features_list = node_features_list\n",
    "        self.node_labels_list = node_labels_list\n",
    "        self.edge_index_list = edge_index_list\n",
    "\n",
    "    # 2 interface functions that need to be defined are len and getitem so that DataLoader can do it's magic\n",
    "    def __len__(self):\n",
    "        return len(self.edge_index_list)\n",
    "\n",
    "    def __getitem__(self, idx):  # we just fetch a single graph\n",
    "        return self.node_features_list[idx], self.node_labels_list[idx], self.edge_index_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edee1dc4-985d-441c-9bc0-c11fde17c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    The main idea here is to take multiple graphs from PPI as defined by the batch size\n",
    "    and merge them into a single graph with multiple connected components.\n",
    "\n",
    "    It's important to adjust the node ids in edge indices such that they form a consecutive range. Otherwise\n",
    "    the scatter functions in the implementation 3 will fail.\n",
    "\n",
    "    :param batch: contains a list of edge_index, node_features, node_labels tuples (as provided by the GraphDataset)\n",
    "    \"\"\"\n",
    "\n",
    "    edge_index_list = []\n",
    "    node_features_list = []\n",
    "    node_labels_list = []\n",
    "    num_nodes_seen = 0\n",
    "\n",
    "    for features_labels_edge_index_tuple in batch:\n",
    "        # Just collect these into separate lists\n",
    "        node_features_list.append(features_labels_edge_index_tuple[0])\n",
    "        node_labels_list.append(features_labels_edge_index_tuple[1])\n",
    "\n",
    "        edge_index = features_labels_edge_index_tuple[2]  # all of the components are in the [0, N] range\n",
    "        edge_index_list.append(edge_index + num_nodes_seen)  # very important! translate the range of this component\n",
    "        num_nodes_seen += len(features_labels_edge_index_tuple[1])  # update the number of nodes we've seen so far\n",
    "\n",
    "    # Merge the PPI graphs into a single graph with multiple connected components\n",
    "    node_features = torch.cat(node_features_list, 0)\n",
    "    node_labels = torch.cat(node_labels_list, 0)\n",
    "    edge_index = torch.cat(edge_index_list, 1)\n",
    "\n",
    "    return node_features, node_labels, edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5385905a-4a11-4a6f-ae32-831113492325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train graph 1 to CPU. It has 1767 nodes and 34085 edges.\n",
      "Loading train graph 2 to CPU. It has 1377 nodes and 31081 edges.\n",
      "Loading train graph 3 to CPU. It has 2263 nodes and 61907 edges.\n",
      "Loading train graph 4 to CPU. It has 2339 nodes and 67769 edges.\n",
      "Loading train graph 5 to CPU. It has 1578 nodes and 37740 edges.\n",
      "Loading train graph 6 to CPU. It has 1021 nodes and 19237 edges.\n",
      "Loading train graph 7 to CPU. It has 1823 nodes and 46153 edges.\n",
      "Loading train graph 8 to CPU. It has 2488 nodes and 72878 edges.\n",
      "Loading train graph 9 to CPU. It has 591 nodes and 8299 edges.\n",
      "Loading train graph 10 to CPU. It has 3312 nodes and 109510 edges.\n",
      "Loading train graph 11 to CPU. It has 2401 nodes and 66619 edges.\n",
      "Loading train graph 12 to CPU. It has 1878 nodes and 48146 edges.\n",
      "Loading train graph 13 to CPU. It has 1819 nodes and 47587 edges.\n",
      "Loading train graph 14 to CPU. It has 3480 nodes and 110234 edges.\n",
      "Loading train graph 15 to CPU. It has 2794 nodes and 88112 edges.\n",
      "Loading train graph 16 to CPU. It has 2326 nodes and 62188 edges.\n",
      "Loading train graph 17 to CPU. It has 2650 nodes and 79714 edges.\n",
      "Loading train graph 18 to CPU. It has 2815 nodes and 88335 edges.\n",
      "Loading train graph 19 to CPU. It has 3163 nodes and 97321 edges.\n",
      "Loading train graph 20 to CPU. It has 3021 nodes and 94359 edges.\n",
      "Loading valid graph 21 to CPU. It has 3230 nodes and 100676 edges.\n",
      "Loading valid graph 22 to CPU. It has 3284 nodes and 104758 edges.\n",
      "Loading test graph 23 to CPU. It has 3224 nodes and 103872 edges.\n",
      "Loading test graph 24 to CPU. It has 2300 nodes and 63628 edges.\n",
      "********************\n",
      "torch.Size([3163, 50]) torch.float32\n",
      "torch.Size([3163, 121]) torch.float32\n",
      "torch.Size([2, 97321]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Let's just define dummy visualization functions for now - just to stop Python interpreter from complaining!\n",
    "# We'll define them in a moment, properly, I swear.\n",
    "\n",
    "def plot_in_out_degree_distributions():\n",
    "    pass\n",
    "\n",
    "def visualize_graph():\n",
    "    pass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\n",
    "\n",
    "config = {\n",
    "    'dataset_name': DatasetType.PPI.name,\n",
    "    'should_visualize': False,\n",
    "    'batch_size': 1,\n",
    "    'ppi_load_test_only': False  # small optimization for loading test graphs only, we won't use it here\n",
    "}\n",
    "\n",
    "data_loader_train, data_loader_val, data_loader_test = load_graph_data(config, device)\n",
    "# Let's fetch a single batch from the train graph data loader\n",
    "node_features, node_labels, edge_index = next(iter(data_loader_train))\n",
    "\n",
    "print('*' * 20)\n",
    "print(node_features.shape, node_features.dtype)\n",
    "print(node_labels.shape, node_labels.dtype)\n",
    "print(edge_index.shape, edge_index.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c0321b-eb8b-4f2e-b0d6-05bdc7854c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_in_out_degree_distributions(edge_index, num_of_nodes, dataset_name):\n",
    "    \"\"\"\n",
    "        Note: It would be easy to do various kinds of powerful network analysis using igraph/networkx, etc.\n",
    "        I chose to explicitly calculate only the node degree statistics here, but you can go much further if needed and\n",
    "        calculate the graph diameter, number of triangles and many other concepts from the network analysis field.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(edge_index, torch.Tensor):\n",
    "        edge_index = edge_index.cpu().numpy()\n",
    "        \n",
    "    assert isinstance(edge_index, np.ndarray), f'Expected NumPy array got {type(edge_index)}.'\n",
    "\n",
    "    # Store each node's input and output degree (they're the same for undirected graphs such as Cora/PPI)\n",
    "    in_degrees = np.zeros(num_of_nodes, dtype=np.int)\n",
    "    out_degrees = np.zeros(num_of_nodes, dtype=np.int)\n",
    "\n",
    "    # Edge index shape = (2, E), the first row contains the source nodes, the second one target/sink nodes\n",
    "    # Note on terminology: source nodes point to target/sink nodes\n",
    "    num_of_edges = edge_index.shape[1]\n",
    "    for cnt in range(num_of_edges):\n",
    "        source_node_id = edge_index[0, cnt]\n",
    "        target_node_id = edge_index[1, cnt]\n",
    "\n",
    "        out_degrees[source_node_id] += 1  # source node points towards some other node -> increment it's out degree\n",
    "        in_degrees[target_node_id] += 1  # similarly here\n",
    "\n",
    "    hist = np.zeros(np.max(out_degrees) + 1)\n",
    "    for out_degree in out_degrees:\n",
    "        hist[out_degree] += 1\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8), dpi=100)  # otherwise plots are really small in Jupyter Notebook\n",
    "    fig.subplots_adjust(hspace=0.6)\n",
    "\n",
    "    plt.subplot(311)\n",
    "    plt.plot(in_degrees, color='red')\n",
    "    plt.xlabel('node id'); plt.ylabel('in-degree count'); plt.title('Input degree for different node ids')\n",
    "\n",
    "    plt.subplot(312)\n",
    "    plt.plot(out_degrees, color='green')\n",
    "    plt.xlabel('node id'); plt.ylabel('out-degree count'); plt.title('Out degree for different node ids')\n",
    "\n",
    "    plt.subplot(313)\n",
    "    plt.plot(hist, color='blue')\n",
    "    plt.xlabel('node degree'); plt.ylabel('# nodes for a given out-degree'); plt.title(f'Node out-degree distribution for {dataset_name} dataset')\n",
    "    plt.xticks(np.arange(0, len(hist), 20.0))\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce18850-97eb-4682-bfb1-34d4ab5b554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suagrawa\\AppData\\Local\\Temp\\ipykernel_4540\\3359927729.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  in_degrees = np.zeros(num_of_nodes, dtype=np.int)\n",
      "C:\\Users\\suagrawa\\AppData\\Local\\Temp\\ipykernel_4540\\3359927729.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  out_degrees = np.zeros(num_of_nodes, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_of_nodes = len(node_labels)\n",
    "plot_in_out_degree_distributions(edge_index, num_of_nodes, config['dataset_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2959ac3-d701-4c38-86bc-b1a2c1c49abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The most interesting and hardest implementation is implementation #3.\n",
    "    Imp1 and imp2 differ in subtle details but are basically the same thing.\n",
    "\n",
    "    So I'll focus on imp #3 in this notebook.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,\n",
    "                 dropout=0.6, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'\n",
    "\n",
    "        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below\n",
    "\n",
    "        gat_layers = []  # collect GAT layers\n",
    "        for i in range(num_of_layers):\n",
    "            layer = GATLayer(\n",
    "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
    "                num_out_features=num_features_per_layer[i+1],\n",
    "                num_of_heads=num_heads_per_layer[i+1],\n",
    "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
    "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
    "                dropout_prob=dropout,\n",
    "                add_skip_connection=add_skip_connection,\n",
    "                bias=bias,\n",
    "                log_attention_weights=log_attention_weights\n",
    "            )\n",
    "            gat_layers.append(layer)\n",
    "\n",
    "        self.gat_net = nn.Sequential(\n",
    "            *gat_layers,\n",
    "        )\n",
    "\n",
    "    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:\n",
    "    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n",
    "    def forward(self, data):\n",
    "        return self.gat_net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83761d4b-8e56-40dd-b5fc-89a996e788f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric\n",
    "\n",
    "    But, it's hopefully much more readable! (and of similar performance)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # We'll use these constants in many functions so just extracting them here as member fields\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    # These may change in the inductive setting - leaving it like this for now (not future proof)\n",
    "    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of \"N\" in tensor)\n",
    "    head_dim = 1       # attention head dim\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
    "        self.add_skip_connection = add_skip_connection\n",
    "\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the \"additive\" scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if add_skip_connection:\n",
    "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "        else:\n",
    "            self.register_parameter('skip_proj', None)\n",
    "\n",
    "        #\n",
    "        # End of trainable weights\n",
    "        #\n",
    "\n",
    "        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting\n",
    "        self.activation = activation\n",
    "        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection\n",
    "        # and for attention coefficients. Functionality-wise it's the same as using independent modules.\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights\n",
    "        self.attention_weights = None  # for later visualization purposes, I cache the weights here\n",
    "\n",
    "        self.init_params()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features, edge_index = data  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
    "\n",
    "        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node\n",
    "        # We apply the dropout to all of the input node features (as mentioned in the paper)\n",
    "        in_nodes_features = self.dropout(in_nodes_features)\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
    "\n",
    "        # shape = (E, NH, 1)\n",
    "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "        # Add stochasticity to neighborhood aggregation\n",
    "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "\n",
    "        #\n",
    "        # Step 3: Neighborhood aggregation\n",
    "        #\n",
    "\n",
    "        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul\n",
    "        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT\n",
    "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "        # This part sums up weighted and projected neighborhood feature vectors for every target node\n",
    "        # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "        return (out_nodes_features, edge_index)\n",
    "\n",
    "    #\n",
    "    # Helper functions (without comments there is very little code so don't be scared!)\n",
    "    #\n",
    "\n",
    "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
    "        \"\"\"\n",
    "        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.\n",
    "        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take\n",
    "        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3\n",
    "        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)\n",
    "        (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3\n",
    "         i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.\n",
    "\n",
    "        Note:\n",
    "        Subtracting the max value from logits doesn't change the end result but it improves the numerical stability\n",
    "        and it's a fairly common \"trick\" used in pretty much every deep learning framework.\n",
    "        Check out this link for more details:\n",
    "\n",
    "        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
    "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
    "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
    "\n",
    "        # Calculate the denominator. shape = (E, NH)\n",
    "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
    "\n",
    "        # 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the\n",
    "        # possibility of the computer rounding a very small number all the way to 0.\n",
    "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
    "\n",
    "        # shape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features\n",
    "        return attentions_per_edge.unsqueeze(-1)\n",
    "\n",
    "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
    "        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
    "\n",
    "        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads\n",
    "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes\n",
    "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
    "\n",
    "        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n",
    "        # target index)\n",
    "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
    "\n",
    "        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n",
    "        # all the locations where the source nodes pointed to i (as dictated by the target index)\n",
    "        # shape = (N, NH) -> (E, NH)\n",
    "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
    "\n",
    "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
    "        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
    "\n",
    "        # shape = (E) -> (E, NH, FOUT)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n",
    "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
    "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
    "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
    "\n",
    "        return out_nodes_features\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "    def explicit_broadcast(self, this, other):\n",
    "        # Append singleton dimensions until this.dim() == other.dim()\n",
    "        for _ in range(this.dim(), other.dim()):\n",
    "            this = this.unsqueeze(-1)\n",
    "\n",
    "        # Explicitly expand so that shapes are the same\n",
    "        return this.expand_as(other)\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
    "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\n",
    "            self.attention_weights = attention_coefficients\n",
    "\n",
    "        if self.add_skip_connection:  # add skip or residual connection\n",
    "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
    "                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH\n",
    "                # thus we're basically copying input vectors NH times and adding to processed vectors\n",
    "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
    "            else:\n",
    "                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output\n",
    "                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n",
    "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        if self.concat:\n",
    "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
    "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
    "        else:\n",
    "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
    "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out_nodes_features += self.bias\n",
    "\n",
    "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcf692-4c1c-4054-b4ac-7bd1cc26615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# 3 different model training/eval phases used in train.py\n",
    "class LoopPhase(enum.Enum):\n",
    "    TRAIN = 0,\n",
    "    VAL = 1,\n",
    "    TEST = 2\n",
    "\n",
    "    \n",
    "writer = SummaryWriter()  # (tensorboard) writer will output to ./runs/ directory by default\n",
    "\n",
    "\n",
    "# Global vars used for early stopping. After some number of epochs (as defined by the patience_period var) without any\n",
    "# improvement on the validation dataset (measured via micro-F1 metric), we'll break out from the training loop.\n",
    "BEST_VAL_MICRO_F1 = 0\n",
    "BEST_VAL_LOSS = 0\n",
    "PATIENCE_CNT = 0\n",
    "\n",
    "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\n",
    "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\n",
    "\n",
    "# Make sure these exist as the rest of the code assumes it\n",
    "os.makedirs(BINARIES_PATH, exist_ok=True)\n",
    "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82e475-410f-479c-a8fe-208c5dda4ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Optimization-Python",
   "language": "python",
   "name": "optimization-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
