{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c1121-4e1a-47a9-88e6-d3630be2a16a",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f1a4b7-28eb-41f2-ab32-5cd4ad731bed",
   "metadata": {},
   "source": [
    "Transductive - you have a single graph (like Cora) you split some nodes (and not graphs) into train/val/test training sets. While you're training you'll be using only the labels from your training nodes. BUT. During the forward prop, by the nature of how spatial GNNs work, you'll be aggregating the feature vectors from your neighbors and some of them may belong to val or even test sets! The main point is - you ARE NOT using their label information but you ARE using the structural information and their features.\n",
    "\n",
    "Inductive - you're probably much more familiar with this one if you come from the computer vision or NLP background. You have a set of training graphs, a separate set of val graphs and of course a separate set of test graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1841ec8-a984-49d7-a978-78892f98e8b6",
   "metadata": {},
   "source": [
    "## 1. https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c239446-da79-4727-8bee-60e375ed0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "#import seaborn as sns\n",
    "#sns.reset_orig()\n",
    "#sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf2a69-6616-4dad-b1f5-943a16ee60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The \n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha - Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        \n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "        \n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        \n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # One per head\n",
    "        \n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "        \n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "        \n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "        \n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix \n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        \n",
    "        edges = adj_matrix.nonzero(as_tuple=False) # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
    "        \n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "        \n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a) \n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "        \n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "        \n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "        \n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "        \n",
    "        return node_feats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec9ace-6edb-4f1c-8135-32fd896fdf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e6d71-8577-4643-b2bd-51a98692d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = GATLayer(2, 2, num_heads=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43733e2b-8285-4790-8301-3283c22762e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4defc3b6-6fb6-4652-b5c2-1cdfdfbd07b3",
   "metadata": {},
   "source": [
    "## 2 Basic GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9924da8-9a70-4d39-9959-3a2ddbf97600",
   "metadata": {},
   "source": [
    "### 2.1 https://towardsdatascience.com/graph-attention-networks-under-the-hood-3bd70dc7a87 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2c9a0-c123-48bc-bfdc-f0b0c085f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z):\n",
    "    return np.where(z > 0, z, z * 0.01)\n",
    "\n",
    "def softmax(z):\n",
    "    if len(z.shape) > 1:\n",
    "        # Softmax for matrix\n",
    "        max_matrix = np.max(z, axis=0)\n",
    "        stable_z = z - max_matrix\n",
    "        e = np.exp(stable_z)\n",
    "        a = e / np.sum(e, axis=0, keepdims=True)\n",
    "    else:\n",
    "        # Softmax for vector\n",
    "        vector_max_value = np.max(z)\n",
    "        a = (np.exp(z - vector_max_value)) / sum(np.exp(z - vector_max_value))\n",
    "\n",
    "    assert a.shape == z.shape\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f59fa-61aa-47ac-bdee-fde634a3e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n----- One-hot vector representation of nodes. Shape(n,n)\\n')\n",
    "X = np.eye(5, 5)\n",
    "n = X.shape[0]\n",
    "np.random.shuffle(X)\n",
    "print(X)\n",
    "\n",
    "print('\\n\\n----- Embedding dimension\\n')\n",
    "emb = 3\n",
    "print(emb)\n",
    "\n",
    "print('\\n\\n----- Weight Matrix. Shape(emb, n)\\n')\n",
    "W = np.random.uniform(-np.sqrt(1. / emb), np.sqrt(1. / emb), (emb, n))\n",
    "print(W)\n",
    "\n",
    "print('\\n\\n----- Adjacency Matrix (undirected graph). Shape(n,n)\\n')\n",
    "A = np.random.randint(2, size=(n, n))\n",
    "np.fill_diagonal(A, 1)  \n",
    "A = (A + A.T)\n",
    "A[A > 1] = 1\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a6d55-7c68-4ac8-b15f-441bf4e6e374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d989ca-60b0-4722-aec6-fc585093c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equation (1)\n",
    "print('\\n\\n----- Linear Transformation. Shape(n, emb)\\n')\n",
    "z1 = X.dot(W.T)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8838221-f2c3-4b77-a148-54ce49f52283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equation (2) - First part\n",
    "print('\\n\\n----- Concat hidden features to represent edges. Shape(len(emb.concat(emb)), number of edges)\\n')\n",
    "edge_coords = np.where(A==1)\n",
    "h_src_nodes = z1[edge_coords[0]]\n",
    "h_dst_nodes = z1[edge_coords[1]]\n",
    "z2 = np.concatenate((h_src_nodes, h_dst_nodes), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6711d6ca-f1a6-4763-90d9-c16dc779d106",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# equation (2) - Second part\n",
    "print('\\n\\n----- Attention coefficients. Shape(1, len(emb.concat(emb)))\\n')\n",
    "att = np.random.rand(1, z2.shape[1])\n",
    "print(att)\n",
    "\n",
    "print('\\n\\n----- Edge representations combined with the attention coefficients. Shape(1, number of edges)\\n')\n",
    "z2_att = z2.dot(att.T)\n",
    "print(z2_att)\n",
    "\n",
    "print('\\n\\n----- Leaky Relu. Shape(1, number of edges)')\n",
    "e = leaky_relu(z2_att)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414b74c-b8cc-42c7-8402-7eae50fceb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21e540-6113-4acd-b389-b57679566f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# equation (3)\n",
    "print('\\n\\n----- Edge scores as matrix. Shape(n,n)\\n')\n",
    "e_matr = np.zeros(A.shape)\n",
    "e_matr[edge_coords[0], edge_coords[1]] = e.reshape(-1,)\n",
    "print(e_matr)\n",
    "\n",
    "print('\\n\\n----- For each node, normalize the edge (or neighbor) contributions using softmax\\n')\n",
    "alpha0 = softmax(e_matr[:,0][e_matr[:,0] != 0]) \n",
    "alpha1 = softmax(e_matr[:,1][e_matr[:,1] != 0])\n",
    "alpha2 = softmax(e_matr[:,2][e_matr[:,2] != 0])\n",
    "alpha3 = softmax(e_matr[:,3][e_matr[:,3] != 0])\n",
    "alpha4 = softmax(e_matr[:,4][e_matr[:,4] != 0])\n",
    "alpha = np.concatenate((alpha0, alpha1, alpha2, alpha3, alpha4))\n",
    "print(alpha)\n",
    "\n",
    "print('\\n\\n----- Normalized edge score matrix. Shape(n,n)\\n')\n",
    "A_scaled = np.zeros(A.shape)\n",
    "A_scaled[edge_coords[0], edge_coords[1]] = alpha.reshape(-1,)\n",
    "print(A_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a68105-817e-4a89-a7a8-7aafeae27568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equation (4)\n",
    "print('\\n\\nNeighborhood aggregation (GCN) scaled with attention scores (GAT). Shape(n, emb)\\n')\n",
    "ND_GAT = A_scaled.dot(z1)\n",
    "print(ND_GAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2ed04-a0ef-44b8-b6c6-9a4221d31463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14547718-6291-4c3c-a4be-593e88e89707",
   "metadata": {},
   "source": [
    "### 2.2 https://github.com/johncava/pytorch-GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba19a0-baee-471c-8066-fa4e45507eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0e264-a301-477b-9ddf-2fdde43b14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(graph):\n",
    "    neighbors = []\n",
    "    for node in graph:\n",
    "        node_neighbors = []\n",
    "        for index, val in enumerate(node):\n",
    "            if val == 1:\n",
    "                node_neighbors.append(index)\n",
    "        neighbors.append(node_neighbors)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420bf43-d03d-41b0-afb0-a649e5528a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy graph structure as a symmetric graph\n",
    "graph = [[1,0,0,0,0,0,0,0,0,0],\n",
    "         [0,1,0,0,0,0,0,0,0,0],\n",
    "         [1,0,1,0,0,0,0,0,0,0],\n",
    "         [0,1,1,1,0,0,0,0,0,0],\n",
    "         [1,1,0,0,1,0,0,0,0,0],\n",
    "         [0,0,1,0,0,1,0,0,0,0],\n",
    "         [1,1,0,0,0,0,1,0,0,0],\n",
    "         [0,0,1,0,1,1,0,1,0,0],\n",
    "         [1,0,0,1,0,1,0,0,1,0],\n",
    "         [0,0,0,1,0,0,1,0,0,1]]\n",
    "\n",
    "# Toy label\n",
    "label = [[1,1],[1,1],[1,1],[1,1],[1,1],[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
    "\n",
    "# Turn array into numpy array\n",
    "graph = np.array(graph)\n",
    "\n",
    "# Turn symmetric graph into an adjacency graph\n",
    "graph = graph + graph.T - np.eye(10)\n",
    "\n",
    "# Random feature matrix for the graph\n",
    "features = np.random.rand(10,10) * 10\n",
    "\n",
    "# Turn features into pytorch Variable\n",
    "features = Variable(torch.Tensor(features))\n",
    "\n",
    "# Turn label into pytorch Variable\n",
    "label = Variable(torch.Tensor(label))\n",
    "\n",
    "# Get neighbors for attention model\n",
    "neighbors = get_neighbors(graph)\n",
    "\n",
    "# Define W_out which would be equal in this case to the number of features of the label dataset => 2\n",
    "W_out = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a016396-77c9-44ae-9d62-ef3245e47e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Attention Model\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        # Linear Function that takes the features (h_i) and turns it into new features (new_h_i)\n",
    "        self.W = nn.Linear(10,W_out)\n",
    "        # Note: Attention Mechanism takes twice the output of Linear Function (W) because of the concatentation of Wh_i and Wh_j (Wh_i || Wh_j)\n",
    "        self.a = nn.Linear(2*W_out,1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # List to hold the new h_i values calculated from the attention mechanism\n",
    "        new_h_list = []\n",
    "        # Go through each node and perform attention in respect to its neighbors (which has been computed previously)\n",
    "        for primary_index,primary_node in enumerate(neighbors):\n",
    "            h = []\n",
    "            W_hjs = []\n",
    "            e = torch.Tensor([])\n",
    "            # Reference Equation (1),(3) : e_ij = a(Wh_i, Wh_j) = Leaky_Relu(attention(Wh_i, Wh_j)) => Neural_Network( Wh_i || Wh_j )\n",
    "            for neighbor in primary_node:\n",
    "                # Neighbor node features matrix multiplied with W. Also stored for future use when multiplying against alphas in line 75\n",
    "                W_hj = self.W(features[neighbor])\n",
    "                # Note: concatenation of e_ij into a single torch tensor such that there is one line to do F.softmax(e) in line 70\n",
    "                e = torch.cat((e,self.leaky_relu(self.a(torch.cat((self.W(x[primary_index]),W_hj))))))\n",
    "                W_hjs.append(W_hj)\n",
    "            # Softmax(e_ij) Reference: Equation (2)\n",
    "            a = F.softmax(e)\n",
    "            # Reference: Equation (4)\n",
    "            new_h = torch.Tensor([0.0]*W_out)\n",
    "            for a_ij, w_hj in zip(a,W_hjs):\n",
    "                new_h += a_ij * w_hj\n",
    "            new_h_list.append(F.leaky_relu(new_h))\n",
    "            ######################################\n",
    "        return torch.stack(new_h_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abce8ea-3fba-4eb7-b1f4-bf0093b7984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Attention Model\n",
    "attention = Attention()\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(attention.parameters(), lr=1e-3)\n",
    "max_iterations = 10\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    prediction = attention(features)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(prediction,label)\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90015579-714a-45ab-872a-57657b531d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff7ed268-398b-447b-bb74-5ab32c569f9c",
   "metadata": {},
   "source": [
    "## 3. https://dsgiitr.com/blogs/gat/\n",
    "### Implementing GAT Layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0bc40-8a1a-4b50-9f49-642e21818e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(2020) # seed for reproducible numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae8efe-64e0-4f46-b4f8-e4c73204ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple PyTorch Implementation of the Graph Attention layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.dropout       = dropout        # drop prob = 0.6\n",
    "        self.in_features   = in_features    # \n",
    "        self.out_features  = out_features   # \n",
    "        self.alpha         = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n",
    "        self.concat        = concat         # conacat = True for all layers except the output layer.\n",
    "\n",
    "        # Xavier Initialization of Weights\n",
    "        # Alternatively use weights_init to apply weights of choice \n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # Linear Transformation\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        # Attention Mechanism\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        # Masked Attention\n",
    "        zero_vec  = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        \n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime   = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87ed97-138a-4692-96e2-775bbca0aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "name_data = 'Cora'\n",
    "dataset = Planetoid(root= 'data/' + name_data, name = name_data)\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "\n",
    "print(f\"Number of Classes in {name_data}:\", dataset.num_classes)\n",
    "print(f\"Number of Node Features in {name_data}:\", dataset.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e5be8-42f1-4838-ac32-b45efc068e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "        \n",
    "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,\n",
    "                             heads=self.out_head, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Dropout before the GAT layer is used to avoid overfitting in small datasets like Cora.\n",
    "        # One can skip them if the dataset is sufficiently large.\n",
    "        \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818626d9-000f-4315-b8ab-dec732569790",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GAT().to(device)\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    if epoch%200 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c541fa-27a6-4c70-86e6-7c75aafa2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a735651-d9b6-4fce-ad2f-2806ad35305b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fbcf5-a33c-4e79-a472-664b9a038382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "471c984e-24ee-4dd9-9b1d-0647818d7419",
   "metadata": {},
   "source": [
    "## 4. https://github.com/raunakkmr/Graph-Attention-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a8bab-b3f8-44a9-9474-f1e9c424da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "from math import ceil\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75490371-de29-4d08-a248-d41e600062b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_heads, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.fcs = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_heads)])\n",
    "        self.a = nn.ModuleList([nn.Linear(2*output_dim, 1) for _ in range(num_heads)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, features, nodes, mapping, rows):\n",
    " \n",
    "\n",
    "        nprime = features.shape[0]\n",
    "        \n",
    "        rows = [np.array([mapping[v] for v in row], dtype=np.int64) for row in rows]\n",
    "        sum_degs = np.hstack(([0], np.cumsum([len(row) for row in rows])))\n",
    "        \n",
    "        mapped_nodes = [mapping[v] for v in nodes]\n",
    "        indices = torch.LongTensor([[v, c] for (v, row) in zip(mapped_nodes, rows) for c in row]).t()\n",
    "\n",
    "\n",
    "        out = []\n",
    "        for k in range(self.num_heads):\n",
    "            h = self.fcs[k](features)\n",
    "\n",
    "            nbr_h = torch.cat(tuple([h[row] for row in rows]), dim=0) # Neighbour\n",
    "            self_h = torch.cat(tuple([h[mapping[nodes[i]]].repeat(len(row), 1) for (i, row) in enumerate(rows)]), dim=0)\n",
    "            \n",
    "            cat_h = torch.cat((self_h, nbr_h), dim=1)\n",
    "\n",
    "            e = self.leakyrelu(self.a[k](cat_h))\n",
    "\n",
    "            alpha = [self.softmax(e[lo : hi]) for (lo, hi) in zip(sum_degs, sum_degs[1:])]\n",
    "            alpha = torch.cat(tuple(alpha), dim=0)\n",
    "            \n",
    "            alpha = alpha.squeeze(1)\n",
    "            alpha = self.dropout(alpha)\n",
    "\n",
    "            adj = torch.sparse.FloatTensor(indices, alpha, torch.Size([nprime, nprime]))\n",
    "            out.append(torch.sparse.mm(adj, h)[mapped_nodes])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33c2c6-9060-4b8b-9e1c-7d449cc5d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, num_heads,\n",
    "                 dropout=0.5, device='cpu'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.num_layers = len(hidden_dims) + 1\n",
    "\n",
    "        dims = [input_dim] + [d*nh for (d, nh) in zip(hidden_dims, num_heads[:-1])] + [output_dim*num_heads[-1]]\n",
    "        in_dims = dims[:-1]\n",
    "        out_dims = [d // nh for (d, nh) in zip(dims[1:], num_heads)]\n",
    "\n",
    "        self.attn = nn.ModuleList([GraphAttention(i, o, nh, dropout) for (i, o, nh) in zip(in_dims, out_dims, num_heads)])\n",
    "\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(dim) for dim in dims[1:-1]])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, features, node_layers, mappings, rows):\n",
    "\n",
    "        out = features\n",
    "        for k in range(self.num_layers):\n",
    "            nodes = node_layers[k+1]\n",
    "            mapping = mappings[k]\n",
    "            \n",
    "            init_mapped_nodes = np.array([mappings[0][v] for v in nodes], dtype=np.int64)\n",
    "            \n",
    "            cur_rows = rows[init_mapped_nodes]\n",
    "            out = self.dropout(out)\n",
    "            \n",
    "            out = self.attn[k](out, nodes, mapping, cur_rows)\n",
    "            \n",
    "            if k+1 < self.num_layers:\n",
    "                out = [self.elu(o) for o in out]\n",
    "                out = torch.cat(tuple(out), dim=1)\n",
    "                out = self.bns[k](out)\n",
    "                \n",
    "            else:\n",
    "                out = torch.cat(tuple([x.flatten().unsqueeze(0) for x in out]), dim=0)\n",
    "                out = out.mean(dim=0).reshape(len(nodes), self.output_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d781975-64bc-4cd6-9748-6bef3f3de79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dict = {\n",
    "    \"stats_per_batch\" : 3,\n",
    "    \"dataset\" : \"cora\",\n",
    "    \"dataset_path\" : \"data/cora/\",\n",
    "    \"mode\" : \"train\",\n",
    "    \"task\" : \"node_classification\",\n",
    "    \"cuda\" : \"True\",\n",
    "    \"hidden_dims\" : [8],\n",
    "    \"num_heads\" : [8, 1],\n",
    "    \"dropout\" : 0.6,\n",
    "    \"batch_size\" : 140,\n",
    "    \"epochs\" : 200,\n",
    "    \"lr\" : 5e-2,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"transductive\" : \"True\",\n",
    "    \"self_loop\" : \"True\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a2c87-e1cf-4996-ad28-5c6dc13dfbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cora(Dataset):\n",
    "\n",
    "    def __init__(self, path, mode, num_layers,\n",
    "                 self_loop=False, normalize_adj=False, transductive=False):\n",
    "\n",
    "        super(Cora, self).__init__()\n",
    "\n",
    "        self.path = path\n",
    "        self.mode = mode\n",
    "        self.num_layers = num_layers\n",
    "        self.self_loop = self_loop\n",
    "        self.normalize_adj = normalize_adj\n",
    "        self.transductive = transductive\n",
    "        self.idx = {\n",
    "            'train' : np.array(range(140)),\n",
    "            'val' : np.array(range(200, 500)),\n",
    "            'test' : np.array(range(500, 1500))\n",
    "        }\n",
    "\n",
    "        print('--------------------------------')\n",
    "        print('Reading cora dataset from {}'.format(path))\n",
    "        citations = np.loadtxt(os.path.join(path, 'cora.cites'), dtype=np.int64)\n",
    "        content = np.loadtxt(os.path.join(path, 'cora.content'), dtype=str)\n",
    "        print('Finished reading data.')\n",
    "\n",
    "        print('Setting up data structures.')\n",
    "        if transductive:\n",
    "            idx = np.arange(content.shape[0])\n",
    "        else:\n",
    "            if mode == 'train':\n",
    "                idx = self.idx['train']\n",
    "                \n",
    "            elif mode == 'val':\n",
    "                idx = np.hstack((self.idx['train'], self.idx['val']))\n",
    "                \n",
    "            elif mode == 'test':\n",
    "                idx = np.hstack((self.idx['train'], self.idx['test']))\n",
    "                \n",
    "        features, labels = content[idx, 1:-1].astype(np.float32), content[idx, -1]\n",
    "        d = {j : i for (i,j) in enumerate(sorted(set(labels)))}\n",
    "        labels = np.array([d[l] for l in labels])\n",
    "\n",
    "        vertices = np.array(content[idx, 0], dtype=np.int64)\n",
    "        d = {j : i for (i,j) in enumerate(vertices)}\n",
    "        \n",
    "        edges = np.array([e for e in citations if e[0] in d.keys() and e[1] in d.keys()])\n",
    "        edges = np.array([d[v] for v in edges.flatten()]).reshape(edges.shape)\n",
    "        \n",
    "        n, m = labels.shape[0], edges.shape[0]\n",
    "        u, v = edges[:, 0], edges[:, 1]\n",
    "        \n",
    "        adj = sp.coo_matrix((np.ones(m), (u, v)),\n",
    "                            shape=(n, n),\n",
    "                            dtype=np.float32)\n",
    "        \n",
    "        adj += adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj) # Building a symmetric adjacency matrix of a given sparse matrix\n",
    "        \n",
    "        if self_loop:\n",
    "            adj += sp.eye(n)\n",
    "            \n",
    "        if normalize_adj:\n",
    "            degrees = np.power(np.array(np.sum(adj, axis=1)), -0.5).flatten()\n",
    "            degrees = sp.diags(degrees)\n",
    "            adj = (degrees.dot(adj.dot(degrees)))\n",
    "        print('Finished setting up data structures.')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.adj = adj.tolil()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx[self.mode])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.transductive:\n",
    "            idx += int(self.idx[self.mode][0])\n",
    "        else:\n",
    "            if self.mode != 'train':\n",
    "                idx += len(self.idx['train'])\n",
    "                \n",
    "                \n",
    "        node_layers, mappings = self._form_computation_graph(idx)\n",
    "        rows = self.adj.rows[node_layers[0]]\n",
    "        \n",
    "        features = self.features[node_layers[0], :]\n",
    "        labels = self.labels[node_layers[-1]]\n",
    "        \n",
    "        features = torch.FloatTensor(features)\n",
    "        labels = torch.LongTensor(labels)\n",
    "\n",
    "        return features, node_layers, mappings, rows, labels\n",
    "\n",
    "    def collate_wrapper(self, batch):\n",
    "\n",
    "        idx = [node_layers[-1][0] for node_layers in [sample[1] for sample in batch]]\n",
    "\n",
    "        \n",
    "        node_layers, mappings = self._form_computation_graph(idx)\n",
    "        \n",
    "        rows = self.adj.rows[node_layers[0]]       \n",
    "        features = self.features[node_layers[0], :]\n",
    "        labels = self.labels[node_layers[-1]]\n",
    "        \n",
    "        features = torch.FloatTensor(features)\n",
    "        labels = torch.LongTensor(labels)\n",
    "\n",
    "        return features, node_layers, mappings, rows, labels\n",
    "\n",
    "    def get_dims(self):\n",
    "\n",
    "        return self.features.shape[1], len(set(self.labels))\n",
    "\n",
    "    def _form_computation_graph(self, idx):\n",
    "\n",
    "        _list, _set = list, set\n",
    "        rows = self.adj.rows\n",
    "        \n",
    "        if type(idx) is int:\n",
    "            node_layers = [np.array([idx], dtype=np.int64)]\n",
    "        elif type(idx) is list:\n",
    "            node_layers = [np.array(idx, dtype=np.int64)]\n",
    "            \n",
    "        for _ in range(self.num_layers):\n",
    "            prev = node_layers[-1]\n",
    "            arr = [node for node in prev]\n",
    "            arr.extend([v for node in arr for v in rows[node]])\n",
    "            arr = np.array(_list(_set(arr)), dtype=np.int64)\n",
    "            node_layers.append(arr)\n",
    "        node_layers.reverse()\n",
    "\n",
    "        mappings = [{j : i for (i,j) in enumerate(arr)} for arr in node_layers]\n",
    "\n",
    "        return node_layers, mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e58500-9832-4ef6-b5a7-47d69009a6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00df7ff-698e-4a96-8c09-7691f557ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import node_classification\n",
    "\n",
    "def get_criterion(task):\n",
    "\n",
    "    if task == 'node_classification':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return criterion\n",
    "\n",
    "def get_dataset(args):\n",
    "\n",
    "    task, dataset_name, *dataset_args = args\n",
    "    if task == 'node_classification':\n",
    "        if dataset_name == 'cora':\n",
    "            dataset = Cora(*dataset_args)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_fname(config):\n",
    "\n",
    "    hidden_dims_str = '_'.join([str(x) for x in config['hidden_dims']])\n",
    "    num_heads_str = '_'.join([str(x) for x in config['num_heads']])\n",
    "    batch_size = config['batch_size']\n",
    "    epochs = config['epochs']\n",
    "    lr = config['lr']\n",
    "    weight_decay = config['weight_decay']\n",
    "    dropout = config['dropout']\n",
    "    transductive = str(config['transductive'])\n",
    "    fname = 'gat_hidden_dims_{}_num_heads_{}_batch_size_{}_epochs_{}_lr_{}_weight_decay_{}_dropout_{}_transductive_{}.pth'.format(\n",
    "        hidden_dims_str, num_heads_str, batch_size, epochs, lr,\n",
    "        weight_decay, dropout, transductive)\n",
    "\n",
    "    return fname\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--json', type=str, default='config.json',\n",
    "                        help='path to json file with arguments, default: config.json')\n",
    "\n",
    "    parser.add_argument('--print_every', type=int, default=16,\n",
    "                        help='print loss and accuracy after how many batches, default: 16')\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, choices=['cora'], default='cora',\n",
    "                        help='name of the dataset, default=cora')\n",
    "    parser.add_argument('--dataset_path', type=str,\n",
    "                        default='/Users/raunak/Documents/Datasets/Cora', \n",
    "                        help='path to dataset')\n",
    "    parser.add_argument('--self_loop', action='store_true',\n",
    "                        help='whether to add self loops to adjacency matrix, default=False')\n",
    "    parser.add_argument('--normalize_adj', action='store_true',\n",
    "                        help='whether to normalize adj like in gcn, default=False')\n",
    "    parser.add_argument('--transductive', action='store_true',\n",
    "                        help='whether to use all nodes while training, default=False')\n",
    "\n",
    "    parser.add_argument('--task', type=str,\n",
    "                        choices=['unsupervised', 'node_classification'],\n",
    "                        default='node_classification',\n",
    "                        help='type of task, default=node_classification')\n",
    "\n",
    "    parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                        help='dropout parameter, default=0.5.')\n",
    "    parser.add_argument('--cuda', action='store_true',\n",
    "                        help='whether to use GPU, default: False')\n",
    "    parser.add_argument('--hidden_dims', type=int, nargs=\"*\",\n",
    "                        help='dimensions of hidden layers, specify through config.json')\n",
    "    parser.add_argument('--num_heads', type=int, nargs=\"*\",\n",
    "                        help='number of attention heads in each layer, length should be equal to len(hidden_dims)+1, specify through config.json')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=8,\n",
    "                        help='training batch size, default=8')\n",
    "    parser.add_argument('--epochs', type=int, default=10,\n",
    "                        help='number of training epochs, default=10')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        help='learning rate, default=1e-3')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                        help='weight decay, default=5e-4')\n",
    "\n",
    "    parser.add_argument('--save', action='store_true',\n",
    "                        help='whether to save model in trained_models/ directory, default: False')\n",
    "    parser.add_argument('--load', action='store_true',\n",
    "                        help='whether to load model in trained_models/ directory')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    config = vars(args)\n",
    "\n",
    "    config.update(json_dict)\n",
    "\n",
    "    config['num_layers'] = len(config['hidden_dims']) + 1\n",
    "\n",
    "    print('--------------------------------')\n",
    "    print('Config:')\n",
    "    for (k, v) in config.items():\n",
    "        print(\"    '{}': '{}'\".format(k, v))\n",
    "    print('--------------------------------')\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98154a-0bf6-4c79-a658-af3538809ca8",
   "metadata": {},
   "source": [
    "###### https://pytorch.org/docs/stable/data.html#dataloader-collate-fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59bce7b-4e79-42d1-b6eb-df119ea0a67d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = parse_args()\n",
    "\n",
    "    #if config['cuda'] and torch.cuda.is_available():\n",
    "    #    device = 'cuda:0'\n",
    "    #else:\n",
    "    device = 'cpu'\n",
    "\n",
    "    dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                    'train', config['num_layers'], config['self_loop'],\n",
    "                    config['normalize_adj'], config['transductive'])\n",
    "    \n",
    "    dataset = get_dataset(dataset_args)\n",
    "    \n",
    "    loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                        shuffle=True, collate_fn=dataset.collate_wrapper)\n",
    "    \n",
    "    input_dim, output_dim = dataset.get_dims()\n",
    "\n",
    "    model = GAT(input_dim, config['hidden_dims'], output_dim,\n",
    "                       config['num_heads'], config['dropout'], device)\n",
    "    model.to(device)\n",
    "\n",
    "    if not config['load']:\n",
    "        criterion = get_criterion(config['task'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                               weight_decay=config['weight_decay'])\n",
    "        \n",
    "        epochs = config['epochs']\n",
    "        stats_per_batch = config['stats_per_batch']\n",
    "        num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "        \n",
    "        model.train()\n",
    "        print('--------------------------------')\n",
    "        print('Training.')\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch {} / {}'.format(epoch+1, epochs))\n",
    "            running_loss = 0.0\n",
    "            num_correct, num_examples = 0, 0\n",
    "            \n",
    "            for (idx, batch) in enumerate(loader):\n",
    "                features, node_layers, mappings, rows, labels = batch\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(features, node_layers, mappings, rows)\n",
    "                loss = criterion(out, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    running_loss += loss.item()\n",
    "                    predictions = torch.max(out, dim=1)[1]\n",
    "                    num_correct += torch.sum(predictions == labels).item()\n",
    "                    num_examples += len(labels)\n",
    "                    \n",
    "                if (idx + 1) % stats_per_batch == 0:\n",
    "                    running_loss /= stats_per_batch\n",
    "                    accuracy = num_correct / num_examples\n",
    "                    print('    Batch {} / {}: loss {}, accuracy {}'.format(\n",
    "                        idx+1, num_batches, running_loss, accuracy))\n",
    "                    running_loss = 0.0\n",
    "                    num_correct, num_examples = 0, 0\n",
    "        print('Finished training.')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        if config['save']:\n",
    "            print('--------------------------------')\n",
    "            directory = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                                    'trained_models')\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            fname = get_fname(config)\n",
    "            path = os.path.join(directory, fname)\n",
    "            print('Saving model at {}'.format(path))\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print('Finished saving model.')\n",
    "            print('--------------------------------')\n",
    "        \n",
    "        \n",
    "    dataset_args = (config['task'], config['dataset'], config['dataset_path'],\n",
    "                    'test', config['num_layers'], config['self_loop'],\n",
    "                    config['normalize_adj'], config['transductive'])\n",
    "    \n",
    "    dataset = get_dataset(dataset_args)\n",
    "    \n",
    "    \n",
    "    loader = DataLoader(dataset=dataset, batch_size=config['batch_size'],\n",
    "                        shuffle=False, collate_fn=dataset.collate_wrapper)\n",
    "    criterion = get_criterion(config['task'])\n",
    "    stats_per_batch = config['stats_per_batch']\n",
    "    \n",
    "    num_batches = int(ceil(len(dataset) / config['batch_size']))\n",
    "    model.eval()\n",
    "    print('--------------------------------')\n",
    "    print('Testing.')\n",
    "    \n",
    "    \n",
    "    running_loss, total_loss = 0.0, 0.0\n",
    "    num_correct, num_examples = 0, 0\n",
    "    total_correct, total_examples = 0, 0\n",
    "    \n",
    "    for (idx, batch) in enumerate(loader):\n",
    "        features, node_layers, mappings, rows, labels = batch\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        out = model(features, node_layers, mappings, rows)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions = torch.max(out, dim=1)[1]\n",
    "        \n",
    "        num_correct += torch.sum(predictions == labels).item()\n",
    "        total_correct += torch.sum(predictions == labels).item()\n",
    "        \n",
    "        num_examples += len(labels)\n",
    "        total_examples += len(labels)\n",
    "        \n",
    "        if (idx + 1) % stats_per_batch == 0:\n",
    "            running_loss /= stats_per_batch\n",
    "            accuracy = num_correct / num_examples\n",
    "            print('    Batch {} / {}: loss {}, accuracy {}'.format(\n",
    "                idx+1, num_batches, running_loss, accuracy))\n",
    "            running_loss = 0.0\n",
    "            num_correct, num_examples = 0, 0\n",
    "            \n",
    "    total_loss /= num_batches\n",
    "    total_accuracy = total_correct / total_examples\n",
    "    print('Loss {}, accuracy {}'.format(total_loss, total_accuracy))\n",
    "    print('Finished testing.')\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10456312-7f2e-4dc5-a523-db06c3d8666d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1b43a-4394-4166-b6a5-b94c96a78893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8f65b-0227-4008-9365-cf7ec3358f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa00e49c-f32d-4057-ba78-47bd3b6053d0",
   "metadata": {},
   "source": [
    "## 5.1 https://github.com/PetarV-/GAT\n",
    "#### pytorch implementatuon of GAT https://arxiv.org/abs/1710.10903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36285b62-aed9-4045-a339-43a8e244f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d0343-e401-42a7-8a6d-2eaf7ed900c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # Wh.shape (N, out_feature)\n",
    "        # self.a.shape (2 * out_feature, 1)\n",
    "        # Wh1&2.shape (N, 1)\n",
    "        # e.shape (N, N)\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.T\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "    \n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37d25b-deab-4963-bff7-2925cb01a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Sparse version of GAT.\"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
    "                                             nclass, \n",
    "                                             dropout=dropout, \n",
    "                                             alpha=alpha, \n",
    "                                             concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d1336-5d53-4e90-a3b3-5400bc69b3f5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b928667-ef4a-4445-8554-03e049ccc783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    # The classes must be sorted before encoding to enable static class encoding.\n",
    "    # In other words, make sure the first class always maps to index 0.\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize_features(features)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e2336-88c4-4e78-bb6c-ba3b5b93b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
    "parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
    "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=10000, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
    "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
    "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab330c8-e4cc-49d8-98cb-c7a0086a08dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcacfb-d015-4573-a879-373d396fe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "if args.sparse:\n",
    "    model = SpGAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "else:\n",
    "    model = GAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a8f67-60f8-443c-94df-54c977c3395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3380f83-b4eb-4a34-bb4e-c1e8a5af8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f88a2a-a69a-497f-9938-a312429a45ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "\n",
    "# Testing\n",
    "compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25e762-c4e9-4015-ab4a-2f075f173d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42635ab1-008a-49d7-be1e-1d671cb52cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f518d-246f-430b-9ad1-b68e6c7f7ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9656de9b-7d38-4508-bf8a-2f9f8a97d7a3",
   "metadata": {},
   "source": [
    "## 5.2 https://github.com/psh150204/GAT *** Can be referred for understanding\n",
    "#### PyTorch implementation of the paper \"Graph Attention Networks\" (ICLR 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f99d37-82da-49c6-a941-a8e75910c8b2",
   "metadata": {},
   "source": [
    "#### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c768e938-d8ab-4283-9e8b-9223dcb46ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import os \n",
    "import time \n",
    "import random \n",
    "import argparse\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim \n",
    "from torch.autograd import Variable\n",
    "import scipy.sparse as sp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc943fc0-8b10-46af-9961-45c6a62be0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolutionLayer,self).__init__()\n",
    "        self.W = nn.Parameter(torch.zeros(in_features, out_features, dtype = torch.float32))\n",
    "        nn.init.xavier_uniform_(self.W) # initialize as described in Glorot & Bengio (2010)\n",
    "    \n",
    "    def forward(self, input, adj):\n",
    "        # input (= X) : a tensor with size [N, F]\n",
    "        # adj (= A_hat) : a tensor with size [N, N]\n",
    "\n",
    "        return torch.mm(adj, torch.mm(input, self.W))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    # single head attention\n",
    "    def __init__(self, in_features, out_features, alpha):\n",
    "        super(Attention, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W = nn.Linear(in_features, out_features, bias = False)\n",
    "        self.a_T = nn.Linear(2 * out_features, 1, bias = False)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.xavier_uniform_(self.a_T.weight)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        # h : a tensor with size [N, F] where N be a number of nodes and F be a number of features\n",
    "        N = h.size(0)\n",
    "        Wh = self.W(h) # h -> Wh : [N, F] -> [N, F']\n",
    "        \n",
    "        # H1 : [N, N, F'], H2 : [N, N, F'], attn_input = [N, N, 2F']\n",
    "\n",
    "        # H1 = [[h1 h1 ... h1]   |  H2 = [[h1 h2 ... hN]   |   attn_input = [[h1||h1 h1||h2 ... h1||hN]\n",
    "        #       [h2 h2 ... h2]   |        [h1 h2 ... hN]   |                 [h2||h1 h2||h2 ... h2||hN]\n",
    "        #            ...         |             ...         |                         ...\n",
    "        #       [hN hN ... hN]]  |        [h1 h2 ... hN]]  |                 [hN||h1 hN||h2 ... hN||hN]]\n",
    "        \n",
    "        H1 = Wh.unsqueeze(1).repeat(1,N,1)\n",
    "        H2 = Wh.unsqueeze(0).repeat(N,1,1)\n",
    "        \n",
    "        attn_input = torch.cat([H1, H2], dim = -1)\n",
    "\n",
    "        e = F.leaky_relu(self.a_T(attn_input).squeeze(-1), negative_slope = self.alpha) # [N, N]\n",
    "        \n",
    "        attn_mask = -1e18*torch.ones_like(e)\n",
    "        masked_e = torch.where(adj > 0, e, attn_mask)\n",
    "        attn_scores = F.softmax(masked_e, dim = -1) # [N, N]\n",
    "\n",
    "        h_prime = torch.mm(attn_scores, Wh) # [N, F']\n",
    "\n",
    "        return F.elu(h_prime) # [N, F']\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    # multi head attention\n",
    "    def __init__(self, in_features, out_features, num_heads, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        \n",
    "        self.concat = concat\n",
    "        self.attentions = nn.ModuleList([Attention(in_features, out_features, alpha) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        # input (= X) : a tensor with size [N, F]\n",
    "\n",
    "        if self.concat :\n",
    "            # concatenate\n",
    "            outputs = []\n",
    "            for attention in self.attentions:\n",
    "                outputs.append(attention(input, adj))\n",
    "            \n",
    "            return torch.cat(outputs, dim = -1) # [N, KF']\n",
    "\n",
    "        else :\n",
    "            # average\n",
    "            output = None\n",
    "            for attention in self.attentions:\n",
    "                if output == None:\n",
    "                    output = attention(input, adj)\n",
    "                else:\n",
    "                    output += attention(input, adj)\n",
    "            \n",
    "            return output/len(self.attentions) # [N, F']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd6dc8-8374-4c1d-b89c-6b0810e3c6be",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9139b284-7e54-4de1-9efc-97f3d7ff0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, F, H, C, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layer1 = GraphConvolutionLayer(F, H)\n",
    "        self.layer2 = GraphConvolutionLayer(H, C)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # X : a tensor with size [N, F]\n",
    "        \n",
    "        x = self.dropout(F.relu(self.layer1(x, adj))) # [N, H]\n",
    "        return self.layer2(x, adj) # [N, C]\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, F, H, C, dropout, alpha, K):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = GraphAttentionLayer(F, H, K, alpha)\n",
    "        self.layer2 = GraphAttentionLayer(K * H, C, 1, alpha, concat = False)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # x : a tensor with size [N, F]\n",
    "\n",
    "        x = self.dropout(F.relu(self.layer1(x, adj))) # [N, KH]\n",
    "        return self.layer2(x, adj) # [N, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761a54a-0825-470a-8c8a-310abe00721c",
   "metadata": {},
   "source": [
    "#### Load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d39812-149e-46dc-b3a0-6cf9ee202483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset=\"cora\"):\n",
    "    \n",
    "    print(\"loading {} dataset ... \". format(dataset))\n",
    "\n",
    "    path=\"./data/\"+dataset+\"/\" \n",
    "\n",
    "    if dataset == 'cora':\n",
    "        idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path,dataset), dtype=np.dtype(str))\n",
    "        features = sp.csr_matrix(idx_features_labels[:,1:-1], dtype=np.float32)\n",
    "        labels = encode_onehot(idx_features_labels[:,-1])\n",
    "\n",
    "        idx = np.array(idx_features_labels[:,0],dtype=np.int32)\n",
    "        idx_map = {j: i for i,j in enumerate(idx)}\n",
    "        edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path,dataset), dtype=np.int32)\n",
    "        edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "        adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        \n",
    "    elif dataset == 'citeseer':\n",
    "        idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path,dataset), dtype=np.dtype(str))\n",
    "        features = sp.csr_matrix(idx_features_labels[:,1:-1], dtype=np.float32)\n",
    "        labels = encode_onehot(idx_features_labels[:,-1])\n",
    "\n",
    "        idx = np.array(idx_features_labels[:,0],dtype=np.dtype(str))\n",
    "        idx_map = {j: i for i,j in enumerate(idx)}\n",
    "        \n",
    "        edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path,dataset), dtype=np.dtype(str))\n",
    "        edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.dtype(str)).reshape(edges_unordered.shape)\n",
    "        \n",
    "        adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:,0], edges[:,1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T>adj) - adj.multiply(adj.T>adj)\n",
    "    features = normalize_features(features)\n",
    "    adj = normalize_adj(adj+sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200,500)\n",
    "    idx_test = range(500,1500)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test \n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "\n",
    "    return correct / len(labels)\n",
    "\n",
    "def normalize_adj(mx): # A_hat = DAD\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    mx_to =  mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "    return mx_to\n",
    "\n",
    "def normalize_features(mx):\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx_to =  r_mat_inv.dot(mx) \n",
    "    return mx_to \n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i,:] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def mkdir_p(mypath):\n",
    "    '''Creates a directory. equivalent to using mkdir -p on the command line'''\n",
    "\n",
    "    from errno import EEXIST\n",
    "    from os import makedirs,path\n",
    "\n",
    "    try:\n",
    "        makedirs(mypath)\n",
    "    except OSError as exc: # Python >2.5\n",
    "        if exc.errno == EEXIST and path.isdir(mypath):\n",
    "            pass\n",
    "        else: raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6cfb41-d778-4096-87b1-85d924341c44",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "loading cora dataset ... \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.75 GiB (GPU 0; 4.00 GiB total capacity; 1.84 GiB already allocated; 815.20 MiB free; 1.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m args\u001b[38;5;241m.\u001b[39mcuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# not args.no_cuda and torch.cuda.is_available()\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(args\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[1;32m--> 118\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 50\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     47\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     48\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 50\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [N, F]\u001b[39;00m\n\u001b[0;32m     51\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(preds[idx_train], labels[idx_train])\n\u001b[0;32m     52\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m accuracy(preds[idx_train], labels[idx_train])\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Optimization-Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m, in \u001b[0;36mGAT.forward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# x : a tensor with size [N, F]\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m)) \u001b[38;5;66;03m# [N, KH]\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x, adj)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Optimization-Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[7], line 67\u001b[0m, in \u001b[0;36mGraphAttentionLayer.forward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     65\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attention \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentions:\n\u001b[1;32m---> 67\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [N, KF']\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# average\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Optimization-Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, h, adj)\u001b[0m\n\u001b[0;32m     28\u001b[0m Wh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW(h) \u001b[38;5;66;03m# h -> Wh : [N, F] -> [N, F']\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# H1 : [N, N, F'], H2 : [N, N, F'], attn_input = [N, N, 2F']\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# H1 = [[h1 h1 ... h1]   |  H2 = [[h1 h2 ... hN]   |   attn_input = [[h1||h1 h1||h2 ... h1||hN]\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#       [h2 h2 ... h2]   |        [h1 h2 ... hN]   |                 [h2||h1 h2||h2 ... h2||hN]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#            ...         |             ...         |                         ...\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#       [hN hN ... hN]]  |        [h1 h2 ... hN]]  |                 [hN||h1 hN||h2 ... hN||hN]]\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m H1 \u001b[38;5;241m=\u001b[39m \u001b[43mWh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m H2 \u001b[38;5;241m=\u001b[39m Wh\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(N,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m attn_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([H1, H2], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.75 GiB (GPU 0; 4.00 GiB total capacity; 1.84 GiB already allocated; 815.20 MiB free; 1.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    # meta settings\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device('cuda' if(torch.cuda.is_available()) else 'cpu')\n",
    "\n",
    "    # load the data\n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_data(args.dataset)\n",
    "    features = features.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # parameter intialization\n",
    "    N = features.size(0) # num_of_nodes\n",
    "    F = features.size(1) # num_of_features\n",
    "    H = args.hidden # hidden nodes\n",
    "    C = labels.max().item() + 1 # num_classes\n",
    "    \n",
    "    # for validation\n",
    "    epochs_since_improvement = 0\n",
    "    best_loss = 10.\n",
    "\n",
    "    # init training object\n",
    "    if args.model == 'GCN':\n",
    "        network = GCN(F, H, C, args.dropout).to(device)\n",
    "\n",
    "        # pre-processing\n",
    "        A_tilde = adj + torch.eye(N)\n",
    "        D_tilde_inv_sqrt = torch.diag(torch.sqrt(torch.sum(A_tilde, dim = 1)) ** -1)\n",
    "        adj = torch.mm(D_tilde_inv_sqrt, torch.mm(A_tilde, D_tilde_inv_sqrt)).to(device) # A_hat\n",
    "        \n",
    "    else:\n",
    "        network = GAT(F, H, C, args.dropout, args.alpha, args.n_heads).to(device)\n",
    "        adj = adj.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(network.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(args.epochs):\n",
    "        t = time.time()\n",
    "        network.train()\n",
    "\n",
    "        preds = network(features, adj) # [N, F]\n",
    "        train_loss = criterion(preds[idx_train], labels[idx_train])\n",
    "        train_acc = accuracy(preds[idx_train], labels[idx_train])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            preds_val = network(features, adj)\n",
    "            val_loss = criterion(preds_val[idx_val], labels[idx_val])\n",
    "            val_acc = accuracy(preds_val[idx_val], labels[idx_val])\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss < best_loss :\n",
    "                best_loss = val_loss\n",
    "                epochs_since_improvement = 0\n",
    "            else:\n",
    "                epochs_since_improvement += 1\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_accs.append(train_acc.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        val_accs.append(val_acc.item())\n",
    "\n",
    "        print('[%d/%d] train loss : %.4f | train acc %.2f%% | val loss %.4f | val acc %.2f%% | time %.3fs'\n",
    "                    %(epoch+1, args.epochs, train_loss.item(), train_acc.item() * 100, val_loss.item(), val_acc.item() * 100, time.time() - t))\n",
    "\n",
    "        if epochs_since_improvement > args.patience - 1 :\n",
    "            print(\"There's no improvements during %d epochs and so stop the training.\"%(args.patience))\n",
    "            break\n",
    "    \n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        preds = network(features, adj)\n",
    "        test_acc = accuracy(preds[idx_test], labels[idx_test])\n",
    "        print('Test Accuracy : %.2f'%(test_acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "if __name__  == \"__main__\":\n",
    "    \n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "    parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
    "    parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
    "    parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train.')\n",
    "    parser.add_argument('--save_every', type=int, default=10, help='Save every n epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "    parser.add_argument('--hidden', type=int, default=64, help='Number of hidden units.')\n",
    "    parser.add_argument('--n_heads', type=int, default=8, help='Number of head attentions.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate (1 - keep probability).')\n",
    "    parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "    parser.add_argument('--patience', type=int, default=10, help='patience')\n",
    "    parser.add_argument('--dataset', type=str, default='cora', choices=['cora','citeseer'], help='Dataset to train.')\n",
    "    parser.add_argument('--model', type=str, default='GAT', choices=['GCN','GAT'], help='Model to train.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    args.cuda = False # not args.no_cuda and torch.cuda.is_available()\n",
    "    print(args.cuda)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58cdf66e-a46d-4424-9439-ed2c7383428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cora dataset ... \n"
     ]
    }
   ],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device('cuda' if(torch.cuda.is_available()) else 'cpu')\n",
    "\n",
    "# load the data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(args.dataset)\n",
    "features = features.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# parameter intialization\n",
    "N = features.size(0) # num_of_nodes\n",
    "F = features.size(1) # num_of_features\n",
    "H = args.hidden # hidden nodes\n",
    "C = labels.max().item() + 1 # num_classes\n",
    "\n",
    "# for validation\n",
    "epochs_since_improvement = 0\n",
    "best_loss = 10.\n",
    "\n",
    "network = GAT(F, H, C, args.dropout, args.alpha, args.n_heads).to(device)\n",
    "adj = adj.to(device)\n",
    "    \n",
    "optimizer = optim.Adam(network.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5e767d-4b10-4726-b41e-c97b316f3f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2708, 1433]), torch.Size([2708, 2708]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.size(), adj.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f8f00-2f9a-4dab-a78a-594a196127c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Train\n",
    "for epoch in range(args.epochs):\n",
    "    t = time.time()\n",
    "    network.train()\n",
    "\n",
    "        preds = network(features, adj) # [N, F]\n",
    "        train_loss = criterion(preds[idx_train], labels[idx_train])\n",
    "        train_acc = accuracy(preds[idx_train], labels[idx_train])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            preds_val = network(features, adj)\n",
    "            val_loss = criterion(preds_val[idx_val], labels[idx_val])\n",
    "            val_acc = accuracy(preds_val[idx_val], labels[idx_val])\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss < best_loss :\n",
    "                best_loss = val_loss\n",
    "                epochs_since_improvement = 0\n",
    "            else:\n",
    "                epochs_since_improvement += 1\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_accs.append(train_acc.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        val_accs.append(val_acc.item())\n",
    "\n",
    "        print('[%d/%d] train loss : %.4f | train acc %.2f%% | val loss %.4f | val acc %.2f%% | time %.3fs'\n",
    "                    %(epoch+1, args.epochs, train_loss.item(), train_acc.item() * 100, val_loss.item(), val_acc.item() * 100, time.time() - t))\n",
    "\n",
    "        if epochs_since_improvement > args.patience - 1 :\n",
    "            print(\"There's no improvements during %d epochs and so stop the training.\"%(args.patience))\n",
    "            break\n",
    "    \n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        preds = network(features, adj)\n",
    "        test_acc = accuracy(preds[idx_test], labels[idx_test])\n",
    "        print('Test Accuracy : %.2f'%(test_acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "if __name__  == \"__main__\":\n",
    "    \n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "    parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
    "    parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
    "    parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train.')\n",
    "    parser.add_argument('--save_every', type=int, default=10, help='Save every n epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "    parser.add_argument('--hidden', type=int, default=64, help='Number of hidden units.')\n",
    "    parser.add_argument('--n_heads', type=int, default=8, help='Number of head attentions.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate (1 - keep probability).')\n",
    "    parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "    parser.add_argument('--patience', type=int, default=10, help='patience')\n",
    "    parser.add_argument('--dataset', type=str, default='cora', choices=['cora','citeseer'], help='Dataset to train.')\n",
    "    parser.add_argument('--model', type=str, default='GAT', choices=['GCN','GAT'], help='Model to train.')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    args.cuda = False # not args.no_cuda and torch.cuda.is_available()\n",
    "    print(args.cuda)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa14a71-cfc9-4c0b-ba93-4e17d0943a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e34535cb-c5ad-4fdc-a934-a5451560aa96",
   "metadata": {},
   "source": [
    "## 5.3 https://github.com/marblet/gat-pytorch\n",
    "#### This is the pytorch inplementation of Graph Attention Networks.https://arxiv.org/abs/1710.10903"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0557724-8166-41a8-b850-8f9a5872446e",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a654b98-3948-4327-bdfc-8af359eecd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ab070-e23b-4f36-8809-e858c41bc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, data, nhid=8, nhead=8, nhead_out=1, alpha=0.2, dropout=0.6):\n",
    "        super(GAT, self).__init__()\n",
    "        nfeat, nclass = data.num_features, data.num_classes\n",
    "        \n",
    "        self.attentions = [GATConv(nfeat, nhid, dropout=dropout, alpha=alpha) for _ in range(nhead)]\n",
    "        self.out_atts = [GATConv(nhid * nhead, nclass, dropout=dropout, alpha=alpha) for _ in range(nhead_out)]\n",
    "        \n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "            \n",
    "        for i, attention in enumerate(self.out_atts):\n",
    "            self.add_module('out_att{}'.format(i), attention)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for att in self.attentions:\n",
    "            att.reset_parameters()\n",
    "        for att in self.out_atts:\n",
    "            att.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_list = data.features, data.edge_list\n",
    "        x = torch.cat([att(x, edge_list) for att in self.attentions], dim=1)\n",
    "        x = F.elu(x)\n",
    "        x = torch.sum(torch.stack([att(x, edge_list) for att in self.out_atts]), dim=0) / len(self.out_atts)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GATConv(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, bias=True):\n",
    "        super(GATConv, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.data, gain=1.414)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, x, edge_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        h = torch.matmul(x, self.weight)\n",
    "\n",
    "        source, target = edge_list\n",
    "        a_input = torch.cat([h[source], h[target]], dim=1)\n",
    "        e = F.leaky_relu(torch.matmul(a_input, self.a), negative_slope=self.alpha)\n",
    "\n",
    "        N = h.size(0)\n",
    "        attention = -1e20*torch.ones([N, N], device=device, requires_grad=True)\n",
    "        attention[source, target] = e[:, 0]\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        if self.bias is not None:\n",
    "            h_prime = h_prime + self.bias\n",
    "\n",
    "        return h_prime\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class SPGAT(nn.Module):\n",
    "    def __init__(self, data, nhid=8, nhead=8, nhead_out=1, alpha=0.2, dropout=0.6):\n",
    "        super(SPGAT, self).__init__()\n",
    "        nfeat, nclass = data.num_features, data.num_classes\n",
    "        self.attentions = [SPGATConv(nfeat, nhid, dropout=dropout, alpha=alpha) for _ in range(nhead)]\n",
    "        self.out_atts = [SPGATConv(nhid * nhead, nclass, dropout=dropout, alpha=alpha) for _ in range(nhead_out)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        for i, attention in enumerate(self.out_atts):\n",
    "            self.add_module('out_att{}'.format(i), attention)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for att in self.attentions:\n",
    "            att.reset_parameters()\n",
    "        for att in self.out_atts:\n",
    "            att.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_list = data.features, data.edge_list\n",
    "        x = torch.cat([att(x, edge_list) for att in self.attentions], dim=1)\n",
    "        x = F.elu(x)\n",
    "        x = torch.sum(torch.stack([att(x, edge_list) for att in self.out_atts]), dim=0) / len(self.out_atts)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def sp_softmax(indices, values, N):\n",
    "    source, _ = indices\n",
    "    v_max = values.max()\n",
    "    exp_v = torch.exp(values - v_max)\n",
    "    exp_sum = torch.zeros(N, 1, device=device)\n",
    "    exp_sum.scatter_add_(0, source.unsqueeze(1), exp_v)\n",
    "    exp_sum += 1e-10\n",
    "    softmax_v = exp_v / exp_sum[source]\n",
    "    return softmax_v\n",
    "\n",
    "\n",
    "def sp_matmul(indices, values, mat):\n",
    "    source, target = indices\n",
    "    out = torch.zeros_like(mat)\n",
    "    out.scatter_add_(0, source.expand(mat.size(1), -1).t(), values * mat[target])\n",
    "    return out\n",
    "\n",
    "\n",
    "class SPGATConv(GATConv):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, bias=True):\n",
    "        super(SPGATConv, self).__init__(in_features, out_features, dropout, alpha, bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_list):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        h = torch.matmul(x, self.weight)\n",
    "\n",
    "        source, target = edge_list\n",
    "        a_input = torch.cat([h[source], h[target]], dim=1)\n",
    "        e = F.leaky_relu(torch.matmul(a_input, self.a), negative_slope=self.alpha)\n",
    "\n",
    "        attention = sp_softmax(edge_list, e, h.size(0))\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        h_prime = sp_matmul(edge_list, attention, h)\n",
    "        if self.bias is not None:\n",
    "            h_prime = h_prime + self.bias\n",
    "\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c3196-b37d-46c3-9a9f-ed760f8b0341",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389be3b6-2354-436b-ab0e-4ce3a877fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, adj, edge_list, features, labels, train_mask, val_mask, test_mask):\n",
    "        self.adj = adj\n",
    "        self.edge_list = edge_list\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.train_mask = train_mask\n",
    "        self.val_mask = val_mask\n",
    "        self.test_mask = test_mask\n",
    "        self.num_features = features.size(1)\n",
    "        self.num_classes = int(torch.max(labels)) + 1\n",
    "\n",
    "    def to(self, device):\n",
    "        self.adj = self.adj.to(device)\n",
    "        self.edge_list = self.edge_list.to(device)\n",
    "        self.features = self.features.to(device)\n",
    "        self.labels = self.labels.to(device)\n",
    "        self.train_mask = self.train_mask.to(device)\n",
    "        self.val_mask = self.val_mask.to(device)\n",
    "        self.test_mask = self.test_mask.to(device)\n",
    "\n",
    "\n",
    "def load_data(dataset_str, norm_feat=True):\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for name in names:\n",
    "        with open(\"data/planetoid/ind.{}.{}\".format(dataset_str, name), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                out = pkl.load(f, encoding='latin1')\n",
    "            else:\n",
    "                out = objects.append(pkl.load(f))\n",
    "\n",
    "            if name == 'graph':\n",
    "                objects.append(out)\n",
    "            else:\n",
    "                out = out.todense() if hasattr(out, 'todense') else out\n",
    "                objects.append(torch.Tensor(out))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx = parse_index_file(\"data/planetoid/ind.{}.test.index\".format(dataset_str))\n",
    "    train_idx = torch.arange(y.size(0), dtype=torch.long)\n",
    "    val_idx = torch.arange(y.size(0), y.size(0) + 500, dtype=torch.long)\n",
    "    sorted_test_idx = np.sort(test_idx)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        len_test_idx = max(test_idx) - min(test_idx) + 1\n",
    "        tx_ext = torch.zeros(len_test_idx, tx.size(1))\n",
    "        tx_ext[sorted_test_idx - min(test_idx), :] = tx\n",
    "        ty_ext = torch.zeros(len_test_idx, ty.size(1))\n",
    "        ty_ext[sorted_test_idx - min(test_idx), :] = ty\n",
    "\n",
    "        tx, ty = tx_ext, ty_ext\n",
    "\n",
    "    features = torch.cat([allx, tx], dim=0)\n",
    "    features[test_idx] = features[sorted_test_idx]\n",
    "    \n",
    "    if norm_feat:\n",
    "        features = preprocess_features(features)\n",
    "\n",
    "    labels = torch.cat([ally, ty], dim=0).max(dim=1)[1]\n",
    "    labels[test_idx] = labels[sorted_test_idx]\n",
    "\n",
    "    edge_list = adj_list_from_dict(graph)\n",
    "    edge_list = add_self_loops(edge_list, features.size(0))\n",
    "    adj = normalize_adj(edge_list)\n",
    "\n",
    "    train_mask = index_to_mask(train_idx, labels.shape[0])\n",
    "    val_mask = index_to_mask(val_idx, labels.shape[0])\n",
    "    test_mask = index_to_mask(test_idx, labels.shape[0])\n",
    "\n",
    "    data = Data(adj, edge_list, features, labels, train_mask, val_mask, test_mask)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def adj_list_from_dict(graph):\n",
    "    G = nx.from_dict_of_lists(graph)\n",
    "    coo_adj = nx.to_scipy_sparse_matrix(G).tocoo()\n",
    "    indices = torch.from_numpy(np.vstack((coo_adj.row, coo_adj.col)).astype(np.int64))\n",
    "    return indices\n",
    "\n",
    "\n",
    "def index_to_mask(index, size):\n",
    "    mask = torch.zeros((size, ), dtype=torch.bool)\n",
    "    mask[index] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def add_self_loops(edge_list, size):\n",
    "    i = torch.arange(size, dtype=torch.int64).view(1, -1)\n",
    "    self_loops = torch.cat((i, i), dim=0)\n",
    "    edge_list = torch.cat((edge_list, self_loops), dim=1)\n",
    "    return edge_list\n",
    "\n",
    "\n",
    "def get_degree(edge_list):\n",
    "    row, col = edge_list\n",
    "    deg = torch.bincount(row)\n",
    "    return deg\n",
    "\n",
    "\n",
    "def normalize_adj(edge_list):\n",
    "    deg = get_degree(edge_list)\n",
    "    row, col = edge_list\n",
    "    deg_inv_sqrt = torch.pow(deg.to(torch.float), -0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0.0\n",
    "    weight = torch.ones(edge_list.size(1))\n",
    "    v = deg_inv_sqrt[row] * weight * deg_inv_sqrt[col]\n",
    "    norm_adj = torch.sparse.FloatTensor(edge_list, v)\n",
    "    return norm_adj\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    rowsum = features.sum(dim=1, keepdim=True)\n",
    "    rowsum[rowsum == 0] = 1\n",
    "    features = features / rowsum\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc4817-e99d-404d-b3b6-8f7036c4090f",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a072fb2-0c13-4603-b037-f306426f018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from copy import deepcopy\n",
    "from numpy import mean, std\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose, use_loss, use_acc, save_model):\n",
    "        assert use_loss or use_acc, 'use loss or (and) acc'\n",
    "        self.patience = patience\n",
    "        self.use_loss = use_loss\n",
    "        self.use_acc = use_acc\n",
    "        self.save_model = save_model\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_val_acc = 0\n",
    "        self.state_dict = None\n",
    "\n",
    "    def check(self, evals, model, epoch):\n",
    "        if self.use_loss and self.use_acc:\n",
    "            # For GAT, based on https://github.com/PetarV-/GAT/blob/master/execute_cora.py\n",
    "            if evals['val_loss'] <= self.best_val_loss or evals['val_acc'] >= self.best_val_acc:\n",
    "                if evals['val_loss'] <= self.best_val_loss and evals['val_acc'] >= self.best_val_acc:\n",
    "                    if self.save_model:\n",
    "                        self.state_dict = deepcopy(model.state_dict())\n",
    "                self.best_val_loss = min(self.best_val_loss, evals['val_loss'])\n",
    "                self.best_val_acc = max(self.best_val_acc, evals['val_acc'])\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        elif self.use_loss:\n",
    "            if evals['val_loss'] < self.best_val_loss:\n",
    "                self.best_val_loss = evals['val_loss']\n",
    "                self.counter = 0\n",
    "                if self.save_model:\n",
    "                    self.state_dict = deepcopy(model.state_dict())\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        elif self.use_acc:\n",
    "            if evals['val_acc'] > self.best_val_acc:\n",
    "                self.best_val_acc = evals['val_acc']\n",
    "                self.counter = 0\n",
    "                if self.save_model:\n",
    "                    self.state_dict = deepcopy(model.state_dict())\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        stop = False\n",
    "        if self.counter >= self.patience:\n",
    "            stop = True\n",
    "            if self.verbose:\n",
    "                print(\"Stop training, epoch:\", epoch)\n",
    "            if self.save_model:\n",
    "                model.load_state_dict(self.state_dict)\n",
    "        return stop\n",
    "\n",
    "\n",
    "def train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output[data.train_mask], data.labels[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "\n",
    "    outputs = {}\n",
    "    for key in ['train', 'val', 'test']:\n",
    "        if key == 'train':\n",
    "            mask = data.train_mask\n",
    "        elif key == 'val':\n",
    "            mask = data.val_mask\n",
    "        else:\n",
    "            mask = data.test_mask\n",
    "        loss = F.nll_loss(output[mask], data.labels[mask]).item()\n",
    "        pred = output[mask].max(dim=1)[1]\n",
    "        acc = pred.eq(data.labels[mask]).sum().item() / mask.sum().item()\n",
    "\n",
    "        outputs['{}_loss'.format(key)] = loss\n",
    "        outputs['{}_acc'.format(key)] = acc\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def run(data, model, lr, weight_decay, epochs=100000, niter=100, early_stopping=True, patience=100,\n",
    "        use_loss=True, use_acc=True, save_model=True, verbose=False):\n",
    "    # for GPU\n",
    "    data.to(device)\n",
    "\n",
    "    val_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for _ in tqdm(range(niter)):\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        model.to(device).reset_parameters()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # for early stopping\n",
    "        if early_stopping:\n",
    "            stop_checker = EarlyStopping(patience, verbose, use_loss, use_acc, save_model)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, optimizer, data)\n",
    "            evals = evaluate(model, data)\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch: {: 4d}'.format(epoch),\n",
    "                      'train loss: {:.5f}'.format(evals['train_loss']),\n",
    "                      'train acc: {:.5f}'.format(evals['train_acc']),\n",
    "                      'val loss: {:.5f}'.format(evals['val_loss']),\n",
    "                      'val acc: {:.5f}'.format(evals['val_acc']))\n",
    "\n",
    "            if early_stopping:\n",
    "                if stop_checker.check(evals, model, epoch):\n",
    "                    break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        evals = evaluate(model, data)\n",
    "        if verbose:\n",
    "            for met, val in evals.items():\n",
    "                print(met, val)\n",
    "\n",
    "        val_acc_list.append(evals['val_acc'])\n",
    "        test_acc_list.append(evals['test_acc'])\n",
    "\n",
    "    print(\"mean\", mean(test_acc_list))\n",
    "    print(\"std\", std(test_acc_list))\n",
    "    return {\n",
    "        'val_acc': mean(val_acc_list),\n",
    "        'test_acc': mean(test_acc_list),\n",
    "        'test_acc_std': std(test_acc_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757401d-16fb-41e8-871e-c5bbe8de9d3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90915c9a-29b0-43a4-a7d0-db2634a563c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d2eab-4956-4305-9925-ed1837da2948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e056d8-e8ad-4cda-9952-a810bc8f04c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111070a-827a-40f7-8db6-d546bcd1baee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bd743-578d-4ed2-a28e-0ee027929c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc94df1-7009-4598-91f8-302f349dc48c",
   "metadata": {},
   "source": [
    "## 6. https://github.com/QData/LaMP\n",
    "#### arxiv.org/pdf/1904.08049.pdf \n",
    "(NEURAL MESSAGE PASSING FOR MULTI-LABEL CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e256c0e-0ab3-460e-ad9b-84e59aa34874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Optimization-Python",
   "language": "python",
   "name": "optimization-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
