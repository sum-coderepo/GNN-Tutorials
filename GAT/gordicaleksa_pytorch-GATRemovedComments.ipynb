{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### https://github.com/sum-coderepo/pytorch-GAT"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\r\n",
        "import os\r\n",
        "import enum\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import networkx as nx\r\n",
        "from networkx.readwrite import json_graph\r\n",
        "import igraph as ig\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1681722119577
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import enum\r\n",
        "\r\n",
        "class DatasetType(enum.Enum):\r\n",
        "    PPI = 0\r\n",
        "\r\n",
        "class GraphVisualizationTool(enum.Enum):\r\n",
        "    IGRAPH = 0\r\n",
        "\r\n",
        "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\r\n",
        "PPI_PATH = os.path.join(DATA_DIR_PATH, 'ppi')\r\n",
        "PPI_URL = 'https://data.dgl.ai/dataset/ppi.zip'\r\n",
        "\r\n",
        "PPI_NUM_INPUT_FEATURES = 50\r\n",
        "PPI_NUM_CLASSES = 121\r\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722126795
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def json_read(path):\r\n",
        "    with open(path, 'r') as file:\r\n",
        "        data = json.load(file)\r\n",
        "\r\n",
        "    return data"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722129376
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_graph_data(training_config, device):\r\n",
        "    dataset_name = training_config['dataset_name'].lower()\r\n",
        "    should_visualize = training_config['should_visualize']\r\n",
        "\r\n",
        "    if dataset_name == DatasetType.PPI.name.lower():\r\n",
        "\r\n",
        "        # Collect train/val/test graphs here\r\n",
        "        edge_index_list = []\r\n",
        "        node_features_list = []\r\n",
        "        node_labels_list = []\r\n",
        "\r\n",
        "        num_graphs_per_split_cumulative = [0]\r\n",
        "        splits = ['test'] if training_config['ppi_load_test_only'] else ['train', 'valid', 'test']\r\n",
        "\r\n",
        "        for split in splits:\r\n",
        "            node_features = np.load(os.path.join(PPI_PATH, f'{split}_feats.npy'))\r\n",
        "            node_labels = np.load(os.path.join(PPI_PATH, f'{split}_labels.npy'))\r\n",
        "            nodes_links_dict = json_read(os.path.join(PPI_PATH, f'{split}_graph.json'))\r\n",
        "            collection_of_graphs = nx.DiGraph(json_graph.node_link_graph(nodes_links_dict))\r\n",
        "            graph_ids = np.load(os.path.join(PPI_PATH, F'{split}_graph_id.npy'))\r\n",
        "            num_graphs_per_split_cumulative.append(num_graphs_per_split_cumulative[-1] + len(np.unique(graph_ids)))\r\n",
        "\r\n",
        "            for graph_id in range(np.min(graph_ids), np.max(graph_ids) + 1):\r\n",
        "                mask = graph_ids == graph_id \r\n",
        "                graph_node_ids = np.asarray(mask).nonzero()[0]\r\n",
        "                graph = collection_of_graphs.subgraph(graph_node_ids)\r\n",
        "                print(f'Loading {split} graph {graph_id} to CPU. '\r\n",
        "                      f'It has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.')\r\n",
        "                edge_index = torch.tensor(list(graph.edges), dtype=torch.long).transpose(0, 1).contiguous()\r\n",
        "                edge_index = edge_index - edge_index.min()  # bring the edges to [0, num_of_nodes] range\r\n",
        "                edge_index_list.append(edge_index)\r\n",
        "                node_features_list.append(torch.tensor(node_features[mask], dtype=torch.float))\r\n",
        "                node_labels_list.append(torch.tensor(node_labels[mask], dtype=torch.float))\r\n",
        "\r\n",
        "\r\n",
        "        if training_config['ppi_load_test_only']:\r\n",
        "            data_loader_test = GraphDataLoader(\r\n",
        "                node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\r\n",
        "                node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\r\n",
        "                edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\r\n",
        "                batch_size=training_config['batch_size'],\r\n",
        "                shuffle=False\r\n",
        "            )\r\n",
        "            return data_loader_test\r\n",
        "        else:\r\n",
        "\r\n",
        "            data_loader_train = GraphDataLoader(\r\n",
        "                node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\r\n",
        "                node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\r\n",
        "                edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],\r\n",
        "                batch_size=training_config['batch_size'],\r\n",
        "                shuffle=True\r\n",
        "            )\r\n",
        "\r\n",
        "            data_loader_val = GraphDataLoader(\r\n",
        "                node_features_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\r\n",
        "                node_labels_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\r\n",
        "                edge_index_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],\r\n",
        "                batch_size=training_config['batch_size'],\r\n",
        "                shuffle=False  # no need to shuffle the validation and test graphs\r\n",
        "            )\r\n",
        "\r\n",
        "            data_loader_test = GraphDataLoader(\r\n",
        "                node_features_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\r\n",
        "                node_labels_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\r\n",
        "                edge_index_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],\r\n",
        "                batch_size=training_config['batch_size'],\r\n",
        "                shuffle=False\r\n",
        "            )\r\n",
        "\r\n",
        "            return data_loader_train, data_loader_val, data_loader_test\r\n",
        "    else:\r\n",
        "        raise Exception(f'{dataset_name} not yet supported.')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722131657
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphDataLoader(DataLoader):\r\n",
        "    def __init__(self, node_features_list, node_labels_list, edge_index_list, batch_size=1, shuffle=False):\r\n",
        "        graph_dataset = GraphDataset(node_features_list, node_labels_list, edge_index_list)\r\n",
        "        super().__init__(graph_dataset, batch_size, shuffle, collate_fn=graph_collate_fn)\r\n",
        "\r\n",
        "class GraphDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, node_features_list, node_labels_list, edge_index_list):\r\n",
        "        self.node_features_list = node_features_list\r\n",
        "        self.node_labels_list = node_labels_list\r\n",
        "        self.edge_index_list = edge_index_list\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.edge_index_list)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.node_features_list[idx], self.node_labels_list[idx], self.edge_index_list[idx]\r\n",
        "\r\n",
        "def graph_collate_fn(batch):\r\n",
        "    edge_index_list = []\r\n",
        "    node_features_list = []\r\n",
        "    node_labels_list = []\r\n",
        "    num_nodes_seen = 0\r\n",
        "\r\n",
        "    for features_labels_edge_index_tuple in batch:\r\n",
        "        # Just collect these into separate lists\r\n",
        "        node_features_list.append(features_labels_edge_index_tuple[0])\r\n",
        "        node_labels_list.append(features_labels_edge_index_tuple[1])\r\n",
        "\r\n",
        "        edge_index = features_labels_edge_index_tuple[2]  # all of the components are in the [0, N] range\r\n",
        "        edge_index_list.append(edge_index + num_nodes_seen)  # very important! translate the range of this component\r\n",
        "        num_nodes_seen += len(features_labels_edge_index_tuple[1])  # update the number of nodes we've seen so far\r\n",
        "\r\n",
        "    # Merge the PPI graphs into a single graph with multiple connected components\r\n",
        "    node_features = torch.cat(node_features_list, 0)\r\n",
        "    node_labels = torch.cat(node_labels_list, 0)\r\n",
        "    edge_index = torch.cat(edge_index_list, 1)\r\n",
        "\r\n",
        "    return node_features, node_labels, edge_index\r\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722136222
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\r\n",
        "\r\n",
        "config = {\r\n",
        "    'dataset_name': DatasetType.PPI.name,\r\n",
        "    'should_visualize': False,\r\n",
        "    'batch_size': 1,\r\n",
        "    'ppi_load_test_only': False  # small optimization for loading test graphs only, we won't use it here\r\n",
        "}\r\n",
        "\r\n",
        "data_loader_train, data_loader_val, data_loader_test = load_graph_data(config, device)\r\n",
        "# Let's fetch a single batch from the train graph data loader\r\n",
        "node_features, node_labels, edge_index = next(iter(data_loader_train))\r\n",
        "\r\n",
        "print('*' * 20)\r\n",
        "print(node_features.shape, node_features.dtype)\r\n",
        "print(node_labels.shape, node_labels.dtype)\r\n",
        "print(edge_index.shape, edge_index.dtype)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading train graph 1 to CPU. It has 1767 nodes and 34085 edges.\nLoading train graph 2 to CPU. It has 1377 nodes and 31081 edges.\nLoading train graph 3 to CPU. It has 2263 nodes and 61907 edges.\nLoading train graph 4 to CPU. It has 2339 nodes and 67769 edges.\nLoading train graph 5 to CPU. It has 1578 nodes and 37740 edges.\nLoading train graph 6 to CPU. It has 1021 nodes and 19237 edges.\nLoading train graph 7 to CPU. It has 1823 nodes and 46153 edges.\nLoading train graph 8 to CPU. It has 2488 nodes and 72878 edges.\nLoading train graph 9 to CPU. It has 591 nodes and 8299 edges.\nLoading train graph 10 to CPU. It has 3312 nodes and 109510 edges.\nLoading train graph 11 to CPU. It has 2401 nodes and 66619 edges.\nLoading train graph 12 to CPU. It has 1878 nodes and 48146 edges.\nLoading train graph 13 to CPU. It has 1819 nodes and 47587 edges.\nLoading train graph 14 to CPU. It has 3480 nodes and 110234 edges.\nLoading train graph 15 to CPU. It has 2794 nodes and 88112 edges.\nLoading train graph 16 to CPU. It has 2326 nodes and 62188 edges.\nLoading train graph 17 to CPU. It has 2650 nodes and 79714 edges.\nLoading train graph 18 to CPU. It has 2815 nodes and 88335 edges.\nLoading train graph 19 to CPU. It has 3163 nodes and 97321 edges.\nLoading train graph 20 to CPU. It has 3021 nodes and 94359 edges.\nLoading valid graph 21 to CPU. It has 3230 nodes and 100676 edges.\nLoading valid graph 22 to CPU. It has 3284 nodes and 104758 edges.\nLoading test graph 23 to CPU. It has 3224 nodes and 103872 edges.\nLoading test graph 24 to CPU. It has 2300 nodes and 63628 edges.\n********************\ntorch.Size([2650, 50]) torch.float32\ntorch.Size([2650, 121]) torch.float32\ntorch.Size([2, 79714]) torch.int64\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722167244
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "\r\n",
        "class GAT(torch.nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,\r\n",
        "                 dropout=0.6, log_attention_weights=False):\r\n",
        "        super().__init__()\r\n",
        "        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'\r\n",
        "\r\n",
        "        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below\r\n",
        "\r\n",
        "        gat_layers = []  # collect GAT layers\r\n",
        "        for i in range(num_of_layers):\r\n",
        "            layer = GATLayer(\r\n",
        "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\r\n",
        "                num_out_features=num_features_per_layer[i+1],\r\n",
        "                num_of_heads=num_heads_per_layer[i+1],\r\n",
        "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\r\n",
        "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\r\n",
        "                dropout_prob=dropout,\r\n",
        "                add_skip_connection=add_skip_connection,\r\n",
        "                bias=bias,\r\n",
        "                log_attention_weights=log_attention_weights\r\n",
        "            )\r\n",
        "            gat_layers.append(layer)\r\n",
        "\r\n",
        "        self.gat_net = nn.Sequential(\r\n",
        "            *gat_layers,\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, data):\r\n",
        "        return self.gat_net(data)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722174247
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GATLayer(torch.nn.Module):\r\n",
        "\r\n",
        "    src_nodes_dim = 0\r\n",
        "    trg_nodes_dim = 1\r\n",
        "    nodes_dim = 0\r\n",
        "    head_dim = 1\r\n",
        "\r\n",
        "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\r\n",
        "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\r\n",
        "\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.num_of_heads = num_of_heads\r\n",
        "        self.num_out_features = num_out_features\r\n",
        "        self.concat = concat  # whether we should concatenate or average the attention heads\r\n",
        "        self.add_skip_connection = add_skip_connection\r\n",
        "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\r\n",
        "\r\n",
        "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\r\n",
        "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\r\n",
        "\r\n",
        "        if bias and concat:\r\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\r\n",
        "        elif bias and not concat:\r\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\r\n",
        "        else:\r\n",
        "            self.register_parameter('bias', None)\r\n",
        "\r\n",
        "        if add_skip_connection:\r\n",
        "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\r\n",
        "        else:\r\n",
        "            self.register_parameter('skip_proj', None)\r\n",
        "\r\n",
        "        self.leakyReLU = nn.LeakyReLU(0.2)  \r\n",
        "        self.activation = activation\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\r\n",
        "\r\n",
        "        self.log_attention_weights = log_attention_weights\r\n",
        "        self.attention_weights = None\r\n",
        "\r\n",
        "        self.init_params()\r\n",
        "        \r\n",
        "    def forward(self, data):\r\n",
        "        in_nodes_features, edge_index = data  # unpack data\r\n",
        "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\r\n",
        "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\r\n",
        "\r\n",
        "        in_nodes_features = self.dropout(in_nodes_features)\r\n",
        "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\r\n",
        "\r\n",
        "        nodes_features_proj = self.dropout(nodes_features_proj)\r\n",
        "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\r\n",
        "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\r\n",
        "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\r\n",
        "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\r\n",
        "\r\n",
        "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\r\n",
        "        attentions_per_edge = self.dropout(attentions_per_edge)\r\n",
        "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\r\n",
        "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\r\n",
        "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\r\n",
        "        return (out_nodes_features, edge_index)\r\n",
        "\r\n",
        "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\r\n",
        "\r\n",
        "        scores_per_edge = scores_per_edge - scores_per_edge.max()\r\n",
        "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\r\n",
        "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\r\n",
        "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\r\n",
        "        return attentions_per_edge.unsqueeze(-1)\r\n",
        "\r\n",
        "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\r\n",
        "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\r\n",
        "        size = list(exp_scores_per_edge.shape)\r\n",
        "        size[self.nodes_dim] = num_of_nodes\r\n",
        "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\r\n",
        "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\r\n",
        "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\r\n",
        "\r\n",
        "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\r\n",
        "        size = list(nodes_features_proj_lifted_weighted.shape)\r\n",
        "        size[self.nodes_dim] = num_of_nodes\r\n",
        "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\r\n",
        "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\r\n",
        "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\r\n",
        "        return out_nodes_features\r\n",
        "\r\n",
        "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\r\n",
        "        src_nodes_index = edge_index[self.src_nodes_dim]\r\n",
        "        trg_nodes_index = edge_index[self.trg_nodes_dim]\r\n",
        "\r\n",
        "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\r\n",
        "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\r\n",
        "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\r\n",
        "\r\n",
        "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\r\n",
        "\r\n",
        "    def explicit_broadcast(self, this, other):\r\n",
        "        for _ in range(this.dim(), other.dim()):\r\n",
        "            this = this.unsqueeze(-1)\r\n",
        "        return this.expand_as(other)\r\n",
        "\r\n",
        "    def init_params(self):\r\n",
        "        nn.init.xavier_uniform_(self.linear_proj.weight)\r\n",
        "        nn.init.xavier_uniform_(self.scoring_fn_target)\r\n",
        "        nn.init.xavier_uniform_(self.scoring_fn_source)\r\n",
        "\r\n",
        "        if self.bias is not None:\r\n",
        "            torch.nn.init.zeros_(self.bias)\r\n",
        "\r\n",
        "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\r\n",
        "        if self.log_attention_weights:  # potentially log for later visualization in playground.py\r\n",
        "            self.attention_weights = attention_coefficients\r\n",
        "\r\n",
        "        if self.add_skip_connection:  # add skip or residual connection\r\n",
        "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:\r\n",
        "                out_nodes_features += in_nodes_features.unsqueeze(1)\r\n",
        "            else:\r\n",
        "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\r\n",
        "\r\n",
        "        if self.concat:\r\n",
        "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\r\n",
        "        else:\r\n",
        "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\r\n",
        "\r\n",
        "        if self.bias is not None:\r\n",
        "            out_nodes_features += self.bias\r\n",
        "\r\n",
        "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722177550
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "\r\n",
        "class LoopPhase(enum.Enum):\r\n",
        "    TRAIN = 0,\r\n",
        "    VAL = 1,\r\n",
        "    TEST = 2\r\n",
        "\r\n",
        "writer = SummaryWriter() \r\n",
        "BEST_VAL_MICRO_F1 = 0\r\n",
        "BEST_VAL_LOSS = 0\r\n",
        "PATIENCE_CNT = 0\r\n",
        "\r\n",
        "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\r\n",
        "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')\r\n",
        "os.makedirs(BINARIES_PATH, exist_ok=True)\r\n",
        "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722193963
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import git\r\n",
        "import re  # regex\r\n",
        "def get_training_state(training_config, model):\r\n",
        "    training_state = {\r\n",
        "       # \"commit_hash\": git.Repo(search_parent_directories=True).head.object.hexsha,\r\n",
        "        \"dataset_name\": training_config['dataset_name'],\r\n",
        "        \"num_of_epochs\": training_config['num_of_epochs'],\r\n",
        "        \"test_perf\": training_config['test_perf'],\r\n",
        "        \"num_of_layers\": training_config['num_of_layers'],\r\n",
        "        \"num_heads_per_layer\": training_config['num_heads_per_layer'],\r\n",
        "        \"num_features_per_layer\": training_config['num_features_per_layer'],\r\n",
        "        \"add_skip_connection\": training_config['add_skip_connection'],\r\n",
        "        \"bias\": training_config['bias'],\r\n",
        "        \"dropout\": training_config['dropout'],\r\n",
        "        \"state_dict\": model.state_dict()\r\n",
        "    }\r\n",
        "    return training_state\r\n",
        "\r\n",
        "\r\n",
        "def print_model_metadata(training_state):\r\n",
        "    header = f'\\n{\"*\"*5} Model training metadata: {\"*\"*5}'\r\n",
        "    print(header)\r\n",
        "\r\n",
        "    for key, value in training_state.items():\r\n",
        "        if key != 'state_dict':  # don't print state_dict it's a bunch of numbers...\r\n",
        "            print(f'{key}: {value}')\r\n",
        "    print(f'{\"*\" * len(header)}\\n')\r\n",
        "\r\n",
        "def get_available_binary_name(dataset_name='unknown'):\r\n",
        "    prefix = f'gat_{dataset_name}'\r\n",
        "\r\n",
        "    def valid_binary_name(binary_name):\r\n",
        "        pattern = re.compile(rf'{prefix}_[0-9]{{6}}\\.pth')\r\n",
        "        return re.fullmatch(pattern, binary_name) is not None\r\n",
        "    valid_binary_names = list(filter(valid_binary_name, os.listdir(BINARIES_PATH)))\r\n",
        "    if len(valid_binary_names) > 0:\r\n",
        "        last_binary_name = sorted(valid_binary_names)[-1]\r\n",
        "        new_suffix = int(last_binary_name.split('.')[0][-6:]) + 1  # increment by 1\r\n",
        "        return f'{prefix}_{str(new_suffix).zfill(6)}.pth'\r\n",
        "    else:\r\n",
        "        return f'{prefix}_000000.pth'"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722981715
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\r\n",
        "def get_training_args():\r\n",
        "    parser = argparse.ArgumentParser()\r\n",
        "    parser.add_argument(\"--num_of_epochs\", type=int, help=\"number of training epochs\", default=200)\r\n",
        "    parser.add_argument(\"--patience_period\", type=int, help=\"number of epochs with no improvement on val before terminating\", default=100)\r\n",
        "    parser.add_argument(\"--lr\", type=float, help=\"model learning rate\", default=5e-3)\r\n",
        "    parser.add_argument(\"--weight_decay\", type=float, help=\"L2 regularization on model weights\", default=0)\r\n",
        "    parser.add_argument(\"--should_test\", type=bool, help='should test the model on the test dataset?', default=True)\r\n",
        "    parser.add_argument(\"--force_cpu\", type=bool, help='use CPU if your GPU is too small', default=False)\r\n",
        "\r\n",
        "    parser.add_argument(\"--dataset_name\", choices=[el.name for el in DatasetType], help='dataset to use for training', default=DatasetType.PPI.name)\r\n",
        "    parser.add_argument(\"--batch_size\", type=int, help='number of graphs in a batch', default=2)\r\n",
        "    parser.add_argument(\"--should_visualize\", type=bool, help='should visualize the dataset?', default=False)\r\n",
        "\r\n",
        "    parser.add_argument(\"--enable_tensorboard\", type=bool, help=\"enable tensorboard logging\", default=False)\r\n",
        "    parser.add_argument(\"--console_log_freq\", type=int, help=\"log to output console (epoch) freq (None for no logging)\", default=10)\r\n",
        "    parser.add_argument(\"--checkpoint_freq\", type=int, help=\"checkpoint model saving (epoch) freq (None for no logging)\", default=5)\r\n",
        "    args = parser.parse_args('')\r\n",
        "\r\n",
        "\r\n",
        "    gat_config = {\r\n",
        "\r\n",
        "        \"num_of_layers\": 3,  \r\n",
        "        \"num_heads_per_layer\": [4, 4, 6],\r\n",
        "        \"num_features_per_layer\": [PPI_NUM_INPUT_FEATURES, 64, 64, PPI_NUM_CLASSES],\r\n",
        "        \"add_skip_connection\": True,\r\n",
        "        \"bias\": True,\r\n",
        "        \"dropout\": 0.0,\r\n",
        "    }\r\n",
        "\r\n",
        "    training_config = dict()\r\n",
        "    for arg in vars(args):\r\n",
        "        training_config[arg] = getattr(args, arg)\r\n",
        "    training_config['ppi_load_test_only'] = False\r\n",
        "\r\n",
        "    training_config.update(gat_config)\r\n",
        "    return training_config"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722982619
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\r\n",
        "def train_gat_ppi(config):\r\n",
        "    global BEST_VAL_MICRO_F1, BEST_VAL_LOSS\r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not config['force_cpu'] else \"cpu\")\r\n",
        "    data_loader_train, data_loader_val, data_loader_test = load_graph_data(config, device)\r\n",
        "\r\n",
        "    gat = GAT(\r\n",
        "        num_of_layers=config['num_of_layers'],\r\n",
        "        num_heads_per_layer=config['num_heads_per_layer'],\r\n",
        "        num_features_per_layer=config['num_features_per_layer'],\r\n",
        "        add_skip_connection=config['add_skip_connection'],\r\n",
        "        bias=config['bias'],\r\n",
        "        dropout=config['dropout'],\r\n",
        "        log_attention_weights=False\r\n",
        "    ).to(device)\r\n",
        "\r\n",
        "    loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\r\n",
        "    optimizer = Adam(gat.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\r\n",
        "\r\n",
        "    main_loop = get_main_loop(\r\n",
        "        config,\r\n",
        "        gat,\r\n",
        "        loss_fn,\r\n",
        "        optimizer,\r\n",
        "        config['patience_period'],\r\n",
        "        time.time())\r\n",
        "\r\n",
        "    BEST_VAL_MICRO_F1, BEST_VAL_LOSS, PATIENCE_CNT = [0, 0, 0]\r\n",
        "\r\n",
        "    for epoch in range(config['num_of_epochs']):\r\n",
        "        main_loop(phase=LoopPhase.TRAIN, data_loader=data_loader_train, epoch=epoch)\r\n",
        "        with torch.no_grad():\r\n",
        "            try:\r\n",
        "                main_loop(phase=LoopPhase.VAL, data_loader=data_loader_val, epoch=epoch)\r\n",
        "            except Exception as e:\r\n",
        "                print(str(e))\r\n",
        "                break\r\n",
        "\r\n",
        "    if config['should_test']:\r\n",
        "        micro_f1 = main_loop(phase=LoopPhase.TEST, data_loader=data_loader_test)\r\n",
        "        config['test_perf'] = micro_f1\r\n",
        "        print('*' * 50)\r\n",
        "        print(f'Test micro-F1 = {micro_f1}')\r\n",
        "    else:\r\n",
        "        config['test_perf'] = -1\r\n",
        "\r\n",
        "    torch.save(\r\n",
        "        get_training_state(config, gat),\r\n",
        "        os.path.join(BINARIES_PATH, get_available_binary_name(config['dataset_name']))\r\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722985846
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\r\n",
        "\r\n",
        "def get_main_loop(config, gat, sigmoid_cross_entropy_loss, optimizer, patience_period, time_start):\r\n",
        "    device = next(gat.parameters()).device\r\n",
        "\r\n",
        "    def main_loop(phase, data_loader, epoch=0):\r\n",
        "        global BEST_VAL_MICRO_F1, BEST_VAL_LOSS, PATIENCE_CNT, writer\r\n",
        "        if phase == LoopPhase.TRAIN:\r\n",
        "            gat.train()\r\n",
        "        else:\r\n",
        "            gat.eval()\r\n",
        "        for batch_idx, (node_features, gt_node_labels, edge_index) in enumerate(data_loader):\r\n",
        "            edge_index = edge_index.to(device)\r\n",
        "            node_features = node_features.to(device)\r\n",
        "            gt_node_labels = gt_node_labels.to(device)\r\n",
        "\r\n",
        "            graph_data = (node_features, edge_index)\r\n",
        "\r\n",
        "            nodes_unnormalized_scores = gat(graph_data)[0]\r\n",
        "            loss = sigmoid_cross_entropy_loss(nodes_unnormalized_scores, gt_node_labels)\r\n",
        "\r\n",
        "            if phase == LoopPhase.TRAIN:\r\n",
        "                optimizer.zero_grad()\r\n",
        "                loss.backward()\r\n",
        "                optimizer.step()\r\n",
        "                \r\n",
        "            pred = (nodes_unnormalized_scores > 0).float().cpu().numpy()\r\n",
        "            gt = gt_node_labels.cpu().numpy()\r\n",
        "            micro_f1 = f1_score(gt, pred, average='micro')\r\n",
        "            global_step = len(data_loader) * epoch + batch_idx\r\n",
        "            if phase == LoopPhase.TRAIN:\r\n",
        "                if config['enable_tensorboard']:\r\n",
        "                    writer.add_scalar('training_loss', loss.item(), global_step)\r\n",
        "                    writer.add_scalar('training_micro_f1', micro_f1, global_step)\r\n",
        "\r\n",
        "                if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0 and batch_idx == 0:\r\n",
        "                    print(f'GAT training: time elapsed= {(time.time() - time_start):.2f} [s] |'\r\n",
        "                          f' epoch={epoch + 1} | batch={batch_idx + 1} | train micro-F1={micro_f1}.')\r\n",
        "\r\n",
        "                if config['checkpoint_freq'] is not None and (epoch + 1) % config['checkpoint_freq'] == 0 and batch_idx == 0:\r\n",
        "                    ckpt_model_name = f'gat_{config[\"dataset_name\"]}_ckpt_epoch_{epoch + 1}.pth'\r\n",
        "                    config['test_perf'] = -1\r\n",
        "                    torch.save(get_training_state(config, gat), os.path.join(CHECKPOINTS_PATH, ckpt_model_name))\r\n",
        "\r\n",
        "            elif phase == LoopPhase.VAL:\r\n",
        "                if config['enable_tensorboard']:\r\n",
        "                    writer.add_scalar('val_loss', loss.item(), global_step)\r\n",
        "                    writer.add_scalar('val_micro_f1', micro_f1, global_step)\r\n",
        "\r\n",
        "                if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0 and batch_idx == 0:\r\n",
        "                    print(f'GAT validation: time elapsed= {(time.time() - time_start):.2f} [s] |'\r\n",
        "                          f' epoch={epoch + 1} | batch={batch_idx + 1} | val micro-F1={micro_f1}')\r\n",
        "\r\n",
        "                if micro_f1 > BEST_VAL_MICRO_F1 or loss.item() < BEST_VAL_LOSS:\r\n",
        "                    BEST_VAL_MICRO_F1 = max(micro_f1, BEST_VAL_MICRO_F1)\r\n",
        "                    BEST_VAL_LOSS = min(loss.item(), BEST_VAL_LOSS)\r\n",
        "                    PATIENCE_CNT = 0\r\n",
        "                else:\r\n",
        "                    PATIENCE_CNT += 1\r\n",
        "\r\n",
        "                if PATIENCE_CNT >= patience_period:\r\n",
        "                    raise Exception('Stopping the training, the universe has no more patience for this training.')\r\n",
        "            else:\r\n",
        "                return micro_f1\r\n",
        "\r\n",
        "    return main_loop"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681722986262
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gat_ppi(get_training_args())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading train graph 1 to CPU. It has 1767 nodes and 34085 edges.\nLoading train graph 2 to CPU. It has 1377 nodes and 31081 edges.\nLoading train graph 3 to CPU. It has 2263 nodes and 61907 edges.\nLoading train graph 4 to CPU. It has 2339 nodes and 67769 edges.\nLoading train graph 5 to CPU. It has 1578 nodes and 37740 edges.\nLoading train graph 6 to CPU. It has 1021 nodes and 19237 edges.\nLoading train graph 7 to CPU. It has 1823 nodes and 46153 edges.\nLoading train graph 8 to CPU. It has 2488 nodes and 72878 edges.\nLoading train graph 9 to CPU. It has 591 nodes and 8299 edges.\nLoading train graph 10 to CPU. It has 3312 nodes and 109510 edges.\nLoading train graph 11 to CPU. It has 2401 nodes and 66619 edges.\nLoading train graph 12 to CPU. It has 1878 nodes and 48146 edges.\nLoading train graph 13 to CPU. It has 1819 nodes and 47587 edges.\nLoading train graph 14 to CPU. It has 3480 nodes and 110234 edges.\nLoading train graph 15 to CPU. It has 2794 nodes and 88112 edges.\nLoading train graph 16 to CPU. It has 2326 nodes and 62188 edges.\nLoading train graph 17 to CPU. It has 2650 nodes and 79714 edges.\nLoading train graph 18 to CPU. It has 2815 nodes and 88335 edges.\nLoading train graph 19 to CPU. It has 3163 nodes and 97321 edges.\nLoading train graph 20 to CPU. It has 3021 nodes and 94359 edges.\nLoading valid graph 21 to CPU. It has 3230 nodes and 100676 edges.\nLoading valid graph 22 to CPU. It has 3284 nodes and 104758 edges.\nLoading test graph 23 to CPU. It has 3224 nodes and 103872 edges.\nLoading test graph 24 to CPU. It has 2300 nodes and 63628 edges.\nGAT training: time elapsed= 0.40 [s] | epoch=1 | batch=1 | train micro-F1=0.36809746762503726.\nGAT validation: time elapsed= 2.68 [s] | epoch=1 | batch=1 | val micro-F1=0.4558713067856574\nGAT training: time elapsed= 26.95 [s] | epoch=11 | batch=1 | train micro-F1=0.7284530725106786.\nGAT validation: time elapsed= 29.31 [s] | epoch=11 | batch=1 | val micro-F1=0.739624392429622\nGAT training: time elapsed= 53.92 [s] | epoch=21 | batch=1 | train micro-F1=0.8431920569581117.\nGAT validation: time elapsed= 56.42 [s] | epoch=21 | batch=1 | val micro-F1=0.7701355187253914\nGAT training: time elapsed= 81.20 [s] | epoch=31 | batch=1 | train micro-F1=0.9089559365833497.\nGAT validation: time elapsed= 83.67 [s] | epoch=31 | batch=1 | val micro-F1=0.875195534698521\nGAT training: time elapsed= 108.40 [s] | epoch=41 | batch=1 | train micro-F1=0.9263574676330872.\nGAT validation: time elapsed= 110.90 [s] | epoch=41 | batch=1 | val micro-F1=0.8967699169728225\nGAT training: time elapsed= 135.80 [s] | epoch=51 | batch=1 | train micro-F1=0.9484065051683971.\nGAT validation: time elapsed= 138.28 [s] | epoch=51 | batch=1 | val micro-F1=0.9241167328164187\nGAT training: time elapsed= 163.01 [s] | epoch=61 | batch=1 | train micro-F1=0.949560595069627.\nGAT validation: time elapsed= 165.42 [s] | epoch=61 | batch=1 | val micro-F1=0.9370321884772124\nGAT training: time elapsed= 190.36 [s] | epoch=71 | batch=1 | train micro-F1=0.954315144277209.\nGAT validation: time elapsed= 192.64 [s] | epoch=71 | batch=1 | val micro-F1=0.9419533335494817\nGAT training: time elapsed= 217.61 [s] | epoch=81 | batch=1 | train micro-F1=0.9758896576849404.\nGAT validation: time elapsed= 220.11 [s] | epoch=81 | batch=1 | val micro-F1=0.9464528592839012\nGAT training: time elapsed= 244.80 [s] | epoch=91 | batch=1 | train micro-F1=0.9867584647356867.\nGAT validation: time elapsed= 247.16 [s] | epoch=91 | batch=1 | val micro-F1=0.9520490654662457\nGAT training: time elapsed= 272.05 [s] | epoch=101 | batch=1 | train micro-F1=0.9700882692367673.\nGAT validation: time elapsed= 274.76 [s] | epoch=101 | batch=1 | val micro-F1=0.9538464200585735\nGAT training: time elapsed= 299.45 [s] | epoch=111 | batch=1 | train micro-F1=0.9855469537368142.\nGAT validation: time elapsed= 301.95 [s] | epoch=111 | batch=1 | val micro-F1=0.9589992242048099\nGAT training: time elapsed= 326.88 [s] | epoch=121 | batch=1 | train micro-F1=0.9636140128561292.\nGAT validation: time elapsed= 329.27 [s] | epoch=121 | batch=1 | val micro-F1=0.9519592498303148\nGAT training: time elapsed= 354.04 [s] | epoch=131 | batch=1 | train micro-F1=0.9900421855016766.\nGAT validation: time elapsed= 356.48 [s] | epoch=131 | batch=1 | val micro-F1=0.9610180405433053\nGAT training: time elapsed= 381.34 [s] | epoch=141 | batch=1 | train micro-F1=0.9925204422499408.\nGAT validation: time elapsed= 383.74 [s] | epoch=141 | batch=1 | val micro-F1=0.9609237313886356\nGAT training: time elapsed= 408.52 [s] | epoch=151 | batch=1 | train micro-F1=0.9885568932890876.\nGAT validation: time elapsed= 411.03 [s] | epoch=151 | batch=1 | val micro-F1=0.9640039235594257\nGAT training: time elapsed= 435.89 [s] | epoch=161 | batch=1 | train micro-F1=0.9929391585516727.\nGAT validation: time elapsed= 438.30 [s] | epoch=161 | batch=1 | val micro-F1=0.9635973935053045\nGAT training: time elapsed= 462.93 [s] | epoch=171 | batch=1 | train micro-F1=0.9781471252824503.\nGAT validation: time elapsed= 465.49 [s] | epoch=171 | batch=1 | val micro-F1=0.959161392749179\nGAT training: time elapsed= 490.44 [s] | epoch=181 | batch=1 | train micro-F1=0.9807494072776003.\nGAT validation: time elapsed= 492.83 [s] | epoch=181 | batch=1 | val micro-F1=0.963234325854703\nGAT training: time elapsed= 517.67 [s] | epoch=191 | batch=1 | train micro-F1=0.9902160799016281.\nGAT validation: time elapsed= 519.99 [s] | epoch=191 | batch=1 | val micro-F1=0.9654997989649939\n**************************************************\nTest micro-F1 = 0.9806478796938173\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1681723551953
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}