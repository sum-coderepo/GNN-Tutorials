{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72522eba-6c2d-4294-b186-4a33899f1336",
   "metadata": {},
   "source": [
    "## 1. https://github.com/pbloem/former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc89cca-cf8a-45db-9616-398eae2b3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, time, math, tqdm, random, sys, gzip, logging, urllib\n",
    "import argparse\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import  SummaryWriter\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "# from torchtext import data, datasets, vocab\n",
    "#from torchtext.legacy import data, datasets, vocab\n",
    "from torchtext import data, datasets, vocab\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, Dataset, BucketIterator\n",
    "from torchtext.datasets import TranslationDataset\n",
    "import inspect\n",
    "import io\n",
    "import warnings\n",
    "import random\n",
    "from __future__ import unicode_literals\n",
    "from collections import defaultdict, Counter\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import codecs\n",
    "import spacy\n",
    "import tarfile\n",
    "import torchtext.data\n",
    "import torchtext.datasets\n",
    "from __future__ import unicode_literals\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42ad6d-9600-4272-80f8-c10e4fa2f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0d684-0242-4860-bd17-da400e5ffa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enwik8(path=None, n_train=int(90e6), n_valid=int(5e6), n_test=int(5e6)):\n",
    "    \"\"\"\n",
    "    Load the enwik8 dataset from the Hutter challenge.\n",
    "    Adapted from https://github.com/openai/blocksparse/blob/master/examples/transformer/enwik8.py\n",
    "    :param path:\n",
    "    :param n_train:\n",
    "    :param n_valid:\n",
    "    :param n_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        path = here('data/enwik8.gz')\n",
    "\n",
    "    with gzip.open(path) if path.endswith('.gz') else open(path) as file:\n",
    "        X = np.fromstring(file.read(n_train + n_valid + n_test), dtype=np.uint8)\n",
    "        trX, vaX, teX = np.split(X, [n_train, n_train + n_valid])\n",
    "        return torch.from_numpy(trX), torch.from_numpy(vaX), torch.from_numpy(teX)\n",
    "\n",
    "def sample(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome log-probabilities\n",
    "    :param temperature: Sampling temperature. 1.0 follows the given distribution,\n",
    "        0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "\n",
    "    p = F.softmax(lnprobs / temperature, dim=0)\n",
    "    cd = dist.Categorical(p)\n",
    "\n",
    "    return cd.sample()\n",
    "\n",
    "def sample_sequence(model, seed, max_context, length=600, temperature=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    Sequentially samples a sequence from the model, token by token.\n",
    "    :param model:\n",
    "    :param seed: The sequence to start with.\n",
    "    :param length: The total number of characters to sample.\n",
    "    :param temperature: The sampling temperature.\n",
    "    :param verbose: If true, the sampled sequence is also printed as it is sampled.\n",
    "    :return: The sampled sequence, including the seed.\n",
    "    \"\"\"\n",
    "\n",
    "    sequence = seed.detach().clone()\n",
    "\n",
    "    if verbose: # Print the seed, surrounded by square brackets\n",
    "        print('[', end='', flush=True)\n",
    "        for c in seed:\n",
    "            print(str(chr(c)), end='', flush=True)\n",
    "        print(']', end='', flush=True)\n",
    "\n",
    "    for _ in range(length):\n",
    "\n",
    "        # Input is the tail end of the sampled sequence (as many tokens as the model can handle)\n",
    "        input = sequence[-max_context:]\n",
    "\n",
    "        # Run the current input through the model\n",
    "        output = model(input[None, :])\n",
    "\n",
    "        # Sample the next token from the probabilitys at the last position of the output.\n",
    "        c = sample(output[0, -1, :], temperature)\n",
    "\n",
    "        if verbose:\n",
    "            print(str(chr(max(32, c))), end='', flush=True)\n",
    "\n",
    "        sequence = torch.cat([sequence, c[None]], dim=0) # Append the sampled token to the sequence\n",
    "\n",
    "    print()\n",
    "    return seed\n",
    "\n",
    "def sample_batch(data, length, batch_size):\n",
    "    \"\"\"\n",
    "    Takes the data (a single sequence of tokens) and slices out a batch of subsequences to provide as input to the model.\n",
    "    For each input instance, it also slices out the sequence that is shifted one position to the right, to provide as a\n",
    "    target for the model.\n",
    "    :param data: The (training) data. A single vector of tokens represented by integers\n",
    "    :param length: The length of the subsequences in the batch.\n",
    "    :param batch_size: The number of subsequences in the batch\n",
    "    :return: A pair (input, target) of minteger matrices representing the input and target for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample the starting indices of the sequences to slice out.\n",
    "    starts = torch.randint(size=(batch_size,), low=0, high=data.size(0) - length - 1)\n",
    "\n",
    "    # Slice out the input sequences\n",
    "    seqs_inputs  = [data[start:start + length] for start in starts]\n",
    "    # -- the start index is the one we just sampled, and the end is exactly 'lentgh' positions after that.\n",
    "    seqs_target = [data[start + 1:start + length + 1] for start in starts]\n",
    "    # -- The target is the same sequence as input, except one character ahead (we are asking the model to predict the\n",
    "    #    next character at each position)\n",
    "\n",
    "    # We now have two lists of torch vectors, which we can concatenate into matrices of batch_size-by-length\n",
    "    inputs = torch.cat([s[None, :] for s in seqs_inputs], dim=0).to(torch.long)\n",
    "    target = torch.cat([s[None, :] for s in seqs_target], dim=0).to(torch.long)\n",
    "    # -- Note that we add a singleton dimenson to each vector, s[None.,:], and then concatenate along that dimension.\n",
    "\n",
    "    return inputs, target\n",
    "\n",
    "def mask_(matrices, maskval=0.0, mask_diagonal=True):\n",
    "    \"\"\"\n",
    "    Masks out all values in the given batch of matrices where i <= j holds,\n",
    "    i < j if mask_diagonal is false\n",
    "    In place operation\n",
    "    :param tns:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    h, w = matrices.size(-2), matrices.size(-1)\n",
    "\n",
    "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
    "    matrices[..., indices[0], indices[1]] = maskval\n",
    "\n",
    "def d(tensor=None):\n",
    "    \"\"\"\n",
    "    Returns a device string either for the best available device,\n",
    "    or for the device corresponding to the argument\n",
    "    :param tensor:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if tensor is None:\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return 'cuda' if tensor.is_cuda else 'cpu'\n",
    "\n",
    "def here(subpath=None):\n",
    "    \"\"\"\n",
    "    :return: the path in which the package resides (the directory containing the 'former' dir)\n",
    "    \"\"\"\n",
    "    if subpath is None:\n",
    "        return os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
    "\n",
    "    return os.path.abspath(os.path.join(os.path.dirname(__file__), '../..', subpath))\n",
    "\n",
    "def contains_nan(tensor):\n",
    "    return bool((tensor != tensor).sum() > 0)\n",
    "\n",
    "\n",
    "tics = []\n",
    "\n",
    "\n",
    "def tic():\n",
    "    tics.append(time.time())\n",
    "\n",
    "def toc():\n",
    "    if len(tics)==0:\n",
    "        return None\n",
    "    else:\n",
    "        return time.time()-tics.pop()\n",
    "\n",
    "\n",
    "# Used for converting between nats and bits\n",
    "LOG2E = math.log2(math.e)\n",
    "LOGE2 = math.log(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93c740-b7e6-44a6-8651-700dadc0c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Canonical implementation of multi-head self attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        \"\"\"\n",
    "        :param emb:\n",
    "        :param heads:\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        s = emb // heads\n",
    "        # - We will break the embedding into `heads` chunks and feed each to a different attention head\n",
    "\n",
    "        self.tokeys    = nn.Linear(emb, emb, bias=False)\n",
    "        self.toqueries = nn.Linear(emb, emb, bias=False)\n",
    "        self.tovalues  = nn.Linear(emb, emb, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(emb, emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
    "\n",
    "        s = e // h\n",
    "\n",
    "        keys    = self.tokeys(x)\n",
    "        queries = self.toqueries(x)\n",
    "        values  = self.tovalues(x)\n",
    "\n",
    "        keys    = keys.view(b, t, h, s)\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        values  = values.view(b, t, h, s)\n",
    "\n",
    "        # -- We first compute the k/q/v's on the whole embedding vectors, and then split into the different heads.\n",
    "        #    See the following video for an explanation: https://youtu.be/KmAISyVvE1Y\n",
    "\n",
    "        # Compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys    = keys / (e ** (1/4))\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now has row-wise self-attention probabilities\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "\n",
    "        return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c102747-4441-4106-89a4-2196f33831be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0, attention_type='default', pos_embedding=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(emb, heads=heads, mask=mask)\n",
    "\n",
    "\n",
    "        self.mask = mask\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_mult * emb, emb)\n",
    "        )\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attended = self.attention(x)\n",
    "\n",
    "        x = self.norm1(attended + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        fedforward = self.ff(x)\n",
    "\n",
    "        x = self.norm2(fedforward + x)\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6cefd-ef1f-4fdd-bf35-721b2fa3ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for generating text (character by character).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, attention_type='default'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=(seq_length * 2 - 1 if attention_type=='relative' else seq_length))\n",
    "\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=True, attention_type=attention_type, pos_embedding=self.pos_embedding))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.toprobs = nn.Linear(emb, num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A (batch, sequence length) integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = tokens.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
    "        x = tokens + positions\n",
    "\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        x = self.toprobs(x.view(b*t, e)).view(b, t, self.num_tokens)\n",
    "\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "class CTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for classifying sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0, wide=False):\n",
    "        \"\"\"\n",
    "        :param emb: Embedding dimension\n",
    "        :param heads: nr. of attention heads\n",
    "        :param depth: Number of transformer blocks\n",
    "        :param seq_length: Expected maximum sequence length\n",
    "        :param num_tokens: Number of tokens (usually words) in the vocabulary\n",
    "        :param num_classes: Number of classes.\n",
    "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
    "                         average pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
    "\n",
    "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "\n",
    "        self.toprobs = nn.Linear(emb, num_classes)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = tokens.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
    "        x = tokens + positions\n",
    "        x = self.do(x)\n",
    "\n",
    "        x = self.tblocks(x)\n",
    "\n",
    "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n",
    "\n",
    "        x = self.toprobs(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b73b4-7d80-47e8-bbdd-cd681c814198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for converting between nats and bits\n",
    "LOG2E = math.log2(math.e)\n",
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "NUM_CLS = 2\n",
    "\n",
    "def go(arg):\n",
    "    \"\"\"\n",
    "    Creates and trains a basic transformer for the IMDB sentiment classification task.\n",
    "    \"\"\"\n",
    "    tbw = SummaryWriter(log_dir=arg.tb_dir) # Tensorboard logging\n",
    "\n",
    "    # load the IMDB data\n",
    "    if arg.final:\n",
    "        train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "        TEXT.build_vocab(train, max_size=arg.vocab_size - 2)\n",
    "        LABEL.build_vocab(train)\n",
    "\n",
    "        train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=arg.batch_size, device=util.d())\n",
    "    else:\n",
    "        tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "        train, test = tdata.split(split_ratio=0.8)\n",
    "\n",
    "        TEXT.build_vocab(train, max_size=arg.vocab_size - 2) # - 2 to make space for <unk> and <pad>\n",
    "        LABEL.build_vocab(train)\n",
    "\n",
    "        train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=arg.batch_size, device=d())\n",
    "\n",
    "    print(f'- nr. of training examples {len(train_iter)}')\n",
    "    print(f'- nr. of {\"test\" if arg.final else \"validation\"} examples {len(test_iter)}')\n",
    "\n",
    "    if arg.max_length < 0:\n",
    "        mx = max([input.text[0].size(1) for input in train_iter])\n",
    "        mx = mx * 2\n",
    "        print(f'- maximum sequence length: {mx}')\n",
    "    else:\n",
    "        mx = arg.max_length\n",
    "\n",
    "    # create the model\n",
    "    model = CTransformer(emb=arg.embedding_size, heads=arg.num_heads, depth=arg.depth, seq_length=mx, num_tokens=arg.vocab_size, num_classes=NUM_CLS, max_pool=arg.max_pool)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    opt = torch.optim.Adam(lr=arg.lr, params=model.parameters())\n",
    "    sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (arg.lr_warmup / arg.batch_size), 1.0))\n",
    "\n",
    "    # training loop\n",
    "    seen = 0\n",
    "    for e in range(arg.num_epochs):\n",
    "\n",
    "        print(f'\\n epoch {e}')\n",
    "        model.train(True)\n",
    "\n",
    "        for batch in tqdm.tqdm(train_iter):\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            input = batch.text[0]\n",
    "            label = batch.label - 1\n",
    "\n",
    "            if input.size(1) > mx:\n",
    "                input = input[:, :mx]\n",
    "            out = model(input)\n",
    "            loss = F.nll_loss(out, label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradients\n",
    "            # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
    "            if arg.gradient_clipping > 0.0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)\n",
    "\n",
    "            opt.step()\n",
    "            sch.step()\n",
    "\n",
    "            seen += input.size(0)\n",
    "            tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.train(False)\n",
    "            tot, cor= 0.0, 0.0\n",
    "\n",
    "            for batch in test_iter:\n",
    "\n",
    "                input = batch.text[0]\n",
    "                label = batch.label - 1\n",
    "\n",
    "                if input.size(1) > mx:\n",
    "                    input = input[:, :mx]\n",
    "                out = model(input).argmax(dim=1)\n",
    "\n",
    "                tot += float(input.size(0))\n",
    "                cor += float((label == out).sum().item())\n",
    "\n",
    "            acc = cor / tot\n",
    "            print(f'-- {\"test\" if arg.final else \"validation\"} accuracy {acc:.3}')\n",
    "            tbw.add_scalar('classification/test-loss', float(loss.item()), e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"-e\", \"--num-epochs\",\n",
    "                        dest=\"num_epochs\",\n",
    "                        help=\"Number of epochs.\",\n",
    "                        default=5, type=int)\n",
    "\n",
    "    parser.add_argument(\"-b\", \"--batch-size\",\n",
    "                        dest=\"batch_size\",\n",
    "                        help=\"The batch size.\",\n",
    "                        default=4, type=int)\n",
    "\n",
    "    parser.add_argument(\"-l\", \"--learn-rate\",\n",
    "                        dest=\"lr\",\n",
    "                        help=\"Learning rate\",\n",
    "                        default=0.0001, type=float)\n",
    "\n",
    "    parser.add_argument(\"-T\", \"--tb_dir\", dest=\"tb_dir\",\n",
    "                        help=\"Tensorboard logging directory\",\n",
    "                        default='./runs')\n",
    "\n",
    "    parser.add_argument(\"-f\", \"--final\", dest=\"final\",\n",
    "                        help=\"Whether to run on the real test set (if not included, the validation set is used).\",\n",
    "                        action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\"--max-pool\", dest=\"max_pool\",\n",
    "                        help=\"Use max pooling in the final classification layer.\",\n",
    "                        action=\"store_true\")\n",
    "\n",
    "    parser.add_argument(\"-E\", \"--embedding\", dest=\"embedding_size\",\n",
    "                        help=\"Size of the character embeddings.\",\n",
    "                        default=128, type=int)\n",
    "\n",
    "    parser.add_argument(\"-V\", \"--vocab-size\", dest=\"vocab_size\",\n",
    "                        help=\"Number of words in the vocabulary.\",\n",
    "                        default=50_000, type=int)\n",
    "\n",
    "    parser.add_argument(\"-M\", \"--max\", dest=\"max_length\",\n",
    "                        help=\"Max sequence length. Longer sequences are clipped (-1 for no limit).\",\n",
    "                        default=512, type=int)\n",
    "\n",
    "    parser.add_argument(\"-H\", \"--heads\", dest=\"num_heads\",\n",
    "                        help=\"Number of attention heads.\",\n",
    "                        default=8, type=int)\n",
    "\n",
    "    parser.add_argument(\"-d\", \"--depth\", dest=\"depth\",\n",
    "                        help=\"Depth of the network (nr. of self-attention layers)\",\n",
    "                        default=6, type=int)\n",
    "\n",
    "    parser.add_argument(\"-r\", \"--random-seed\",\n",
    "                        dest=\"seed\",\n",
    "                        help=\"RNG seed. Negative for random\",\n",
    "                        default=1, type=int)\n",
    "\n",
    "    parser.add_argument(\"--lr-warmup\",\n",
    "                        dest=\"lr_warmup\",\n",
    "                        help=\"Learning rate warmup.\",\n",
    "                        default=10_000, type=int)\n",
    "\n",
    "    parser.add_argument(\"--gradient-clipping\",\n",
    "                        dest=\"gradient_clipping\",\n",
    "                        help=\"Gradient clipping.\",\n",
    "                        default=1.0, type=float)\n",
    "\n",
    "    options = parser.parse_args([])\n",
    "\n",
    "    print('OPTIONS ', options)\n",
    "\n",
    "    go(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e16b63-b27b-4a81-8762-52bedf0c9871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3ae38-e04b-4993-9901-b15dde39d90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950a414-672e-442a-828a-898fee926012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "093fce78-5071-4e1f-901d-84619f65bdf3",
   "metadata": {},
   "source": [
    "## 2. https://github.com/jsbaan/transformer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000dbff1-c22a-4039-b6f9-e8f6050afaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8b46d-241b-4bd1-9443-11c72d22541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "class Vocabulary:\n",
    "    BOS = \"BOS\"\n",
    "    EOS = \"EOS\"\n",
    "    PAD = \"PAD\"\n",
    "\n",
    "    def __init__(self, list_of_sentences: Optional[List[str]]):\n",
    "        self.token2index = {self.BOS: 0, self.EOS: 1, self.PAD: 2}\n",
    "        self.index2token = {v: k for k, v in self.token2index.items()}\n",
    "        if not list_of_sentences:\n",
    "            return\n",
    "        for sentence in list_of_sentences:\n",
    "            self.add_tokens(self.tokenize(sentence))\n",
    "\n",
    "    def add_tokens(self, tokens: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Adds tokens the vocabulary\n",
    "        :param tokens:\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for token in tokens:\n",
    "            if token not in self.token2index:\n",
    "                i = len(self.token2index.items())\n",
    "                self.token2index[token] = i\n",
    "                self.index2token[i] = token\n",
    "\n",
    "    def tokenize(self, sentence: str, add_special_tokens: bool = True) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split on all tokens and punctuation. Optionally adds BOS and EOS tokens.\n",
    "        :param sentence:\n",
    "        :param add_special_tokens:\n",
    "        :return: List of string tokens\n",
    "        \"\"\"\n",
    "        tokens = re.findall(r\"\\w+|[^\\s\\w]+\", sentence)\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.BOS] + tokens + [self.EOS]\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, sentence: str, add_special_tokens: bool = True) -> List[int]:\n",
    "        \"\"\"\n",
    "        Converts a string to a list of token indices given the vocabulary\n",
    "        :param sentence: a string representing a sentence\n",
    "        :param add_special_tokens: whether or not to add a bos and eos token\n",
    "        :return: list of token indices\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(sentence, add_special_tokens)\n",
    "        return [self.token2index[token] for token in tokens]\n",
    "\n",
    "    def batch_encode(\n",
    "        self, sentences: List[str], padding=True, add_special_tokens: bool = False\n",
    "    ) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Convert a list of string sentences to nested list of token indices. Optionally adds padding & bos+eos tokens\n",
    "        :param sentences: A list of sentences to be encoded into a batch\n",
    "        :param padding: Boolean that allows for padding up to the longest sequence in the batch\n",
    "        :param add_special_tokens: Boolean that allows for adding a BOS and EOS token to each sentence in the batch\n",
    "        :return: nested list of tokenized sequences\n",
    "        \"\"\"\n",
    "        tokenized_sentences = [\n",
    "            self.encode(sentence, add_special_tokens) for sentence in sentences\n",
    "        ]\n",
    "        if padding:\n",
    "            max_length = max([len(tokens) for tokens in tokenized_sentences])\n",
    "            tokenized_sentences = [\n",
    "                s + ((max_length - len(s)) * [self.token2index[self.PAD]])\n",
    "                for s in tokenized_sentences\n",
    "            ]\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb185f1-365e-4074-a347-41bb3f0e5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def construct_future_mask(seq_len: int):\n",
    "    \"\"\"\n",
    "    Construct a binary mask that contains 1's for all valid connections and 0's for all outgoing future connections.\n",
    "    This mask will be applied to the attention logits in decoder self-attention such that all logits with a 0 mask\n",
    "    are set to -inf.\n",
    "    :param seq_len: length of the input sequence\n",
    "    :return: (seq_len,seq_len) mask\n",
    "    \"\"\"\n",
    "    subsequent_mask = torch.triu(torch.full((seq_len, seq_len), 1), diagonal=1)\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "\n",
    "def construct_batches(\n",
    "    corpus: List[Dict[str, str]],\n",
    "    vocab: Vocabulary,\n",
    "    batch_size: int,\n",
    "    src_lang_key: str,\n",
    "    tgt_lang_key: str,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> Tuple[Dict[str, List[torch.Tensor]], Dict[str, List[torch.Tensor]]]:\n",
    "    \"\"\"\n",
    "    Constructs batches given a corpus.\n",
    "    :param corpus: The input corpus is a list of aligned source and target sequences, packed in a dictionary.\n",
    "    :param vocab: The vocabulary object.\n",
    "    :param batch_size: The number of sequences in a batch\n",
    "    :param src_lang_key: The source language key is a string that the source sequences are keyed under. E.g. \"en\"\n",
    "    :param tgt_lang_key: The target language key is a string that the target sequences are keyed under. E.g. \"nl\"\n",
    "    :param device: whether or not to move tensors to gpu\n",
    "    :return: A tuple containing two dictionaries. The first represents the batches, the second the attention masks.\n",
    "    \"\"\"\n",
    "    pad_token_id = vocab.token2index[vocab.PAD]\n",
    "    batches: Dict[str, List] = {\"src\": [], \"tgt\": []}\n",
    "    masks: Dict[str, List] = {\"src\": [], \"tgt\": []}\n",
    "    for i in range(0, len(corpus), batch_size):\n",
    "        src_batch = torch.IntTensor(\n",
    "            vocab.batch_encode(\n",
    "                [pair[src_lang_key] for pair in corpus[i : i + batch_size]],\n",
    "                add_special_tokens=True,\n",
    "                padding=True,\n",
    "            )\n",
    "        )\n",
    "        tgt_batch = torch.IntTensor(\n",
    "            vocab.batch_encode(\n",
    "                [pair[tgt_lang_key] for pair in corpus[i : i + batch_size]],\n",
    "                add_special_tokens=True,\n",
    "                padding=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        src_padding_mask = src_batch != pad_token_id\n",
    "        future_mask = construct_future_mask(tgt_batch.shape[-1])\n",
    "\n",
    "        # Move tensors to gpu; if available\n",
    "        if device is not None:\n",
    "            src_batch = src_batch.to(device)  # type: ignore\n",
    "            tgt_batch = tgt_batch.to(device)  # type: ignore\n",
    "            src_padding_mask = src_padding_mask.to(device)\n",
    "            future_mask = future_mask.to(device)\n",
    "        batches[\"src\"].append(src_batch)\n",
    "        batches[\"tgt\"].append(tgt_batch)\n",
    "        masks[\"src\"].append(src_padding_mask)\n",
    "        masks[\"tgt\"].append(future_mask)\n",
    "    return batches, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe052a-2e76-4f4c-9baa-2bbec3ba8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class SinusoidEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Mostly copied from\n",
    "    https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pos_embed = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim)\n",
    "        )\n",
    "        pos_embed[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_embed[:, 1::2] = torch.cos(position * div_term)\n",
    "        pos_embed = pos_embed.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer(\"pos_embed\", pos_embed, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds positional embeddings to token embeddings.\n",
    "        N = batch size\n",
    "        L = sequence length\n",
    "        E = embedding dim\n",
    "        :param x: token embeddings. Shape: (N, L, E)\n",
    "        :return: token_embeddings + positional embeddings. Shape: (N, L, E)\n",
    "        \"\"\"\n",
    "        x = x + self.pos_embed[:, : x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17488554-839e-4653-a7e2-fc347545aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadAttention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % num_heads == 0\n",
    "        self.qkv_dim = hidden_dim // num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(hidden_dim, 3 * num_heads * self.qkv_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(num_heads * self.qkv_dim, hidden_dim, bias=False)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\" Weight initialization taken from the UvA DL1 PyTorch Transformer tutorial. \"\"\"\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        src_padding_mask: Optional[torch.BoolTensor] = None,\n",
    "        future_mask: Optional[torch.BoolTensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform multi-head attention using one projection matrix. Self attention is performed when encoder_hidden_states\n",
    "        is None, in which case input x represents encoder token embeddings. Otherwise, cross-attention is performed.\n",
    "        In that case, input x represents the decoder hidden states.\n",
    "        N = batch size\n",
    "        S = source sequence length\n",
    "        T = target sequence length\n",
    "        E = embedding dimensionality\n",
    "        :param x: Either encoder or decoder hidden states. Shape: (N, S or T, E)\n",
    "        :param encoder_hidden_states: Encoder hidden states to perform cross-attention with. Shape: (N, S, E)\n",
    "        :param src_padding_mask: Used for encoder self-attention and cross-attention to handle pad tokens.\n",
    "        Masks all incoming \"connections\" or \"logits\" from any token position to any pad token in a sequence.\n",
    "        Shape: (N, S)\n",
    "        :param future_mask: Used for decoder self-attention to avoid any token i attending to a token >i, i.e. \"peaking\"\n",
    "        Shape: (T, T).\n",
    "        :return: Contextualized token embeddings. Shape depends on attention type. (N, S, E) for encoder self-attention\n",
    "        and decoder cross-attention. (N, T, E) for decoder self-attention.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, sequence_length, hidden_dim = x.size()\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            q, k, v = self._self_attention_projection(x)\n",
    "        else:\n",
    "            q, k, v = self._cross_attention_projection(encoder_hidden_states, x)\n",
    "\n",
    "        # Swap dimensions to (batch_size, n_heads, seq_len, qkv_dim). Required for the matrix multiplication below\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # Compute (contextualized) value vector for each \"head\"\n",
    "        values, attn = self.scaled_dot_product(q, k, v, src_padding_mask, future_mask)\n",
    "\n",
    "        # Concatenate contextualized value vectors from all heads\n",
    "        values = values.reshape(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "        # Linearly transform the concatenation of all heads' value vectors (8*64=512) to the original hidden dim (512)\n",
    "        output = self.o_proj(values)\n",
    "        return output\n",
    "\n",
    "    def _self_attention_projection(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Project x and interpret the result as chunks that represent q, k and v vectors for every head.\n",
    "        Input x can be encoder or decoder hidden states, depending on which one calls this MHA module.\n",
    "        N = batch size\n",
    "        S = source sequence length\n",
    "        T = target sequence length\n",
    "        E = embedding dimensionality\n",
    "        H = number of heads\n",
    "        :param x: Encoder or decoder hidden states. (N, S or T, E)\n",
    "        :return: query, key and value vectors. (N, S or T, H, E/H)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, _ = x.shape\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.qkv_dim)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        return q, k, v\n",
    "\n",
    "    def _cross_attention_projection(\n",
    "        self, encoder_hidden_states: torch.Tensor, decoder_hidden_states: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Projects decoder hidden states into query vectors and encoder hidden states into key and value vectors.\n",
    "        The columns of W_proj determine how much independent linear combinations of the input we obtain - which we\n",
    "        then interpret as heads and qkv vectors. Thus we can simply split the weight matrix and project the decoder\n",
    "        hidden states x into q separately from projecting the encoder_hidden_states into k and v.\n",
    "        N = batch size\n",
    "        S = source sequence length\n",
    "        T = target sequence length\n",
    "        E = embedding dimensionality\n",
    "        H = number of heads\n",
    "        :param encoder_hidden_states: Shape: (N, S, E)\n",
    "        :param decoder_hidden_states: Shape: (N, T, E)\n",
    "        :return: query vector: Shape: (N, T, H, E/H) and key and value vectors both (N, S, H, E/H)\n",
    "        \"\"\"\n",
    "        batch_size, src_sequence_length, hidden_dim = encoder_hidden_states.shape\n",
    "        batch_size, tgt_sequence_length, hidden_dim = decoder_hidden_states.shape\n",
    "\n",
    "        # Split weight matrix\n",
    "        w_q, w_kv = self.qkv_proj.weight.split([hidden_dim, 2 * hidden_dim])\n",
    "\n",
    "        # Project encoder_hidden_states into k's, and v's\n",
    "        k, v = (\n",
    "            F.linear(input=encoder_hidden_states, weight=w_kv)\n",
    "            .reshape(batch_size, src_sequence_length, self.num_heads, 2 * self.qkv_dim)\n",
    "            .chunk(2, dim=-1)\n",
    "        )\n",
    "\n",
    "        # Project decoder hidden states into q's\n",
    "        q = F.linear(input=decoder_hidden_states, weight=w_q).reshape(\n",
    "            batch_size, tgt_sequence_length, self.num_heads, self.qkv_dim\n",
    "        )\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    def scaled_dot_product(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        src_padding_mask: Optional[torch.BoolTensor] = None,\n",
    "        future_mask: Optional[torch.BoolTensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        For cross-attention, the sequence length of q and (k,v) may differ as q is projected from decoder hidden states\n",
    "        and kv from encoder hidden states.\n",
    "        N = batch size\n",
    "        S = source sequence length\n",
    "        T = target sequence length\n",
    "        E = embedding dimensionality\n",
    "        H = number of heads\n",
    "        :param q: Tensor stacking query vectors for all tokens and all heads. Shape: (N, H, S or T, E/H)\n",
    "        :param k: Tensor stacking key vectors for all tokens and all heads. Shape: (N, H, S or T, E/H)\n",
    "        :param v: Tensor stacking value vectors for all tokens and all heads. Shape: (N, H, S or T, E/H)\n",
    "        :param src_padding_mask: Used for encoder self-attention and cross-attention to handle pad tokens.\n",
    "        Masks all incoming \"connections\" or \"logits\" from any token position to any pad token in a sequence.\n",
    "        Shape: (N, S)\n",
    "        :param future_mask: Used for decoder self-attention to avoid any token i attending to a token >i, i.e. \"peaking\"\n",
    "        Shape: (T, T).\n",
    "        :return: values (N, H, S or T, E/H), attention scores (N, H, S or T, S or T)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute attention logits. Dot product between each query and key vector, through one matrix multiplication.\n",
    "        # Results in un-normalized attention scores for each position's query vector to each position's key vector\n",
    "        # Result is (batch_size, num_heads, seq_length, seq_length)\n",
    "        attn_logits = torch.matmul(q, torch.transpose(k, -2, -1),)\n",
    "\n",
    "        # Scale logits by constant to create less spiky softmax distribution\n",
    "        attn_logits = attn_logits / math.sqrt(q.size()[-1])\n",
    "\n",
    "        # Apply attention mask (for pad tokens and future-masking in cross-attention)\n",
    "        if src_padding_mask is not None or future_mask is not None:\n",
    "            attn_logits = self.mask_logits(attn_logits, src_padding_mask, future_mask)  # type: ignore\n",
    "\n",
    "        # Transform logits to attention probability distribution (one distribution per non-masked token index)\n",
    "        attention = F.softmax(attn_logits, dim=-1)\n",
    "\n",
    "        # Weighted sum of value vectors for each input token using attention scores -> new contextualized representation\n",
    "        # (batch_size, num_heads, sequence_length, qkv_dim)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "\n",
    "    @staticmethod\n",
    "    def mask_logits(\n",
    "        logits: torch.Tensor,\n",
    "        src_padding_mask: Optional[torch.BoolTensor] = None,\n",
    "        future_mask: Optional[torch.BoolTensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Reshape masks to fit the shape of the logits and set all indices with \"False\" to -inf\n",
    "        N = batch size\n",
    "        S = source sequence length\n",
    "        T = target sequence length\n",
    "        E = embedding dimensionality\n",
    "        H = number of heads\n",
    "        :param logits: Tensor containing attention logits. Shape: (N, H, S or T, S or T)\n",
    "        :param src_padding_mask: Used for encoder self-attention and cross-attention to handle pad tokens.\n",
    "        Masks all incoming \"connections\" or \"logits\" from any token position to any pad token in a sequence.\n",
    "        Shape: (N, S)\n",
    "        :param future_mask: Used for decoder self-attention to avoid any token i attending to a token >i, i.e. \"peaking\"\n",
    "        Shape: (T, T).\n",
    "        :return: masked_logits (N, H, S or T, S or T)\n",
    "        \"\"\"\n",
    "        if src_padding_mask is not None:\n",
    "            masked_logits = logits.masked_fill(\n",
    "                src_padding_mask[:, None, None, :] == 0, float(\"-inf\")\n",
    "            )\n",
    "        if future_mask is not None:\n",
    "            masked_logits = logits.masked_fill(future_mask == 0, float(\"-inf\"))\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f26b51-66a4-4a12-a251-329dcfab74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: torch.nn.Embedding,\n",
    "        hidden_dim: int,\n",
    "        ff_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        dropout_p: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed = embedding\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.positional_encoding = SinusoidEncoding(hidden_dim, max_len=5000)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(hidden_dim, ff_dim, num_heads, dropout_p)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, src_padding_mask: torch.BoolTensor = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs one encoder forward pass given input token ids and an optional attention mask.\n",
    "        N = batch size\n",
    "        S = source sequence length\n",
    "        E = embedding dimensionality\n",
    "        :param input_ids: Tensor containing input token ids. Shape: (N, S)\n",
    "        :param src_padding_mask: An attention mask to ignore pad-tokens in the source input. Shape (N, S)\n",
    "        :return: The encoder's final (contextualized) token embeddings. Shape: (N, S, E)\n",
    "        \"\"\"\n",
    "        x = self.embed(input_ids) * math.sqrt(self.hidden_dim)  # (N, S, E)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block.forward(x, src_padding_mask=src_padding_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, ff_dim: int, num_heads: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.self_mha = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=dropout_p)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, src_padding_mask: torch.BoolTensor = None):\n",
    "        \"\"\"\n",
    "        Performs one encoder *block* forward pass given the previous block's output and an optional attention mask.\n",
    "        N = batch size\n",
    "        S = source sequence length\n",
    "        E = embedding dimensionality\n",
    "        :param x: Tensor containing the output of the previous encoder block. Shape: (N, S, E)\n",
    "        :param src_padding_mask: An attention mask to ignore pad-tokens in the source input. Shape (N, S)\n",
    "        :return: Updated intermediate encoder (contextualized) token embeddings. Shape: (N, S, E)\n",
    "        \"\"\"\n",
    "        output = self.dropout1(\n",
    "            self.self_mha.forward(x, src_padding_mask=src_padding_mask)\n",
    "        )\n",
    "        x = self.layer_norm1(x + output)\n",
    "\n",
    "        output = self.dropout2(self.feed_forward(x))\n",
    "        x = self.layer_norm2(x + output)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c960c1-9052-4ad1-b663-ef356f257b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: torch.nn.Embedding,\n",
    "        hidden_dim: int,\n",
    "        ff_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        vocab_size: int,\n",
    "        dropout_p: float,\n",
    "        tie_output_to_embedding: Optional[bool] = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = embedding\n",
    "        self.positional_encoding = SinusoidEncoding(hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderBlock(hidden_dim, ff_dim, num_heads, dropout_p)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "\n",
    "        # Note: a linear layer multiplies the input with a transpose of the weight matrix, so no need to do that here.\n",
    "        if tie_output_to_embedding:\n",
    "            self.output_layer.weight = nn.Parameter(self.embed.weight)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\" Perform xavier weight initialization\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_tokens: torch.IntTensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        src_padding_mask: Optional[torch.BoolTensor] = None,\n",
    "        future_mask: Optional[torch.BoolTensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs one decoder forward pass given encoder hidden states, the decoder input tokens and attention masks.\n",
    "        N = batch size\n",
    "        S = source sequence length\n",
    "        T = target sequence length\n",
    "        E = embedding dimensionality\n",
    "        V = vocabulary size\n",
    "        :param input_tokens: Decoder input tokens. Shape: (N, T)\n",
    "        :param encoder_hidden_states: The encoder's final (contextualized) token embeddings. Shape: (N, S, E)\n",
    "        :param src_padding_mask: An attention mask to ignore pad-tokens in the source input. Shape (N, S)\n",
    "        :param future_mask: An attention mask to ignore future-tokens in the target input. Shape (T, T)\n",
    "        :return: Unnormalized logits over the vocabulary for every token in the batch. Shape (N, T, V)\n",
    "        \"\"\"\n",
    "        # (batch_size, sequence_length, hidden_dim)\n",
    "        x = self.embed(input_tokens) * math.sqrt(self.hidden_dim)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x, encoder_hidden_states, src_padding_mask, future_mask)\n",
    "\n",
    "        # (batch_size, sequence_length, vocab_size)\n",
    "        logits = self.output_layer(x)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f660fc-51cb-4e53-9f00-7b8f8478e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        ff_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        max_decoding_length: int,\n",
    "        vocab_size: int,\n",
    "        padding_idx: int,\n",
    "        bos_idx: int,\n",
    "        dropout_p: float,\n",
    "        tie_output_to_embedding: Optional[bool] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Because the encoder embedding, and decoder embedding and decoder pre-softmax transformeation share embeddings\n",
    "        # weights, initialize one here and pass it on.\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim, padding_idx=padding_idx)\n",
    "        self.encoder = TransformerEncoder(\n",
    "            self.embed, hidden_dim, ff_dim, num_heads, num_layers, dropout_p\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            self.embed,\n",
    "            hidden_dim,\n",
    "            ff_dim,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            vocab_size,\n",
    "            dropout_p,\n",
    "            tie_output_to_embedding,\n",
    "        )\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.bos_idx = bos_idx\n",
    "        self.max_decoding_length = max_decoding_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced2b59-be89-4e2f-9884-5fe31447f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrScheduler\n",
    "\n",
    "class NoamOpt:\n",
    "    \"\"\"\n",
    "    Copied from https://nlp.seas.harvard.edu/2018/04/03/attention.html#hardware-and-schedule\n",
    "    A wrapper class for the Adam optimizer (or others) that implements learning rate scheduling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update parameters and rate\"\n",
    "        \"\"\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p[\"lr\"] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"\"\"\n",
    "        Implement `lrate` above\n",
    "        \"\"\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (\n",
    "            self.model_size ** (-0.5)\n",
    "            * min(step ** (-0.5), step * self.warmup ** (-1.5))\n",
    "        )\n",
    "\n",
    "\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(\n",
    "        model.encoder.hidden_dim,\n",
    "        2,\n",
    "        4000,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2856d9-885d-4242-9ba0-ce8d96cd296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    transformer: nn.Module,\n",
    "    scheduler: Any,\n",
    "    criterion: Any,\n",
    "    batches: Dict[str, List[torch.Tensor]],\n",
    "    masks: Dict[str, List[torch.Tensor]],\n",
    "    n_epochs: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training loop\n",
    "    :param transformer: the transformer model\n",
    "    :param scheduler: the learning rate scheduler\n",
    "    :param criterion: the optimization criterion (loss function)\n",
    "    :param batches: aligned src and tgt batches that contain tokens ids\n",
    "    :param masks: source key padding mask and target future mask for each batch\n",
    "    :param n_epochs: the number of epochs to train the model for\n",
    "    :return: the accuracy and loss on the latest batch\n",
    "    \"\"\"\n",
    "    transformer.train(True)\n",
    "    num_iters = 0\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        for i, (src_batch, src_mask, tgt_batch, tgt_mask) in enumerate(\n",
    "            zip(batches[\"src\"], masks[\"src\"], batches[\"tgt\"], masks[\"tgt\"])\n",
    "        ):\n",
    "            encoder_output = transformer.encoder(src_batch, src_padding_mask=src_mask)  # type: ignore\n",
    "\n",
    "            # Perform one decoder forward pass to obtain *all* next-token predictions for every index i given its\n",
    "            # previous *gold standard* tokens [1,..., i] (i.e. teacher forcing) in parallel/at once.\n",
    "            decoder_output = transformer.decoder(\n",
    "                tgt_batch,\n",
    "                encoder_output,\n",
    "                src_padding_mask=src_mask,\n",
    "                future_mask=tgt_mask,\n",
    "            )  # type: ignore\n",
    "\n",
    "            # Align labels with predictions: the last decoder prediction is meaningless because we have no target token\n",
    "            # for it. The BOS token in the target is also not something we want to compute a loss for.\n",
    "            decoder_output = decoder_output[:, :-1, :]\n",
    "            tgt_batch = tgt_batch[:, 1:]\n",
    "\n",
    "            # Set pad tokens in the target to -100 so they don't incur a loss\n",
    "            # tgt_batch[tgt_batch == transformer.padding_idx] = -100\n",
    "\n",
    "            # Compute the average cross-entropy loss over all next-token predictions at each index i given [1, ..., i]\n",
    "            # for the entire batch. Note that the original paper uses label smoothing (I was too lazy).\n",
    "            batch_loss = criterion(\n",
    "                decoder_output.contiguous().permute(0, 2, 1),\n",
    "                tgt_batch.contiguous().long(),\n",
    "            )\n",
    "\n",
    "            # Rough estimate of per-token accuracy in the current training batch\n",
    "            batch_accuracy = (\n",
    "                torch.sum(decoder_output.argmax(dim=-1) == tgt_batch)\n",
    "            ) / torch.numel(tgt_batch)\n",
    "\n",
    "            if num_iters % 100 == 0:\n",
    "                print(\n",
    "                    f\"epoch: {e}, num_iters: {num_iters}, batch_loss: {batch_loss}, batch_accuracy: {batch_accuracy}\"\n",
    "                )\n",
    "\n",
    "            # Update parameters\n",
    "            batch_loss.backward()\n",
    "            scheduler.step()\n",
    "            scheduler.optimizer.zero_grad()\n",
    "            num_iters += 1\n",
    "    return batch_loss, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b509a91-084e-46df-ac95-3801edbefaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "class TestTransformerTraining(unittest.TestCase):\n",
    "    seed = 0\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    def test_copy_task(self):\n",
    "        \"\"\"\n",
    "        Test training by trying to (over)fit a simple copy dataset - bringing the loss to ~zero. (GPU required)\n",
    "        \"\"\"\n",
    "        device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "        if device.type == \"cpu\":\n",
    "            print(\"This unit test was not run because it requires a GPU\")\n",
    "            return\n",
    "\n",
    "        # Hyperparameters\n",
    "        synthetic_corpus_size = 600\n",
    "        batch_size = 60\n",
    "        n_epochs = 200\n",
    "        n_tokens_in_batch = 10\n",
    "\n",
    "        # Construct vocabulary and create synthetic data by uniform randomly sampling tokens from it\n",
    "        # Note: the original paper uses byte pair encodings, we simply take each word to be a token.\n",
    "        corpus = [\"These are the tokens that will end up in our vocabulary\"]\n",
    "        vocab = Vocabulary(corpus)\n",
    "        vocab_size = len(\n",
    "            list(vocab.token2index.keys())\n",
    "        )  # 14 tokens including bos, eos and pad\n",
    "        valid_tokens = list(vocab.token2index.keys())[3:]\n",
    "        corpus += [\n",
    "            \" \".join(choices(valid_tokens, k=n_tokens_in_batch))\n",
    "            for _ in range(synthetic_corpus_size)\n",
    "        ]\n",
    "\n",
    "        # Construct src-tgt aligned input batches (note: the original paper uses dynamic batching based on tokens)\n",
    "        corpus = [{\"src\": sent, \"tgt\": sent} for sent in corpus]\n",
    "        batches, masks = construct_batches(\n",
    "            corpus,\n",
    "            vocab,\n",
    "            batch_size=batch_size,\n",
    "            src_lang_key=\"src\",\n",
    "            tgt_lang_key=\"tgt\",\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Initialize transformer\n",
    "        transformer = Transformer(\n",
    "            hidden_dim=512,\n",
    "            ff_dim=2048,\n",
    "            num_heads=8,\n",
    "            num_layers=2,\n",
    "            max_decoding_length=25,\n",
    "            vocab_size=vocab_size,\n",
    "            padding_idx=vocab.token2index[vocab.PAD],\n",
    "            bos_idx=vocab.token2index[vocab.BOS],\n",
    "            dropout_p=0.1,\n",
    "            tie_output_to_embedding=True,\n",
    "        ).to(device)\n",
    "\n",
    "        # Initialize learning rate scheduler, optimizer and loss (note: the original paper uses label smoothing)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9\n",
    "        )\n",
    "        scheduler = NoamOpt(\n",
    "            transformer.hidden_dim, factor=1, warmup=400, optimizer=optimizer,\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Start training and verify ~zero loss and >90% accuracy on the last batch\n",
    "        latest_batch_loss, latest_batch_accuracy = train(\n",
    "            transformer, scheduler, criterion, batches, masks, n_epochs=n_epochs\n",
    "        )\n",
    "        self.assertEqual(latest_batch_loss.item() <= 0.01, True)\n",
    "        self.assertEqual(latest_batch_accuracy >= 0.99, True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1889b1-4799-44fd-9b0f-7bc24aca4139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4d2d8-1e55-4fa5-b9e4-e6e3c35a6985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d412efb-3ee9-42d2-b4db-21d0baa88cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba74b40c-b882-4282-83f1-cc8d8e9a47c8",
   "metadata": {},
   "source": [
    "## 3. https://github.com/fkodom/transformer-from-scratch\n",
    "This code does not include masked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf75b9-5575-4358-91b7-7718989299c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547557b-0c7f-4754-b646-443dce2e3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(\n",
    "    seq_len: int,\n",
    "    dim_model: int,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** torch.div(dim, dim_model, rounding_mode=\"floor\"))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))\n",
    "\n",
    "\n",
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4238c-eb3b-41f3-a5a6-9df8f2093e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_head_attention.py\n",
    "\n",
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_k)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94969ec-7d9e-4102-8851-6b852ad6d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feed_forward(src)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        src += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f28f1-5ac0-4968-9934-5829094d9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        tgt = self.attention_1(tgt, tgt, tgt)\n",
    "        tgt = self.attention_2(tgt, memory, memory)\n",
    "        return self.feed_forward(tgt)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.linear = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
    "        tgt += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        return torch.softmax(self.linear(tgt), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ece41-c413-446e-af24-2ed319a09814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        return self.decoder(tgt, self.encoder(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148aa25d-413c-4e8d-b3a0-43e65b92adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.rand(64, 32, 512)\n",
    "tgt = torch.rand(64, 16, 512)\n",
    "out = Transformer()(src, tgt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a28eb-15b9-4423-82fc-0f1ff27bb282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2901022-0c7c-447c-b360-6b5b80c234a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b10acc-451b-48c3-ab63-b538862671ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b166cd0b-c18a-4749-9fcc-281dc76759c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. https://github.com/SamLynnEvans/Transformer\n",
    "#### Blog:- https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f7d6d-9ce6-41c8-a9a8-61f8b22546b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, time, math, tqdm, random, sys, gzip\n",
    "import argparse\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import  SummaryWriter\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, sys\n",
    "from torch.autograd import Variable\n",
    "# from torchtext import data, datasets, vocab\n",
    "#from torchtext.legacy import data, datasets, vocab\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random, tqdm, sys, math, gzip\n",
    "import re\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from torch.nn.init import xavier_uniform_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e46e6b-35f9-472a-a750-d838312d1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, opt):\n",
    "    \n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    e_output = model.encoder(src, src_mask)\n",
    "    \n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    \n",
    "    trg_mask = nopeak_mask(1, opt)\n",
    "    \n",
    "    out = model.out(model.decoder(outputs,\n",
    "    e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "    \n",
    "    probs, ix = out[:, -1].data.topk(opt.k)\n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "    \n",
    "    outputs = torch.zeros(opt.k, opt.max_len).long()\n",
    "    if opt.device == 0:\n",
    "        outputs = outputs.cuda()\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "    \n",
    "    e_outputs = torch.zeros(opt.k, e_output.size(-2),e_output.size(-1))\n",
    "    if opt.device == 0:\n",
    "        e_outputs = e_outputs.cuda()\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "    \n",
    "    return outputs, e_outputs, log_scores\n",
    "\n",
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "    \n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "    \n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "    \n",
    "    return outputs, log_scores\n",
    "\n",
    "def beam_search(src, model, SRC, TRG, opt):\n",
    "    \n",
    "\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, opt)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    for i in range(2, opt.max_len):\n",
    "    \n",
    "        trg_mask = nopeak_mask(i, opt)\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:,:i],\n",
    "        e_outputs, src_mask, trg_mask))\n",
    "\n",
    "        out = F.softmax(out, dim=-1)\n",
    "    \n",
    "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, opt.k)\n",
    "        \n",
    "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
    "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
    "        for vec in ones:\n",
    "            i = vec[0]\n",
    "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
    "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
    "\n",
    "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
    "\n",
    "        if num_finished_sentences == opt.k:\n",
    "            alpha = 0.7\n",
    "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "    \n",
    "    if ind is None:\n",
    "        length = (outputs[0]==eos_tok).nonzero()[0]\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "    \n",
    "    else:\n",
    "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913cc96-cd65-4d74-a84e-70d6ac649bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed.py\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 200, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b5c2c-2097-4334-9782-c09f1b75a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sublayers \n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * N * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "    \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1c420-6b4e-494a-b2d3-42f7267f3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layer\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, \\\n",
    "        src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae5820-a7b9-442a-a512-28bb0d034a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batch\n",
    "\n",
    "def nopeak_mask(size, opt):\n",
    "    np_mask = np.triu(np.ones((1, size, size)),\n",
    "    k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    if opt.device == 0:\n",
    "      np_mask = np_mask.cuda()\n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg, opt):\n",
    "    \n",
    "    src_mask = (src != opt.src_pad).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != opt.trg_pad).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, opt)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask\n",
    "\n",
    "# patch on Torchtext's batching process that makes it more efficient\n",
    "# from http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n",
    "\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f424b6-adea-4e6a-890f-9da227c34d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Models\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        #print(\"DECODER\")\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "\n",
    "def get_model(opt, src_vocab, trg_vocab):\n",
    "    \n",
    "    assert opt.d_model % opt.heads == 0\n",
    "    assert opt.dropout < 1\n",
    "\n",
    "    model = Transformer(src_vocab, trg_vocab, opt.d_model, opt.n_layers, opt.heads, opt.dropout)\n",
    "       \n",
    "    if opt.load_weights is not None:\n",
    "        print(\"loading pretrained weights...\")\n",
    "        model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights'))\n",
    "    else:\n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) \n",
    "    \n",
    "    if opt.device == 0:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2908e-98d9-480d-be33-32e267b8d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process\n",
    "\n",
    "def read_data(opt):\n",
    "    \n",
    "    if opt.src_data is not None:\n",
    "        try:\n",
    "            opt.src_data = open(opt.src_data).read().strip().split('\\n')\n",
    "        except:\n",
    "            print(\"error: '\" + opt.src_data + \"' file not found\")\n",
    "            quit()\n",
    "    \n",
    "    if opt.trg_data is not None:\n",
    "        try:\n",
    "            opt.trg_data = open(opt.trg_data).read().strip().split('\\n')\n",
    "        except:\n",
    "            print(\"error: '\" + opt.trg_data + \"' file not found\")\n",
    "            quit()\n",
    "\n",
    "def create_fields(opt):\n",
    "    \n",
    "    spacy_langs = ['en', 'fr', 'de', 'es', 'pt', 'it', 'nl']\n",
    "    if opt.src_lang not in spacy_langs:\n",
    "        print('invalid src language: ' + opt.src_lang + 'supported languages : ' + spacy_langs)  \n",
    "    if opt.trg_lang not in spacy_langs:\n",
    "        print('invalid trg language: ' + opt.trg_lang + 'supported languages : ' + spacy_langs)\n",
    "    \n",
    "    print(\"loading spacy tokenizers...\")\n",
    "    \n",
    "    t_src = tokenize(opt.src_lang)\n",
    "    t_trg = tokenize(opt.trg_lang)\n",
    "\n",
    "    TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "    SRC = data.Field(lower=True, tokenize=t_src.tokenizer)\n",
    "\n",
    "    if opt.load_weights is not None:\n",
    "        try:\n",
    "            print(\"loading presaved fields...\")\n",
    "            SRC = pickle.load(open(f'{opt.load_weights}/SRC.pkl', 'rb'))\n",
    "            TRG = pickle.load(open(f'{opt.load_weights}/TRG.pkl', 'rb'))\n",
    "        except:\n",
    "            print(\"error opening SRC.pkl and TXT.pkl field files, please ensure they are in \" + opt.load_weights + \"/\")\n",
    "            quit()\n",
    "        \n",
    "    return(SRC, TRG)\n",
    "\n",
    "def create_dataset(opt, SRC, TRG):\n",
    "\n",
    "    print(\"creating dataset and iterator... \")\n",
    "\n",
    "    raw_data = {'src' : [line for line in opt.src_data], 'trg': [line for line in opt.trg_data]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "    \n",
    "    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
    "    \n",
    "    data_fields = [('src', SRC), ('trg', TRG)]\n",
    "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "\n",
    "    train_iter = MyIterator(train, batch_size=opt.batchsize, device=opt.device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True, shuffle=True)\n",
    "    \n",
    "    os.remove('translate_transformer_temp.csv')\n",
    "\n",
    "    if opt.load_weights is None:\n",
    "        SRC.build_vocab(train)\n",
    "        TRG.build_vocab(train)\n",
    "        if opt.checkpoint > 0:\n",
    "            try:\n",
    "                os.mkdir(\"weights\")\n",
    "            except:\n",
    "                print(\"weights folder already exists, run program with -load_weights weights to load them\")\n",
    "                quit()\n",
    "            pickle.dump(SRC, open('weights/SRC.pkl', 'wb'))\n",
    "            pickle.dump(TRG, open('weights/TRG.pkl', 'wb'))\n",
    "\n",
    "    opt.src_pad = SRC.vocab.stoi['<pad>']\n",
    "    opt.trg_pad = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "    opt.train_len = get_len(train_iter)\n",
    "\n",
    "    return train_iter\n",
    "\n",
    "def get_len(train):\n",
    "\n",
    "    for i, b in enumerate(train):\n",
    "        pass\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7d2e7-fa18-4723-a88b-f0dd8eadc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "class tokenize(object):\n",
    "    \n",
    "    def __init__(self, lang):\n",
    "        self.nlp = spacy.load(lang)\n",
    "            \n",
    "    def tokenizer(self, sentence):\n",
    "        sentence = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a263c-de3f-4bda-8fc3-be2f0f15df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt):\n",
    "    \n",
    "    print(\"training model...\")\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    if opt.checkpoint > 0:\n",
    "        cptime = time.time()\n",
    "                 \n",
    "    for epoch in range(opt.epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        if opt.floyd is False:\n",
    "            print(\"   %dm: epoch %d [%s]  %d%%  loss = %s\" %\\\n",
    "            ((time.time() - start)//60, epoch + 1, \"\".join(' '*20), 0, '...'), end='\\r')\n",
    "        \n",
    "        if opt.checkpoint > 0:\n",
    "            torch.save(model.state_dict(), 'weights/model_weights')\n",
    "                    \n",
    "        for i, batch in enumerate(opt.train): \n",
    "\n",
    "            src = batch.src.transpose(0,1)\n",
    "            trg = batch.trg.transpose(0,1)\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, opt)\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            opt.optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad)\n",
    "            loss.backward()\n",
    "            opt.optimizer.step()\n",
    "            if opt.SGDR == True: \n",
    "                opt.sched.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % opt.printevery == 0:\n",
    "                 p = int(100 * (i + 1) / opt.train_len)\n",
    "                 avg_loss = total_loss/opt.printevery\n",
    "                 if opt.floyd is False:\n",
    "                    print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
    "                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss), end='\\r')\n",
    "                 else:\n",
    "                    print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n",
    "                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss))\n",
    "                 total_loss = 0\n",
    "            \n",
    "            if opt.checkpoint > 0 and ((time.time()-cptime)//60) // opt.checkpoint >= 1:\n",
    "                torch.save(model.state_dict(), 'weights/model_weights')\n",
    "                cptime = time.time()\n",
    "   \n",
    "   \n",
    "        print(\"%dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, loss = %.03f\" %\\\n",
    "        ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88924eaf-9e17-4535-9873-c8d68d3fce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('-src_data', default='data\\english.txt')\n",
    "    parser.add_argument('-trg_data', default='D:\\\\Source\\\\GNN-Tutorials\\\\data\\\\french.txt')\n",
    "    parser.add_argument('-src_lang', default='en_core_web_sm')\n",
    "    parser.add_argument('-trg_lang', default='fr')\n",
    "    parser.add_argument('-no_cuda', action='store_true')\n",
    "    parser.add_argument('-SGDR', action='store_true')\n",
    "    parser.add_argument('-epochs', type=int, default=2)\n",
    "    parser.add_argument('-d_model', type=int, default=512)\n",
    "    parser.add_argument('-n_layers', type=int, default=6)\n",
    "    parser.add_argument('-heads', type=int, default=8)\n",
    "    parser.add_argument('-dropout', type=int, default=0.1)\n",
    "    parser.add_argument('-batchsize', type=int, default=1500)\n",
    "    parser.add_argument('-printevery', type=int, default=100)\n",
    "    parser.add_argument('-lr', type=int, default=0.0001)\n",
    "    parser.add_argument('-load_weights')\n",
    "    parser.add_argument('-create_valset', action='store_true')\n",
    "    parser.add_argument('-max_strlen', type=int, default=80)\n",
    "    parser.add_argument('-floyd', action='store_true')\n",
    "    parser.add_argument('-checkpoint', type=int, default=0)\n",
    "\n",
    "    opt = parser.parse_args([])\n",
    "    \n",
    "    opt.device = 0 if opt.no_cuda is False else -1\n",
    "    if opt.device == 0:\n",
    "        assert torch.cuda.is_available()\n",
    "    \n",
    "    read_data(opt)\n",
    "    SRC, TRG = create_fields(opt)\n",
    "    opt.train = create_dataset(opt, SRC, TRG)\n",
    "    model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n",
    "\n",
    "    opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "    if opt.SGDR == True:\n",
    "        opt.sched = CosineWithRestarts(opt.optimizer, T_max=opt.train_len)\n",
    "\n",
    "    if opt.checkpoint > 0:\n",
    "        print(\"model weights will be saved every %d minutes and at end of epoch to directory weights/\"%(opt.checkpoint))\n",
    "    \n",
    "    if opt.load_weights is not None and opt.floyd is not None:\n",
    "        os.mkdir('weights')\n",
    "        pickle.dump(SRC, open('weights/SRC.pkl', 'wb'))\n",
    "        pickle.dump(TRG, open('weights/TRG.pkl', 'wb'))\n",
    "    \n",
    "    train_model(model, opt)\n",
    "\n",
    "    if opt.floyd is False:\n",
    "        promptNextAction(model, opt, SRC, TRG)\n",
    "\n",
    "def yesno(response):\n",
    "    while True:\n",
    "        if response != 'y' and response != 'n':\n",
    "            response = input('command not recognised, enter y or n : ')\n",
    "        else:\n",
    "            return response\n",
    "\n",
    "def promptNextAction(model, opt, SRC, TRG):\n",
    "\n",
    "    saved_once = 1 if opt.load_weights is not None or opt.checkpoint > 0 else 0\n",
    "    \n",
    "    if opt.load_weights is not None:\n",
    "        dst = opt.load_weights\n",
    "    if opt.checkpoint > 0:\n",
    "        dst = 'weights'\n",
    "\n",
    "    while True:\n",
    "        save = yesno(input('training complete, save results? [y/n] : '))\n",
    "        if save == 'y':\n",
    "            while True:\n",
    "                if saved_once != 0:\n",
    "                    res = yesno(\"save to same folder? [y/n] : \")\n",
    "                    if res == 'y':\n",
    "                        break\n",
    "                dst = input('enter folder name to create for weights (no spaces) : ')\n",
    "                if ' ' in dst or len(dst) < 1 or len(dst) > 30:\n",
    "                    dst = input(\"name must not contain spaces and be between 1 and 30 characters length, enter again : \")\n",
    "                else:\n",
    "                    try:\n",
    "                        os.mkdir(dst)\n",
    "                    except:\n",
    "                        res= yesno(input(dst + \" already exists, use anyway? [y/n] : \"))\n",
    "                        if res == 'n':\n",
    "                            continue\n",
    "                    break\n",
    "            \n",
    "            print(\"saving weights to \" + dst + \"/...\")\n",
    "            torch.save(model.state_dict(), f'{dst}/model_weights')\n",
    "            if saved_once == 0:\n",
    "                pickle.dump(SRC, open(f'{dst}/SRC.pkl', 'wb'))\n",
    "                pickle.dump(TRG, open(f'{dst}/TRG.pkl', 'wb'))\n",
    "                saved_once = 1\n",
    "            \n",
    "            print(\"weights and field pickles saved to \" + dst)\n",
    "\n",
    "        res = yesno(input(\"train for more epochs? [y/n] : \"))\n",
    "        if res == 'y':\n",
    "            while True:\n",
    "                epochs = input(\"type number of epochs to train for : \")\n",
    "                try:\n",
    "                    epochs = int(epochs)\n",
    "                except:\n",
    "                    print(\"input not a number\")\n",
    "                    continue\n",
    "                if epochs < 1:\n",
    "                    print(\"epochs must be at least 1\")\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            opt.epochs = epochs\n",
    "            train_model(model, opt)\n",
    "        else:\n",
    "            print(\"exiting program...\")\n",
    "            break\n",
    "\n",
    "    # for asking about further training use while true loop, and return\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a6fe6-b698-48ae-94ee-51893c2190fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9a4d3-c490-411e-a820-6160148220dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-src_data', default='english.txt')\n",
    "parser.add_argument('-trg_data', default='english1.txt')\n",
    "parser.add_argument('-src_lang', default='en')\n",
    "parser.add_argument('-trg_lang', default='fr')\n",
    "parser.add_argument('-no_cuda', action='store_true')\n",
    "parser.add_argument('-SGDR', action='store_true')\n",
    "parser.add_argument('-epochs', type=int, default=2)\n",
    "parser.add_argument('-d_model', type=int, default=512)\n",
    "parser.add_argument('-n_layers', type=int, default=6)\n",
    "parser.add_argument('-heads', type=int, default=8)\n",
    "parser.add_argument('-dropout', type=int, default=0.1)\n",
    "parser.add_argument('-batchsize', type=int, default=1500)\n",
    "parser.add_argument('-printevery', type=int, default=100)\n",
    "parser.add_argument('-lr', type=int, default=0.0001)\n",
    "parser.add_argument('-load_weights')\n",
    "parser.add_argument('-create_valset', action='store_true')\n",
    "parser.add_argument('-max_strlen', type=int, default=80)\n",
    "parser.add_argument('-floyd', action='store_true')\n",
    "parser.add_argument('-checkpoint', type=int, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc85c0-837b-4a6f-99bd-59fd4d30151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13864f1a-e67f-44cb-8b17-379344d6198b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930da974-ca1f-44a6-b8da-e7f19941c7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99f7e8-3c24-451a-8390-e3cdb4cbe7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5ca24-0a80-41fb-8c5d-974f5898379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05657f71-97c2-429f-a7f3-4fb23f465a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len], \\\n",
    "        requires_grad=False).cuda()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302a3d7-6ba1-4843-9fd0-27e1956b912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "input_seq = batch.English.transpose(0,1)\n",
    "input_pad = EN_TEXT.vocab.stoi['<pad>']\n",
    "# creates mask with 0s wherever there is padding in the input\n",
    "input_msk = (input_seq != input_pad).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b62a8b-2949-4e23-ae4c-696daf99bbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49ab05-2f78-4881-9c24-e73dc2ce63ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d80a5-3a94-47f0-89e2-af9e1a9f83a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6b627-013e-49e1-8cf8-79c794a2e88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85bf769a-6852-467f-9a37-6676fec96ee2",
   "metadata": {},
   "source": [
    "## 5. https://github.com/PytLab/transformer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5cd25f-9627-4bac-8a49-99d144514b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca427e-5900-4070-ba17-580d774d766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [123, 0, 23, 5] -> [[..512..], [...512...], ...]\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51759786-97c0-443d-8ba1-96f40a2d4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant positional encoding matrix\n",
    "        pe_matrix = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe_matrix[pos, i] = math.sin(pos/10000**(2*i/d_model))\n",
    "                pe_matrix[pos, i+1] = math.cos(pos/10000**(2*i/d_model))\n",
    "        pe_matrix = pe_matrix.unsqueeze(0)     # Add one dimension for batch size\n",
    "        self.register_buffer('pe', pe_matrix)  # Register as persistent buffer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a sentence after embedding with dim (batch, number of words, vector dimension)\n",
    "        seq_len = x.size()[1]\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10348c-966a-4b37-8ce3-0046658e42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given Query, Key, Value, calculate the final weighted value\n",
    "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
    "    # Shape of q and k are the same, both are (batch_size, seq_len, d_k)\n",
    "    # Shape of v is (batch_size, seq_len, d_v)\n",
    "    attention_scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(q.shape[-1])  # size (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # Apply mask to scores\n",
    "    # <pad>\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, value=-1e9)\n",
    "        \n",
    "    # Softmax along the last dimension\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "        \n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947859d-8df6-4917-986f-e40fef15086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = self.d_v = d_model//n_heads\n",
    "        \n",
    "        # self attention linear layers\n",
    "        # Linear layers for q, k, v vectors generation in different heads\n",
    "        self.q_linear_layers = []\n",
    "        self.k_linear_layers = []\n",
    "        self.v_linear_layers = []\n",
    "        for i in range(n_heads):\n",
    "            self.q_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.k_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.v_linear_layers.append(torch.nn.Linear(d_model, self.d_v))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(n_heads*self.d_v, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        multi_head_attention_outputs = []\n",
    "        for q_linear, k_linear, v_linear in zip(self.q_linear_layers,\n",
    "                                                self.k_linear_layers,\n",
    "                                                self.v_linear_layers):\n",
    "            new_q = q_linear(q)  # size: (batch_size, seq_len, d_k)\n",
    "            new_k = k_linear(k)  # size: (batch_size, seq_len, d_k)\n",
    "            new_v = v_linear(v)  # size: (batch_size, seq_len, d_v)\n",
    "            \n",
    "            # Scaled Dot-Product attention\n",
    "            head_v = scaled_dot_product_attention(new_q, new_k, new_v, mask, self.dropout)  # (batch_size, seq_len, d_v)\n",
    "            multi_head_attention_outputs.append(head_v)\n",
    "            \n",
    "        # Concat\n",
    "        #import pdb; pdb.set_trace()\n",
    "        concat = torch.cat(multi_head_attention_outputs, -1)  # (batch_size, seq_len, n_heads*d_v)\n",
    "        \n",
    "        # Linear layer to recover to original shap\n",
    "        output = self.out(concat)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cef7cb-3dde-48aa-a156-b9eb869c5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear_2 = torch.nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e488922-39cb-4b3c-a485-fe9ffa975e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.alpha = torch.nn.Parameter(torch.ones(self.d_model))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(self.d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x size: (batch_size, seq_len, d_model)\n",
    "        x_hat = (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps)\n",
    "        x_tilde = self.alpha*x_hat + self.beta\n",
    "        return x_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f262a-28b3-4144-b7ce-55dd3705f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        x = x + self.dropout_1(self.multi_head_attention(x, x, x, mask))\n",
    "        x = self.norm_1(x)\n",
    "        \n",
    "        x = x + self.dropout_2(self.feed_forward(x))\n",
    "        x = self.norm_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c391573-81b5-4a95-9b35-3a73e00d59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.norm_3 = LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_3 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.multi_head_attention_1 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(n_heads, d_model)\n",
    "        \n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, trg_mask):\n",
    "        x = self.dropout_1(self.multi_head_attention_1(x, x, x, trg_mask))\n",
    "        x = x + self.norm_1(x)\n",
    "        \n",
    "        x = self.dropout_2(self.multi_head_attention_2(x, encoder_output, encoder_output, src_mask))\n",
    "        x = x + self.norm_2(x)\n",
    "        \n",
    "        x = self.dropout_3(self.feed_forward(x))\n",
    "        x = x + self.norm_3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a168275-6079-4d9a-b994-e8032ce4a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clone_layer(module, N):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6510d-dcad-416b-a44a-9ab99f5854aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.encoder_layers = clone_layer(EncoderLayer(d_model, n_heads), N)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for encoder in self.encoder_layers:\n",
    "            x = encoder(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95d230-86eb-45f8-8b02-bf6b10927f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.decoder_layers = clone_layer(DecoderLayer(d_model, n_heads), N)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, trg, encoder_output, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for decoder in self.decoder_layers:\n",
    "            x = decoder(x, encoder_output, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1890b-b87e-4992-b100-4bb3375e4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, N, n_heads)\n",
    "        self.decoder = Decoder(trg_vocab_size, d_model, N, n_heads)\n",
    "        self.linear = torch.nn.Linear(d_model, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(trg, encoder_output, src_mask, trg_mask)\n",
    "        output = self.linear(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131be00-413a-44d6-b4df-b04684b5b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b83c0-ae72-41b3-ae56-f6facf4d888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "tokenizer = lambda sentence: [tok.text for tok in nlp.tokenizer(sentence) if tok.text != \" \"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f093dbf-88f9-45ec-bb77-81c7ec456420",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(lower=True, tokenize=tokenizer)\n",
    "TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2966a5-a22f-482c-b27f-b18e1322f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data = open('data/english.txt', 'r', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c8e3d-fcf3-4c00-b080-13ab01150ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_data = open('data/french.txt', 'r', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa5935-65ae-409e-8e31-c9f72f9a65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'src': [line for line in src_data], 'trg': [line for line in trg_data]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9a15f-f5db-4680-a578-fcbc520f3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ead49-80a7-4d12-9003-2a026aed7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(raw_data, columns=['src', 'trg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00949c49-54e0-4816-8dc4-ee1cdb71e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('en_to_fr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613aa959-946d-4518-a13d-d23bb52263b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('src', SRC), ('trg', TRG)]\n",
    "train_set = data.TabularDataset('./en_to_fr.csv', format='csv', fields=data_fields)\n",
    "SRC.build_vocab(train_set)\n",
    "TRG.build_vocab(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad714c-e9c6-486b-8890-e463fa08ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "N = 6\n",
    "src_vocab_size = len(SRC.vocab)\n",
    "trg_vocab_size = len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830670b-0a7a-40da-a460-8cd447f38b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(src_vocab_size, trg_vocab_size, d_model, N, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca09d0-2094-4002-b97d-c3fc692356f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913e338-6f2e-4889-b225-69efdc57d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea964f-e457-4d6d-9ff1-c694bffbbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.Iterator(train_set, batch_size=32, sort_key=lambda x: (len(x.src), len(x.trg)), shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda84a9-6c4f-4e03-96e9-0c860dc0a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_mask(src_input, trg_input):\n",
    "    # Source input mask\n",
    "    pad = SRC.vocab.stoi['<pad>']\n",
    "    src_mask = (src_input != pad).unsqueeze(1)\n",
    "    \n",
    "    # Target input mask\n",
    "    trg_mask = (trg_input != pad).unsqueeze(1)\n",
    "    \n",
    "    seq_len = trg_input.size(1)\n",
    "    nopeak_mask = np.tril(np.ones((1, seq_len, seq_len)), k=0).astype('uint8')\n",
    "    nopeak_mask = torch.from_numpy(nopeak_mask) != 0\n",
    "    trg_mask = trg_mask & nopeak_mask\n",
    "    \n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c25274a-a201-4a91-8e17-0171fb9f39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(n_epochs, output_interval=100):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            \n",
    "            src_input = batch.src.transpose(0, 1)  # size (batch_size, seq_len)\n",
    "            trg = batch.trg.transpose(0, 1)  # size (batch_size, seq_len)\n",
    "            \n",
    "            trg_input = trg[:, :-1]\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # create src & trg masks\n",
    "            src_mask, trg_mask = create_mask(src_input, trg_input)\n",
    "            preds = model(src_input, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data\n",
    "\n",
    "            if (i + 1) % output_interval == 0:\n",
    "                avg_loss = total_loss/output_interval\n",
    "                print('time = {}, epoch = {}, iter = {}, loss = {}'.format((time.time() - start)/60,\n",
    "                                                                           epoch + 1,\n",
    "                                                                           i + 1,\n",
    "                                                                           avg_loss))\n",
    "                total_loss = 0\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48247205-e264-480c-b2b1-176af0e88baa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_model(3, output_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565617f8-dceb-4000-a53c-fcec3b5810cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48259b90-69e7-4168-9bac-93a854b30375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f489b137-4161-4979-9f2c-f2ae2894c28b",
   "metadata": {},
   "source": [
    "## 6. https://github.com/tree-park/bert-pretrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9643fcd-2d72-4f17-abfc-a901081d12a1",
   "metadata": {},
   "source": [
    "### https://github.com/tree-park/bert-pretrainer/tree/main/transformer-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657be66e-aba5-4512-b80e-a7497fa8f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding\n",
    "\"\"\"\n",
    "Word Embedding & Positional Embedding\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Word Embedding\n",
    "    Let the model learn sequence information with positional-encoding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # self.affine = Affine(vocab_size, emb_dim)\n",
    "        self.embedding = WordEmbedding(vocab_size, emb_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def word_emb(self, inp):\n",
    "        scale = torch.sqrt(torch.FloatTensor([inp.size(0)]))\n",
    "        out = self.embedding(inp) / scale\n",
    "        return out\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): [bsize, maxlen, emb_dim]\n",
    "        Returns: [bsize, maxlen, emb_dim]\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        임베딩값 dim 값으로 나눠주는거 놓침 \n",
    "        \"\"\"\n",
    "        # [bsize, maxlen, emb_dim]\n",
    "        out = self.embedding(inp)\n",
    "        # [bsize, maxlen, emb_dim]\n",
    "        pe_rst = positional_embedding(out.size(0), out.size(1), out.size(2))\n",
    "        # [bsize, maxlen, emb_dim]\n",
    "        return self.dropout(out + pe_rst)\n",
    "\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        # self.affine = Affine(vocab_size, emb_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        scale = torch.sqrt(torch.FloatTensor([inp.size(0)]))\n",
    "        out = self.embedding(inp) / scale\n",
    "        return out\n",
    "\n",
    "\n",
    "def positional_embedding(bsize, maxlen, d_m):\n",
    "    out = torch.stack(\n",
    "        [positional_encoding(maxlen, d_m)] * bsize\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def positional_encoding(maxlen, dim):\n",
    "    \"\"\" Give unique value by position and dimension \"\"\"\n",
    "\n",
    "    def term(i):\n",
    "        return 1 / (10000 ** (2 * (i // 2) / dim))\n",
    "\n",
    "    pos = torch.as_tensor(np.arange(maxlen))\n",
    "    dims = np.arange(dim)\n",
    "    dims = torch.tensor(list(map(lambda x: term(x), dims)))\n",
    "    # [maxlen, dim]\n",
    "    pe_val = pos.unsqueeze(1) * dims\n",
    "    # [maxlen, dim]\n",
    "    pe = torch.zeros(maxlen, dim)\n",
    "    pe[:, 0::2] = torch.sin(pe_val[:, 0::2])\n",
    "    pe[:, 1::2] = torch.cos(pe_val[:, 0::2])\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b2d3e-eaf3-4329-9a49-035f5fc6d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modules\n",
    "\"\"\"\n",
    "Low level layers\n",
    "    - Linear Layer\n",
    "    - Attention\n",
    "    - Multi-head Attention\n",
    "    - Feed Forward Layer\n",
    "    - Add & Norm Layer\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Affine(nn.Module):\n",
    "    \"\"\" Fully Connected Layer \"\"\"\n",
    "\n",
    "    def __init__(self, i_dim, o_dim):\n",
    "        super(Affine, self).__init__()\n",
    "        self.W = nn.Parameter(nn.init.xavier_normal_(torch.empty(i_dim, o_dim)))\n",
    "        self.b = nn.Parameter(nn.init.uniform_(torch.empty(o_dim)))\n",
    "\n",
    "    def forward(self, inp, linear=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp ([Tensor]): [bsize, maxlen, emb_size]\n",
    "            linear (bool): bool\n",
    "        Returns: [bsize, maxlen, hid_size]\n",
    "        \"\"\"\n",
    "        # [bsize, maxlen, emb_size] * [emb_size, hid_size]\n",
    "        if linear:\n",
    "            return torch.mm(inp, self.W) + self.b\n",
    "        return F.relu((torch.matmul(inp, self.W)) + self.b)\n",
    "\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "    def __init__(self, d_inp, eps=1e-05):\n",
    "        super(NormLayer, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = Affine(d_inp, d_inp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): [bsize, maxlen, dim]\n",
    "        Returns: [bsize, maxlen, dim]\n",
    "        \"\"\"\n",
    "        return self.gamma((x - torch.mean(x)) / torch.sqrt(torch.var(x) + self.eps))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    \"\"\" Scaled Dot-product Attention \"\"\"\n",
    "\n",
    "    def __init__(self, d_inp, d_q, d_k, d_v):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wq = Affine(d_inp, d_q)\n",
    "        self.Wk = Affine(d_inp, d_k)\n",
    "        self.Wv = Affine(d_inp, d_v)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (): [bsize, maxlen, d_m]\n",
    "            key (): [bsize, maxlen, d_m]\n",
    "            value (): [bsize, maxlen, d_m]\n",
    "            mask (): [bsize, ?, maxlen]\n",
    "        Returns:  [bsize, maxlen, d_k]\n",
    "        \"\"\"\n",
    "        # [bsize, maxlen, d_k]\n",
    "        wq = self.Wq(query)\n",
    "        wk = self.Wk(key)\n",
    "        wv = self.Wv(value)\n",
    "        # attention distribution\n",
    "        # Energy [bsize, maxlen, d_q] @ [bsize, d_k, maxlen] = [bsize, maxlen, maxlen]\n",
    "        attn_dstr = torch.bmm(wq, torch.transpose(wk, 1, 2)) / torch.sqrt(torch.FloatTensor([key.size(-1)]))\n",
    "        if mask is not None:\n",
    "            attn_dstr = attn_dstr.masked_fill(mask == 0, -1e10)\n",
    "        attn_dstr = F.softmax(attn_dstr, dim=2)\n",
    "        # [bsize, maxlen, maxlen] @ [bsize, maxlen, d_v] = [bsize, maxlen, d_v]\n",
    "        attn = torch.bmm(attn_dstr, wv)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ab32e-0771-469b-a9d5-163916e2c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sublayer\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-head Attention + Add&Norm \"\"\"\n",
    "\n",
    "    def __init__(self, d_m, n_head=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_m % n_head == 0\n",
    "        d_k = int(d_m / n_head)\n",
    "        self.head = n_head\n",
    "        self.Wo = Affine(d_m, d_m)\n",
    "        self.attn_layers = nn.ModuleList(\n",
    "            [Attention(d_m, d_k, d_k, d_k) for _ in range(n_head)])\n",
    "        self.dropout = nn.Dropout(p=0.01)\n",
    "        self.addnorm = NormLayer(d_m)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (Tensor): [batch size, maxlen, d_m]\n",
    "            key (Tensor): [batch size, maxlen, d_m]\n",
    "            value (Tensor): [batch size, maxlen, d_m]\n",
    "            mask (Tensor): [batch size, ?, maxlen]\n",
    "        Returns: [batch size, maxlen, d_m]\n",
    "        \"\"\"\n",
    "        heads = []\n",
    "        for layer in self.attn_layers:  # TODO 이게 맞는지 확인\n",
    "            # head : [batch size, maxlen, d_k]\n",
    "            head = layer(query, key, value, mask)\n",
    "            heads.append(head)\n",
    "        # [batch size, maxlen, d_k*head]\n",
    "        multi_attn = self.Wo(torch.cat(heads, dim=2))\n",
    "        multi_attn = self.dropout(multi_attn)\n",
    "\n",
    "        # [batch size, maxlen, d_k*head]\n",
    "        resdl = query + multi_attn\n",
    "        # [batch size, maxlen, d_k*head]\n",
    "        out = self.addnorm(resdl)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionWiseFFLayer(nn.Module):\n",
    "    \"\"\" Position-wise FeedForward + Add&Norm \"\"\"\n",
    "\n",
    "    def __init__(self, d_m, d_ff):\n",
    "        super(PositionWiseFFLayer, self).__init__()\n",
    "        self.W1 = Affine(d_m, d_ff)\n",
    "        self.W2 = Affine(d_ff, d_m)\n",
    "        self.dropout = nn.Dropout(p=0.01)\n",
    "        self.addnorm = NormLayer(d_m)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp (Tensor): [batch size, maxlen, d_m]\n",
    "        Returns: [batch size, maxlen, d_m]\n",
    "        \"\"\"\n",
    "        # [batch size, maxlen, d_ff]\n",
    "        out = torch.relu(self.W1(inp))\n",
    "        # [batch size, maxlen, d_m]\n",
    "        out = self.W2(out)\n",
    "        resdl = inp + self.dropout(out)\n",
    "        # [batch size, maxlen, d_m]\n",
    "        out = self.addnorm(resdl)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d81c9-5e62-4ace-b93f-0179e76f892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layer\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_m, d_ff):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.multi_attn = MultiHeadAttention(d_m)\n",
    "        self.pw_ff = PositionWiseFFLayer(d_m, d_ff)\n",
    "\n",
    "    def forward(self, inp, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp (Tensor): [batch size, maxlen, d_m]\n",
    "            mask (Tensor): [batch size, 1, maxlen]\n",
    "        Returns: [batch size, maxlen, d_m]\n",
    "        \"\"\"\n",
    "        # Sub-layer 1\n",
    "        out = self.multi_attn(inp, inp, inp, mask)\n",
    "        # Sub-layer 2\n",
    "        out = self.pw_ff(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, inp_dim, d_m, d_ff):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.multi_attn = MultiHeadAttention(d_m, inp_dim)\n",
    "        self.multi_attn = MultiHeadAttention(d_m, inp_dim)\n",
    "        self.pw_ff = PositionWiseFFLayer(d_m, d_ff)\n",
    "\n",
    "    def forward(self, inp, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp (Tensor): [batch size, maxlen, d_m]\n",
    "            enc_out (Tensor): [batch size, maxlen, d_m]\n",
    "            src_mask (Tensor): [batch size, 1, maxlen]\n",
    "            trg_mask (Tensor): [batch size, maxlen, maxlen]\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        # Sub-layer 1\n",
    "        # [batch size, maxlen, d_m]\n",
    "        out = self.multi_attn(inp, inp, inp, trg_mask)  # masked self attention\n",
    "        # Sub-layer 2\n",
    "        # [batch size, maxlen, d_m]\n",
    "        out = self.multi_attn(out, enc_out, enc_out, src_mask)  # encoder-decoder attention\n",
    "        # Sub-layer 3\n",
    "        # [batch size, maxlen, d_m]\n",
    "        out = self.pw_ff(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff965c-690a-4c83-945e-d0ce964cecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transformer\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\" Assemble layers to build Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, d_m, vocab_size, d_ff, n=3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.inp_emb = WordEmbedding(vocab_size, d_m)\n",
    "        self.out_emb = WordEmbedding(vocab_size, d_m)\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [Encoder(d_m, d_ff) for _ in range(n)])\n",
    "        self.dec_layers = nn.ModuleList(\n",
    "            [Decoder(d_m, d_m, d_ff) for _ in range(n)])\n",
    "        self.affine = Affine(d_m, vocab_size)\n",
    "        self.n = n\n",
    "\n",
    "    def encoder(self, inp_batch, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp_batch (Tensor): [batch size, maxlen]\n",
    "            src_mask (Tensor): [bsize, 1, maxlen]\n",
    "        Returns: [batch size, maxlen, d_m]\n",
    "        \"\"\"\n",
    "        # [batch size, maxlen, d_m]\n",
    "        i_emb = self.inp_emb(inp_batch)\n",
    "        # Encoder\n",
    "        enc = i_emb\n",
    "        for layer in self.enc_layers:\n",
    "            # [batch size, maxlen, d_m]\n",
    "            enc = layer(enc, src_mask)\n",
    "        return enc\n",
    "\n",
    "    def forward(self, inp_batch, out_batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp_batch (Tensor): [batch size, maxlen]\n",
    "            out_batch (Tensor): [batch size, maxlen]\n",
    "        Returns: [batch size, maxlen, vocab_size]\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        src_mask = mask_not_pad(inp_batch)\n",
    "        # [batch size, maxlen, d_m]\n",
    "        enc = self.encoder(inp_batch, src_mask)\n",
    "\n",
    "        # Decoder\n",
    "        trg_mask = mask_get_dec(out_batch)\n",
    "        # [batch size, maxlen, d_m]\n",
    "        o_emb = self.out_emb(out_batch)\n",
    "        dec = o_emb\n",
    "        for layer in self.dec_layers:\n",
    "            # [batch size, maxlen, d_m]\n",
    "            dec = layer(dec, enc, src_mask, trg_mask)\n",
    "        # [batch size, maxlen, vocab_size]\n",
    "        rst = F.log_softmax(self.affine(dec), dim=2)\n",
    "        return rst\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, inp_batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp_batch (Tensor): [batch size, maxlen]\n",
    "        Returns: [batch size, maxlen, vocab_size]\n",
    "        \"\"\"\n",
    "        src_mask = mask_not_pad(inp_batch)\n",
    "        # [batch size, maxlen, d_m]\n",
    "        enc = self.encoder(inp_batch, src_mask)\n",
    "        # [batch size, maxlen, d_m] @ [d_m, vocab_size]\n",
    "        # => [batch size, maxlen, vocab_size]\n",
    "        rst = F.log_softmax(self.affine(enc), dim=2)\n",
    "        rst = torch.argmax(rst, dim=-1).tolist()\n",
    "        return rst\n",
    "\n",
    "\n",
    "def mask_not_pad(x):\n",
    "    \"\"\"\n",
    "    Mark True at PAD\n",
    "    Args:\n",
    "        x (Tensor): [bsize, maxlen] with word idx\n",
    "    Returns: [bsize, 1, maxlen] with bool if idx <=0, True\n",
    "    \"\"\"\n",
    "    return (x > 0).unsqueeze(1)\n",
    "\n",
    "\n",
    "def mask_get_dec(x):\n",
    "    \"\"\"\n",
    "    Mark dec right sequence\n",
    "    Args:\n",
    "        x (Tensor): [bsize, maxlen] with bool\n",
    "    Returns: [bsize, maxlen, maxlen] with bool\n",
    "    \"\"\"\n",
    "    # [bsize, 1, maxlen]\n",
    "    pad_masked = mask_not_pad(x)\n",
    "    # [maxlen, maxlen]\n",
    "    seq_masked = torch.tril(torch.ones(x.size(1), x.size(1)))\n",
    "    # [bsize, maxlen, maxlen]\n",
    "    seq_masked = seq_masked.unsqueeze(0).repeat(x.size(0), 1, 1)\n",
    "    # [bsize, maxlen, maxlen]\n",
    "    masked = seq_masked.masked_fill(pad_masked == 0, 0)\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af884862-846b-4fcd-ba60-da7dd55b8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_feature, eps=0.01, momentum=0.9):  # maxlen\n",
    "        super(BatchNorm, self).__init__()\n",
    "        shape = 1, 1, num_feature  # (batch, maxlen, hidd), norm target is hidd\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.gamma = nn.Parameter(nn.init.xavier_normal_(torch.empty(shape)))\n",
    "        self.beta = nn.Parameter(nn.init.xavier_normal_(torch.empty(shape)))\n",
    "\n",
    "        # The variables that are not model parameters are initialized to 0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def update_movings(self, mean, var):\n",
    "        self.moving_mean = self.momentum * self.moving_mean + (1 - self.momentum) * mean\n",
    "        self.moving_var = self.momentum * self.moving_var + (1 - self.momentum) * var\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # If `X` is not on the main memory, copy `moving_mean` and\n",
    "        # `moving_var` to the device where `X` is located\n",
    "        if not torch.is_grad_enabled():\n",
    "            self.moving_mean = self.moving_mean.to(batch.device)\n",
    "            self.moving_var = self.moving_var.to(batch.device)\n",
    "            normed = (batch - self.moving_mean) / torch.sqrt(self.moving_var + self.eps)\n",
    "        else:\n",
    "            mean = torch.mean(batch, dim=(0, 1), keepdim=True)\n",
    "            var = torch.var(batch, dim=(0, 1), keepdim=True)\n",
    "            normed = (batch - mean) / torch.sqrt(var + self.eps)\n",
    "            self.update_movings(mean, var)\n",
    "        new_batch = self.gamma * normed + self.beta\n",
    "        return new_batch\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(nn.NLLLoss):\n",
    "    def __init__(self, a: float = 0.01, reduction='mean', ignore_index=-100):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.a = a\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, pred, trg):\n",
    "        K = pred.size(-1)  # class number\n",
    "        trg_idx = trg != self.ignore_index  # identify not PAD\n",
    "        trg = trg[trg_idx]\n",
    "\n",
    "        log_pred = F.log_softmax(pred[trg_idx], dim=-1)\n",
    "        loss = -torch.sum(log_pred, dim=-1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        nll_loss = F.nll_loss(log_pred, trg, reduction=self.reduction)\n",
    "        loss = nll_loss * (1 - self.a) + self.a * (loss / K)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, a: float = 0.01, reduction='mean', ignore_index=-100):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "        self.a = a\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, pred, trg):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31ef2c-045e-452b-aff3-0c8bf8027551",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocess \n",
    "from enum import Enum\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "\n",
    "class TokenMarks(Enum):\n",
    "    PAD = '[PAD]'  # 0\n",
    "    UNKNOWN = '[UKN]'  # 1\n",
    "    END = '[END]'  # 2\n",
    "    START = '[SRT]'  # 3\n",
    "    CLS = '[CLS]'    # 4 Classification token\n",
    "    SEP = '[SEP]'  # 5\n",
    "    MASK = '[MASK]'   # 6\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    WORD2IDX = {mark: idx for idx, mark in enumerate(list(TokenMarks), start=0)}\n",
    "\n",
    "    def __init__(self, min_cnt, need_sep=False):\n",
    "        self.min_cnt = min_cnt\n",
    "        self.excepts = '.#$%^&*'\n",
    "        self.__word2idx = {k: v for k, v in self.WORD2IDX.items()}\n",
    "        self.__idx2word = {}\n",
    "        self._idx2word = {}\n",
    "        if not need_sep:\n",
    "            del self.__word2idx[TokenMarks.SEP]\n",
    "\n",
    "    def load(self, corpus: list):\n",
    "        vocabs = {}\n",
    "        for sent in corpus:\n",
    "            for word in sent[1: -1]:\n",
    "                if word not in vocabs.keys():\n",
    "                    vocabs[word] = 0\n",
    "                vocabs[word] += 1\n",
    "        idx = len(self.WORD2IDX)\n",
    "        for w in vocabs.keys():\n",
    "            if self._vocabs_filter(w, vocabs[w]):\n",
    "                self.__word2idx[w] = idx\n",
    "                idx += 1\n",
    "\n",
    "        return self.__word2idx\n",
    "\n",
    "    def _vocabs_filter(self, v, cnt):\n",
    "        if cnt < self.min_cnt:\n",
    "            return\n",
    "        if v in self.excepts:\n",
    "            return\n",
    "        return v\n",
    "\n",
    "    def to_idx2word(self):\n",
    "        self.__idx2word = {idx: w for w, idx in self.__word2idx.items()}\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        if not self.__idx2word:\n",
    "            return -1\n",
    "        return self.__idx2word[idx]\n",
    "\n",
    "    @property\n",
    "    def keys(self):\n",
    "        return self.__word2idx.keys()\n",
    "\n",
    "    @property\n",
    "    def word2idx(self):\n",
    "        return self.__word2idx\n",
    "\n",
    "    @property\n",
    "    def idx2word(self):\n",
    "        return self.__idx2word\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.__word2idx.keys()[len(TokenMarks):]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__word2idx)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return self.__word2idx[item]\n",
    "        except KeyError:\n",
    "            return self.__word2idx[TokenMarks.UNKNOWN]\n",
    "\n",
    "\n",
    "def preprocessor(corpus: list, lang='ko'):\n",
    "    result = []\n",
    "    tkner = Tokenizer(lang)\n",
    "    for line in corpus:\n",
    "        line = line.strip()\n",
    "        sents = tkner.sent_seperator(line)\n",
    "        sents = [tkner.tokenizer(sent) for sent in sents]\n",
    "        sents = [_to_word(sent) for sent in sents]\n",
    "        for sent in sents:\n",
    "            if len(sent) < 5:\n",
    "                continue\n",
    "            words = [TokenMarks.START] + sent + [TokenMarks.END]\n",
    "            result.append(words)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _to_word(sent: list) -> list:\n",
    "    \"\"\" Filter word as stop words \"\"\"\n",
    "    rst = []\n",
    "    for word in sent:\n",
    "        word = word.strip()\n",
    "        if not word or word in '\"\\'\\\\₩':\n",
    "            continue\n",
    "        rst.append(word)\n",
    "    return rst\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, lang='ko'):\n",
    "        if lang == 'ko':\n",
    "            kkm = Kkma()\n",
    "            self.sent_seperator = kkm.sentences\n",
    "            self.tokenizer = kkm.morphs\n",
    "\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f941da4-f191-4a6e-bdd7-d8cc6478eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Language Model\n",
    "\n",
    "class BasicLM:\n",
    "    def __init__(self, dconf, mconf):\n",
    "        self.dconf = dconf\n",
    "        self.mconf = mconf\n",
    "\n",
    "        self.ko_vocab = Vocab(self.dconf.min_cnt)\n",
    "        self.voc_size = 0\n",
    "        self.dataset = None\n",
    "        self._dataload = None\n",
    "\n",
    "        self.model = None\n",
    "        self.loss = None\n",
    "        self.perpelexity = None\n",
    "        self.optim = None\n",
    "        self.lrscheder = None\n",
    "\n",
    "    def train(self):\n",
    "        raise\n",
    "\n",
    "    def predict(self, corpus):\n",
    "        raise\n",
    "\n",
    "    def save(self, fname: str):\n",
    "        \"\"\" save model \"\"\"\n",
    "\n",
    "        torch.save({\n",
    "            'model': self.model.state_dict(),\n",
    "            'optim': self.optim.state_dict(),\n",
    "            'ko_vocab': self.ko_vocab,\n",
    "        }, 'results/model/' + fname)\n",
    "\n",
    "    def load(self, fname: str, retrain=False):\n",
    "        \"\"\" load pytorch model \"\"\"\n",
    "        if not self.model:\n",
    "            raise\n",
    "        checkpoint = torch.load('results/model/' + fname)\n",
    "        self.model.load_state_dict(checkpoint['model'])\n",
    "        if self.optim and retrain:\n",
    "            self.optim.load_state_dict(checkpoint['optim'])\n",
    "        self.ko_vocab = checkpoint['ko_vocab']\n",
    "        self.ko_vocab.to_idx2word()\n",
    "        self.model.eval()\n",
    "        print(len(self.ko_vocab))\n",
    "\n",
    "    def test(self, kor: list):\n",
    "        \"\"\" Translate Korean to English \"\"\"\n",
    "        pred = self.predict(kor)\n",
    "        print(pred)\n",
    "        rst = []\n",
    "        for sent_idx in pred:\n",
    "            sent = [self.ko_vocab.get_word(idx) for idx in sent_idx if not 0]\n",
    "            rst.append(sent)\n",
    "        return rst\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(pred, target):\n",
    "    trg_idx = target > 0\n",
    "    trg = target[trg_idx]\n",
    "    pred = pred.argmax(1)[trg_idx]\n",
    "    acc = sum(trg == pred).item() / len(target)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740c9c3-166d-483e-ba95-c1b2c0dbc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Util\n",
    "import json\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, path: str):\n",
    "        self.__dict__.update(load_json(path))\n",
    "\n",
    "    def save(self, path: str):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.__dict__, f)\n",
    "\n",
    "    def update(self, config: dict):\n",
    "        self.__dict__.update(config)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.__dict__.__setitem__(key, value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def load_data(path: str):\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        result = f.readlines()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3ea4a-f41f-43e1-b098-a806257c793b",
   "metadata": {},
   "source": [
    "#### Bert Pretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695a990-a02d-46ed-985f-9f0c2bf4e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, sep_idx):\n",
    "        super(BERTEmbedding, self).__init__()\n",
    "        self.word_emb = WordEmbedding(vocab_size, emb_dim)\n",
    "        self.segment_emb = nn.Embedding(2, emb_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.sep_idx = sep_idx\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        inp [bsize, maxlen]\n",
    "        \"\"\"\n",
    "        # [bsize, maxlen, emb_dim]\n",
    "        idx_emb = self.word_emb(inp)\n",
    "        pe_emb = positional_embedding(idx_emb.size(0), idx_emb.size(1), idx_emb.size(2))\n",
    "        seg_idx = make_seg_idx(inp, self.sep_idx)\n",
    "        seg_emb = self.segment_emb(seg_idx)\n",
    "        emb = idx_emb + pe_emb + seg_emb\n",
    "        return self.dropout(emb)\n",
    "\n",
    "\n",
    "def make_seg_idx(inp, sep_idx):\n",
    "    \"\"\"\n",
    "    inp [bsize, maxlen]\n",
    "    \"\"\"\n",
    "    out = np.zeros_like(inp)\n",
    "    dup = []\n",
    "    for i, posi in [i for i in zip(*np.where(inp == sep_idx))]:\n",
    "        if i in dup:\n",
    "            continue\n",
    "        dup.append(i)\n",
    "        out[i][:posi+1] = 1\n",
    "    return torch.tensor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e36c9-5386-42e0-bd0e-3e8a37dfca54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee82ada-8949-4109-b5df-bf3a401af362",
   "metadata": {},
   "outputs": [],
   "source": [
    "## layer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_m, d_ff, n_head):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.multi_attn = MultiHeadAttention(d_m, n_head)\n",
    "        self.pw_ff = PositionWiseFFLayer(d_m, d_ff)\n",
    "\n",
    "    def forward(self, inp, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp (Tensor): [batch size, maxlen, d_m]\n",
    "            mask (Tensor): [batch size, 1, maxlen]\n",
    "        Returns: [batch size, maxlen, d_m]\n",
    "        \"\"\"\n",
    "        # Sub-layer 1\n",
    "        out = self.multi_attn(inp, inp, inp, mask)\n",
    "        # Sub-layer 2\n",
    "        out = self.pw_ff(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26652cf-ecfb-483f-a4a0-b993064a9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bert\n",
    "class BERT(nn.Module):\n",
    "    \"\"\" Assemble layers to build Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, sep_idx, d_m=768, attn_heads=12, n=12):\n",
    "        super(BERT, self).__init__()\n",
    "        self.inp_emb = BERTEmbedding(vocab_size, d_m, sep_idx)\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [Encoder(d_m, d_m*4, attn_heads) for _ in range(n)])\n",
    "\n",
    "        self.affine_1 = Affine(d_m, vocab_size)\n",
    "        self.affine_2 = Affine(d_m, 2)\n",
    "        self.n = n\n",
    "\n",
    "    def encoder(self, inp_batch, src_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp_batch (Tensor): [batch size, maxlen]\n",
    "            src_mask (Tensor): [bsize, 1, maxlen]\n",
    "        Returns: [batch size, maxlen, d_m]\n",
    "        \"\"\"\n",
    "        # [batch size, maxlen, d_m]\n",
    "        i_emb = self.inp_emb(inp_batch)\n",
    "        # Encoder\n",
    "        enc = i_emb\n",
    "        for layer in self.enc_layers:\n",
    "            # [batch size, maxlen, d_m]\n",
    "            enc = layer(enc, src_mask)\n",
    "        return enc\n",
    "\n",
    "    def forward(self, inp_batch, lm_posi):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp_batch (Tensor): [batch size, maxlen]\n",
    "            lm_posi (Tensor): [batch size, trg_size(15% of sent)]\n",
    "        Returns: [batch size, maxlen, vocab_size]\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        src_mask = mask_not_pad(inp_batch)\n",
    "        # [batch size, maxlen, d_m]\n",
    "        enc = self.encoder(inp_batch, src_mask)\n",
    "\n",
    "        # Next Word Prediction\n",
    "        # [batch size, maxlen, vocab_size]\n",
    "        lm_enc = self.affine_1(enc)\n",
    "        # [batch size, num_mask, vocab_size]\n",
    "        rst_1 = [F.log_softmax(lm_enc[i][posi], dim=-1) for i, posi in enumerate(lm_posi)]\n",
    "\n",
    "        # Sentence location Prediction\n",
    "        # [batch size, d_m]\n",
    "        cls = enc[:, 0]\n",
    "        # [batch size, 2]\n",
    "        rst_2 = F.log_softmax(self.affine_2(cls), dim=-1)\n",
    "        return rst_1, rst_2\n",
    "\n",
    "    def predict_next_word(self, inp_batch, lm_posi):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp_batch (Tensor): [batch size, maxlen]\n",
    "        Returns: [batch size, maxlen, vocab_size]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            src_mask = mask_not_pad(inp_batch)\n",
    "            enc = self.encoder(inp_batch, src_mask)\n",
    "            lm_enc = self.affine_1(enc)\n",
    "            rst = [F.log_softmax(lm_enc[i][posi], dim=-1) for i, posi in enumerate(lm_posi)]\n",
    "        return rst\n",
    "\n",
    "    def predict_is_next_sent(self, inp_batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp_batch (Tensor): [batch size, maxlen]\n",
    "        Returns: [batch size, maxlen, vocab_size]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            src_mask = mask_not_pad(inp_batch)\n",
    "            enc = self.encoder(inp_batch, src_mask)\n",
    "            cls = enc[:, 0]\n",
    "            rst = F.log_softmax(self.affine_2(cls), dim=-1)\n",
    "        return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4686058-95ec-4e48-a865-e10fa1af3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data Batchify\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Corpus(Dataset):\n",
    "\n",
    "    def __init__(self, data_set):\n",
    "        self._data = data_set\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        inp, nsp_trgs, lm_trg, lm_posi = self._data[idx][0], self._data[idx][1], \\\n",
    "                                         self._data[idx][2], self._data[idx][3]\n",
    "\n",
    "        return torch.tensor(inp), torch.tensor(nsp_trgs), torch.tensor(lm_trg), lm_posi\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inp, nsp_trgs, lm_trg, lm_posi = zip(*batch)\n",
    "    pad_inp = torch.nn.utils.rnn.pad_sequence(inp, batch_first=True)\n",
    "    return pad_inp, nsp_trgs, lm_trg, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9bc38-271e-4c93-a654-1c4358a602a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(preds, target):\n",
    "    preds = [pred.argmax(-1) for pred in preds]\n",
    "    true_positive, cnt = 0, 0\n",
    "    for t, p in zip(target, preds):\n",
    "        true_positive += sum(t == p)\n",
    "        cnt += len(p)\n",
    "    true_positive = max(1, true_positive)\n",
    "    acc = true_positive/cnt\n",
    "    return float(acc)\n",
    "\n",
    "\n",
    "class BERTEmbedding(BasicLM):\n",
    "    def __init__(self, *args):\n",
    "        super(BERTEmbedding, self).__init__(*args)\n",
    "        self.ko_vocab = Vocab(self.dconf.min_cnt, need_sep=True)\n",
    "\n",
    "    def train(self):\n",
    "        ko_corpus = preprocessor(load_data(self.dconf.train_ko_path), lang='ko')\n",
    "        self.ko_vocab.load(ko_corpus)\n",
    "\n",
    "        train_set = self.dataset_form(ko_corpus)\n",
    "        self.dataset = Corpus(train_set)\n",
    "        self._dataload = DataLoader(self.dataset,\n",
    "                                    batch_size=self.mconf.batch_size,\n",
    "                                    num_workers=0, collate_fn=collate_fn)\n",
    "        self.mconf.ko_size = len(self.ko_vocab) + 1\n",
    "\n",
    "        self.model = BERT(self.mconf.ko_size, self.ko_vocab[TokenMarks.SEP],\n",
    "                          self.mconf.d_m, self.mconf.attn_heads, self.mconf.n_layer)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optim = optim.Adam(params=self.model.parameters(), lr=self.mconf.lr)\n",
    "        self.lrscheder = optim.lr_scheduler.ReduceLROnPlateau(self.optim, patience=5)\n",
    "\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        self.model.train()\n",
    "        # self.info()\n",
    "        for epoch in tqdm(range(self.mconf.epoch), desc='epoch'):\n",
    "            for i, batch in tqdm(enumerate(self._dataload), desc=\"step\", total=len(self._dataload)):\n",
    "                self.optim.zero_grad()\n",
    "                # src, trg for Masked LM\n",
    "                inp, nsp_trgs, lm_trg, lm_posi = batch\n",
    "                pred_lm, pred_nsp = self.model(inp, lm_posi)\n",
    "\n",
    "                b_loss_nsp = self.loss(pred_nsp, torch.tensor(nsp_trgs))\n",
    "                b_loss_mlm = sum([self.loss(pred, trg) for pred, trg in zip(pred_lm, lm_trg)])\n",
    "                b_loss = b_loss_mlm + b_loss_nsp\n",
    "                b_loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "\n",
    "                total_acc += accuracy(pred_lm, lm_trg)\n",
    "                total_loss += b_loss.item()\n",
    "\n",
    "            itersize = math.ceil(len(self.dataset) / self.mconf.batch_size)\n",
    "            ppl = math.exp(total_loss / len(self.dataset))\n",
    "            print(epoch, total_loss, total_acc / itersize, ppl)\n",
    "            self.lrscheder.step(total_loss)\n",
    "            total_loss = 0\n",
    "        self.ko_vocab.to_idx2word()\n",
    "\n",
    "    def load(self, fname: str, retrain=False):\n",
    "        self.model = BERT(self.mconf.d_m, self.mconf.ko_size, self.ko_vocab[TokenMarks.SEP])\n",
    "        super().load(fname)\n",
    "\n",
    "    def predict(self, corpus):\n",
    "        ko_corpus = preprocessor(corpus, lang='ko')\n",
    "        pred_set = self.dataset_form(ko_corpus)\n",
    "        inp, nsp_trgs, lm_trg, lm_posi = zip(*pred_set)\n",
    "\n",
    "        dataset = torch.nn.utils.rnn.pad_sequence(torch.tensor(inp), batch_first=True)\n",
    "        pred_mlm = self.model.predict_next_word(dataset, lm_posi)\n",
    "        pred_nsp = self.model.predict_is_next_sent(dataset)\n",
    "        pred_words = [pred.argmax(-1) for pred in pred_mlm]\n",
    "        for p, t in zip(pred_words, lm_trg):\n",
    "            print([self.ko_vocab.idx2word[idx] for idx in t])\n",
    "            print([self.ko_vocab.idx2word[idx] for idx in p.tolist()])\n",
    "        print()\n",
    "        print(pred_nsp.argmax(-1), nsp_trgs)\n",
    "\n",
    "    def masking_words(self, sent, sep_loc):\n",
    "        \"\"\"\n",
    "        Masking predicted word\n",
    "        15 % of words in sentence will be TARGET of LM\n",
    "            80 % of target will be [MASK]\n",
    "            10 % of target will be random word\n",
    "            10 % of target will be origin word\n",
    "        \"\"\"\n",
    "        inp = np.array(sent)\n",
    "        num_mask = round(len(sent) * 0.15)\n",
    "\n",
    "        mask_candi = [i for i in range(1, len(sent) - 1)]\n",
    "        mask_candi.remove(sep_loc)\n",
    "        # target idx of location\n",
    "        mask_posi = random.sample(mask_candi, num_mask)\n",
    "        trg = [i for i in inp[mask_posi]]\n",
    "        to_mask = mask_posi[:int(num_mask * 0.8)]  # index of masking target\n",
    "        to_rand = mask_posi[int(num_mask * 0.8):int(num_mask * 0.9)]  # index of random target\n",
    "        inp[to_mask] = self.ko_vocab[TokenMarks.MASK]\n",
    "        inp[to_rand] = \\\n",
    "            random.sample([i for i in range(len(TokenMarks), len(self.ko_vocab))], len(to_rand))\n",
    "        return inp, trg, mask_posi\n",
    "\n",
    "    def dataset_form(self, ko_corpus):\n",
    "        rst = []\n",
    "        for i in range(len(ko_corpus)):\n",
    "            i_trg = ko_corpus[i]\n",
    "            if random.random() > 0.5 and i != len(ko_corpus) - 1:\n",
    "                next_trg, nsp_trg = ko_corpus[i + 1], 1\n",
    "            else:\n",
    "                next_trg, nsp_trg = ko_corpus[int(random.random() * len(ko_corpus))], 0\n",
    "\n",
    "            s1_inp = [self.ko_vocab[x] for x in i_trg]\n",
    "            s2_inp = [self.ko_vocab[x] for x in next_trg]\n",
    "            sep_loc = len(s1_inp) + 1\n",
    "            src = [self.ko_vocab[TokenMarks.CLS]] \\\n",
    "                  + s1_inp + [self.ko_vocab[TokenMarks.SEP]] \\\n",
    "                  + s2_inp + [self.ko_vocab[TokenMarks.SEP]]\n",
    "            inp, lm_trg, posi = self.masking_words(src, sep_loc)\n",
    "\n",
    "            rst.append((inp, nsp_trg, lm_trg, posi))\n",
    "        return rst\n",
    "\n",
    "    def info(self):\n",
    "        print(\"Model's state_dict:\")\n",
    "        for param_tensor in self.model.state_dict():\n",
    "            print(param_tensor, \"\\t\", self.model.state_dict()[param_tensor].size())\n",
    "\n",
    "        print(\"Optimizer's state_dict:\")\n",
    "        for var_name in self.optim.state_dict():\n",
    "            print(var_name, \"\\t\", self.optim.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c98c8e-47a4-41f9-9661-93d2aecbd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configs\n",
    "dconf_path = 'config/data.json'\n",
    "mconf_path = 'config/lm.json'\n",
    "dconf = Config(dconf_path)\n",
    "mconf = Config(mconf_path)\n",
    "\n",
    "# load w2v model and train\n",
    "lm = BERTEmbedding(dconf, mconf)\n",
    "lm.train()\n",
    "\n",
    "lm.save('trained.pth')\n",
    "mconf.save(mconf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eae66d-bf37-4673-92fe-652ad93f7324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f4d02-4c40-465f-87a2-4e6445525b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc720633-817a-4006-b8bc-4f127806228b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d4e32-320b-4362-b852-b7a80aa543ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_shape, head):\n",
    "        super(Attention, self).__init__()\n",
    "        self.head = head\n",
    "        self.input_shape = input_shape\n",
    "        self.head_dims = int(input_shape // head)\n",
    "\n",
    "        self.query = nn.Linear(self.head_dims, self.head_dims)\n",
    "        self.key = nn.Linear(self.head_dims, self.head_dims)\n",
    "        self.value = nn.Linear(self.head_dims, self.head_dims)\n",
    "        self.fc = nn.Linear(self.head_dims*head, input_shape)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch = query.shape[0]\n",
    "        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1]\n",
    "        \n",
    "        query = query.reshape(batch, query_len, self.head, self.head_dims)\n",
    "        key = key.reshape(batch, key_len, self.head, self.head_dims)\n",
    "        value = value.reshape(batch, value_len, self.head, self.head_dims)\n",
    "\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        score = torch.einsum(\"bqhd,bkhd->bhqk\", [query, key])\n",
    "        \n",
    "        if mask is not None:\n",
    "            score.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        score = torch.softmax(score/((self.head_dims)**(1/2)), dim=-1)\n",
    "        \n",
    "        out = torch.einsum(\"bhqv,bvhd->bqhd\", [score, value])\n",
    "        out = out.reshape(batch, query_len, self.head*self.head_dims)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, input_shape, head, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = Attention(input_shape, head)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(input_shape, input_shape*forward_expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_shape*forward_expansion, input_shape)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(input_shape)\n",
    "        self.layernorm2 = nn.LayerNorm(input_shape)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask):\n",
    "        attention = self.attention(query, key, value, mask)\n",
    "        add = attention + query \n",
    "        regulazation = self.dropout(self.layernorm1(add))\n",
    "        forward = self.feed_forward(regulazation)\n",
    "        out = self.dropout(self.layernorm2(forward + regulazation))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_out,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_out)\n",
    "        self.postional_embedding =  nn.Parameter(torch.zeros(1, max_len, embedding_out))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    embedding_out,\n",
    "                    heads,\n",
    "                    dropout,\n",
    "                    forward_expansion\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        word_embedding = self.word_embedding(x)\n",
    "        postional_embedding = self.postional_embedding[:, :x.shape[1], :]\n",
    "        out = self.dropout(word_embedding + postional_embedding)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_out,\n",
    "        head,\n",
    "        forward_expansion,\n",
    "        dropout\n",
    "    ):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = Attention(embedding_out, head)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embedding_out, \n",
    "            head, \n",
    "            dropout, \n",
    "            forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(embedding_out)\n",
    "\n",
    "    def forward(self, query, key, value, src_mask, causal_mask):\n",
    "        attention = self.attention(query, query, query, causal_mask)\n",
    "        query = self.dropout(self.norm(attention + query))\n",
    "        out = self.transformer_block(query, key, value, src_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_out,\n",
    "        num_layers,\n",
    "        head,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_out)\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, max_len, embedding_out))\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "            DecoderBlock(\n",
    "                embedding_out,\n",
    "                head,\n",
    "                forward_expansion,\n",
    "                dropout\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_out, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask, casual_mask):\n",
    "        x = self.dropout(self.word_embedding(x) + self.positional_embedding[:, :x.shape[1], :])\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x, \n",
    "                encoder_output, \n",
    "                encoder_output, \n",
    "                src_mask, \n",
    "                casual_mask\n",
    "            )\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Transformers(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vocab_size,\n",
    "        output_vocab_size,\n",
    "        pad_idx,\n",
    "        embedding_out,\n",
    "        num_layers,\n",
    "        forward_expansion,\n",
    "        head,\n",
    "        dropout,\n",
    "        max_len\n",
    "    ):\n",
    "        super(Transformers, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "            input_vocab_size,\n",
    "            embedding_out,\n",
    "            num_layers,\n",
    "            head,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_len\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            output_vocab_size,\n",
    "            embedding_out,\n",
    "            num_layers,\n",
    "            head,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_len\n",
    "        )\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    #From @HuggingFace\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "    \n",
    "    def pad_mask(self, inputs):\n",
    "        pad_mask = (inputs != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return pad_mask\n",
    "\n",
    "    def causal_mask(self, target):\n",
    "        N, target_len = target.shape\n",
    "        target_mask = torch.tril(torch.ones((N, target_len, target_len))).unsqueeze(1)\n",
    "        return target_mask\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        pad_mask = self.pad_mask(inputs)\n",
    "        causal_mask = self.causal_mask(target)\n",
    "        encoder_output = self.encoder(inputs, pad_mask)\n",
    "        decoder_out = self.decoder(target, encoder_output, pad_mask, causal_mask)\n",
    "        return decoder_out\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Depends on the Tokenizer\n",
    "    input_vocab_size = 100\n",
    "    output_vocab_size = 200\n",
    "\n",
    "    #DEFAULT TRANSFORMERS PARAMETERS:-\n",
    "    pad_idx = 0 \n",
    "    embedding_out = 512\n",
    "    num_layers = 6\n",
    "    forward_expansion = 4\n",
    "    head = 8\n",
    "    dropout = 0.1\n",
    "    max_len = 512\n",
    "\n",
    "    inputs = torch.randint(0, 100, (32, 200))\n",
    "    targets = torch.randint(0, 100, (32,100))\n",
    "\n",
    "    model = Transformers(\n",
    "        input_vocab_size,\n",
    "        output_vocab_size,\n",
    "        pad_idx,\n",
    "        embedding_out,\n",
    "        num_layers,\n",
    "        forward_expansion,\n",
    "        head,\n",
    "        dropout,\n",
    "        max_len\n",
    "    )\n",
    "\n",
    "    start = time()\n",
    "    y = model(inputs, targets)\n",
    "    print(f'INFERENCE TIME = {time() - start}sec')\n",
    "    x = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'NUMBER OF PARAMETERS ARE = {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1326352-a5b1-4a76-8c44-3b88f01677fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503db581-4713-4106-9d44-f2d9f0171ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e50c64-9e83-4280-ab5a-9934e0b593cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c89267-dd34-4900-a501-e3cf72d2cc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdbe62d6-281e-4b88-92e9-0ed574167ca6",
   "metadata": {},
   "source": [
    "## 7. https://github.com/jadore801120/attention-is-all-you-need-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e5f17e-f9d1-422e-a49e-b744a406e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "PAD_WORD = '<blank>'\n",
    "UNK_WORD = '<unk>'\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e318729-bcb7-4566-a0f9-cfa8fc4b1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96c96b0b-1efb-4a63-9bad-f3aead93beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sublayer\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e71ef33-4156-4401-a1cd-c292f9d71109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    ''' Compose with two layers '''\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_slf_attn\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    ''' Compose with three layers '''\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(\n",
    "            self, dec_input, enc_output,\n",
    "            slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        dec_output, dec_slf_attn = self.slf_attn(\n",
    "            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
    "        dec_output, dec_enc_attn = self.enc_attn(\n",
    "            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60199ac-e0f3-4f8a-a92c-d69b199f571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, lr_mul, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.lr_mul = lr_mul\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.lr_mul * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb32490a-5dbb-49aa-a8f7-2853eeeb83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(\n",
    "        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_hid, n_position=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Not a parameter\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n",
    "        ''' Sinusoid position encoding table '''\n",
    "        # TODO: make it with torch instead of numpy\n",
    "\n",
    "        def get_position_angle_vec(position):\n",
    "            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, dropout=0.1, n_position=200, scale_emb=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src_seq, src_mask, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Forward\n",
    "        enc_output = self.src_word_emb(src_seq)\n",
    "        if self.scale_emb:\n",
    "            enc_output *= self.d_model ** 0.5\n",
    "        enc_output = self.dropout(self.position_enc(enc_output))\n",
    "        enc_output = self.layer_norm(enc_output)\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n",
    "            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attn_list\n",
    "        return enc_output,\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' A decoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, pad_idx, n_position=200, dropout=0.1, scale_emb=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list, dec_enc_attn_list = [], []\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = self.trg_word_emb(trg_seq)\n",
    "        if self.scale_emb:\n",
    "            dec_output *= self.d_model ** 0.5\n",
    "        dec_output = self.dropout(self.position_enc(dec_output))\n",
    "        dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n",
    "            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    ''' A sequence to sequence model with attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n",
    "            d_word_vec=512, d_model=512, d_inner=2048,\n",
    "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,\n",
    "            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True,\n",
    "            scale_emb_or_prj='prj'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n",
    "\n",
    "        # In section 3.4 of paper \"Attention Is All You Need\", there is such detail:\n",
    "        # \"In our model, we share the same weight matrix between the two\n",
    "        # embedding layers and the pre-softmax linear transformation...\n",
    "        # In the embedding layers, we multiply those weights by \\sqrt{d_model}\".\n",
    "        #\n",
    "        # Options here:\n",
    "        #   'emb': multiply \\sqrt{d_model} to embedding output\n",
    "        #   'prj': multiply (\\sqrt{d_model} ^ -1) to linear projection output\n",
    "        #   'none': no multiplication\n",
    "\n",
    "        assert scale_emb_or_prj in ['emb', 'prj', 'none']\n",
    "        scale_emb = (scale_emb_or_prj == 'emb') if trg_emb_prj_weight_sharing else False\n",
    "        self.scale_prj = (scale_emb_or_prj == 'prj') if trg_emb_prj_weight_sharing else False\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=src_pad_idx, dropout=dropout, scale_emb=scale_emb)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_trg_vocab=n_trg_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=trg_pad_idx, dropout=dropout, scale_emb=scale_emb)\n",
    "\n",
    "        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) \n",
    "\n",
    "        assert d_model == d_word_vec, \\\n",
    "        'To facilitate the residual connections, \\\n",
    "         the dimensions of all module outputs shall be the same.'\n",
    "\n",
    "        if trg_emb_prj_weight_sharing:\n",
    "            # Share the weight between target word embedding & last dense layer\n",
    "            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight\n",
    "\n",
    "        if emb_src_trg_weight_sharing:\n",
    "            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight\n",
    "\n",
    "\n",
    "    def forward(self, src_seq, trg_seq):\n",
    "\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
    "        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_mask)\n",
    "        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
    "        seq_logit = self.trg_word_prj(dec_output)\n",
    "        if self.scale_prj:\n",
    "            seq_logit *= self.d_model ** -0.5\n",
    "\n",
    "        return seq_logit.view(-1, seq_logit.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "777bdfe2-adac-4a61-b582-6a7166673186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from transformer.Models import Transformer, get_pad_mask, get_subsequent_mask\n",
    "\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    ''' Load a trained model and translate in beam search fashion. '''\n",
    "\n",
    "    def __init__(\n",
    "            self, model, beam_size, max_seq_len,\n",
    "            src_pad_idx, trg_pad_idx, trg_bos_idx, trg_eos_idx):\n",
    "        \n",
    "\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.alpha = 0.7\n",
    "        self.beam_size = beam_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_bos_idx = trg_bos_idx\n",
    "        self.trg_eos_idx = trg_eos_idx\n",
    "\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "        self.register_buffer('init_seq', torch.LongTensor([[trg_bos_idx]]))\n",
    "        self.register_buffer(\n",
    "            'blank_seqs', \n",
    "            torch.full((beam_size, max_seq_len), trg_pad_idx, dtype=torch.long))\n",
    "        self.blank_seqs[:, 0] = self.trg_bos_idx\n",
    "        self.register_buffer(\n",
    "            'len_map', \n",
    "            torch.arange(1, max_seq_len + 1, dtype=torch.long).unsqueeze(0))\n",
    "\n",
    "\n",
    "    def _model_decode(self, trg_seq, enc_output, src_mask):\n",
    "        trg_mask = get_subsequent_mask(trg_seq)\n",
    "        dec_output, *_ = self.model.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
    "        return F.softmax(self.model.trg_word_prj(dec_output), dim=-1)\n",
    "\n",
    "\n",
    "    def _get_init_state(self, src_seq, src_mask):\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        enc_output, *_ = self.model.encoder(src_seq, src_mask)\n",
    "        dec_output = self._model_decode(self.init_seq, enc_output, src_mask)\n",
    "        \n",
    "        best_k_probs, best_k_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "\n",
    "        scores = torch.log(best_k_probs).view(beam_size)\n",
    "        gen_seq = self.blank_seqs.clone().detach()\n",
    "        gen_seq[:, 1] = best_k_idx[0]\n",
    "        enc_output = enc_output.repeat(beam_size, 1, 1)\n",
    "        return enc_output, gen_seq, scores\n",
    "\n",
    "\n",
    "    def _get_the_best_score_and_idx(self, gen_seq, dec_output, scores, step):\n",
    "        assert len(scores.size()) == 1\n",
    "        \n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        # Get k candidates for each beam, k^2 candidates in total.\n",
    "        best_k2_probs, best_k2_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "\n",
    "        # Include the previous scores.\n",
    "        scores = torch.log(best_k2_probs).view(beam_size, -1) + scores.view(beam_size, 1)\n",
    "\n",
    "        # Get the best k candidates from k^2 candidates.\n",
    "        scores, best_k_idx_in_k2 = scores.view(-1).topk(beam_size)\n",
    " \n",
    "        # Get the corresponding positions of the best k candidiates.\n",
    "        best_k_r_idxs, best_k_c_idxs = best_k_idx_in_k2 // beam_size, best_k_idx_in_k2 % beam_size\n",
    "        best_k_idx = best_k2_idx[best_k_r_idxs, best_k_c_idxs]\n",
    "\n",
    "        # Copy the corresponding previous tokens.\n",
    "        gen_seq[:, :step] = gen_seq[best_k_r_idxs, :step]\n",
    "        # Set the best tokens in this beam search step\n",
    "        gen_seq[:, step] = best_k_idx\n",
    "\n",
    "        return gen_seq, scores\n",
    "\n",
    "\n",
    "    def translate_sentence(self, src_seq):\n",
    "        # Only accept batch size equals to 1 in this function.\n",
    "        # TODO: expand to batch operation.\n",
    "        assert src_seq.size(0) == 1\n",
    "\n",
    "        src_pad_idx, trg_eos_idx = self.src_pad_idx, self.trg_eos_idx \n",
    "        max_seq_len, beam_size, alpha = self.max_seq_len, self.beam_size, self.alpha \n",
    "\n",
    "        with torch.no_grad():\n",
    "            src_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "            enc_output, gen_seq, scores = self._get_init_state(src_seq, src_mask)\n",
    "\n",
    "            ans_idx = 0   # default\n",
    "            for step in range(2, max_seq_len):    # decode up to max length\n",
    "                dec_output = self._model_decode(gen_seq[:, :step], enc_output, src_mask)\n",
    "                gen_seq, scores = self._get_the_best_score_and_idx(gen_seq, dec_output, scores, step)\n",
    "\n",
    "                # Check if all path finished\n",
    "                # -- locate the eos in the generated sequences\n",
    "                eos_locs = gen_seq == trg_eos_idx   \n",
    "                # -- replace the eos with its position for the length penalty use\n",
    "                seq_lens, _ = self.len_map.masked_fill(~eos_locs, max_seq_len).min(1)\n",
    "                # -- check if all beams contain eos\n",
    "                if (eos_locs.sum(1) > 0).sum(0).item() == beam_size:\n",
    "                    # TODO: Try different terminate conditions.\n",
    "                    _, ans_idx = scores.div(seq_lens.float() ** alpha).max(0)\n",
    "                    ans_idx = ans_idx.item()\n",
    "                    break\n",
    "        return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be5cb0-6921-402d-9720-3150c1ba1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] The warmup steps may be not enough.\n",
      "(sz_b, warmup) = (2048, 4000) is the official setting.\n",
      "Using smaller batch w/o longer warmup may cause the warmup stage ends with only little data trained.\n",
      "Namespace(data_pkl='m30k_deen_shr.pkl', train_path=None, val_path=None, epoch=3, batch_size=32, d_model=512, d_inner_hid=2048, d_k=64, d_v=64, n_head=8, n_layers=6, n_warmup_steps=4000, lr_mul=2.0, seed=None, dropout=0.1, embs_share_weight=False, proj_share_weight=False, scale_emb_or_prj='prj', output_dir='output', use_tb=False, save_mode='best', no_cuda=False, label_smoothing=False, cuda=True, d_word_vec=512, max_token_seq_len=100, src_pad_idx=1, trg_pad_idx=1, src_vocab_size=5374, trg_vocab_size=4556)\n",
      "[Info] Training performance will be written to file: output\\train.log and output\\valid.log\n",
      "[ Epoch 0 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - (Training)   ppl:  179.73567, accuracy: 20.731 %, lr:  0.00032, elapse: 80.946 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - (Validation) ppl:  104.05315, accuracy: 25.069 %, lr:  0.00032, elapse: 0.080 min\n",
      "    - [Info] The checkpoint file has been updated.\n",
      "[ Epoch 1 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  - (Training)   :  50%|██████████████████████████████▊                              | 458/907 [05:23<16:04,  2.15s/it]"
     ]
    }
   ],
   "source": [
    "def cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Apply label smoothing if needed '''\n",
    "\n",
    "    loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n",
    "\n",
    "    pred = pred.max(1)[1]\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(trg_pad_idx)\n",
    "    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "\n",
    "    return loss, n_correct, n_word\n",
    "\n",
    "\n",
    "def cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(trg_pad_idx)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction='sum')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def patch_src(src, pad_idx):\n",
    "    src = src.transpose(0, 1)\n",
    "    return src\n",
    "\n",
    "\n",
    "def patch_trg(trg, pad_idx):\n",
    "    trg = trg.transpose(0, 1)\n",
    "    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n",
    "    return trg, gold\n",
    "\n",
    "\n",
    "def train_epoch(model, training_data, optimizer, opt, device, smoothing):\n",
    "    ''' Epoch operation in training phase'''\n",
    "\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "\n",
    "    desc = '  - (Training)   '\n",
    "    for batch in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "        # prepare data\n",
    "        src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "        trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, trg_seq)\n",
    "\n",
    "        # backward and update parameters\n",
    "        loss, n_correct, n_word = cal_performance(\n",
    "            pred, gold, opt.trg_pad_idx, smoothing=smoothing) \n",
    "        loss.backward()\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        # note keeping\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "\n",
    "def eval_epoch(model, validation_data, device, opt):\n",
    "    ''' Epoch operation in evaluation phase '''\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "\n",
    "    desc = '  - (Validation) '\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "            # prepare data\n",
    "            src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "            trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n",
    "\n",
    "            # forward\n",
    "            pred = model(src_seq, trg_seq)\n",
    "            loss, n_correct, n_word = cal_performance(\n",
    "                pred, gold, opt.trg_pad_idx, smoothing=False)\n",
    "\n",
    "            # note keeping\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "\n",
    "def train(model, training_data, validation_data, optimizer, device, opt):\n",
    "    ''' Start training '''\n",
    "\n",
    "    # Use tensorboard to plot curves, e.g. perplexity, accuracy, learning rate\n",
    "    if opt.use_tb:\n",
    "        print(\"[Info] Use Tensorboard\")\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        tb_writer = SummaryWriter(log_dir=os.path.join(opt.output_dir, 'tensorboard'))\n",
    "\n",
    "    log_train_file = os.path.join(opt.output_dir, 'train.log')\n",
    "    log_valid_file = os.path.join(opt.output_dir, 'valid.log')\n",
    "\n",
    "    print('[Info] Training performance will be written to file: {} and {}'.format(\n",
    "        log_train_file, log_valid_file))\n",
    "\n",
    "    with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n",
    "        log_tf.write('epoch,loss,ppl,accuracy\\n')\n",
    "        log_vf.write('epoch,loss,ppl,accuracy\\n')\n",
    "\n",
    "    def print_performances(header, ppl, accu, start_time, lr):\n",
    "        print('  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, lr: {lr:8.5f}, '\\\n",
    "              'elapse: {elapse:3.3f} min'.format(\n",
    "                  header=f\"({header})\", ppl=ppl,\n",
    "                  accu=100*accu, elapse=(time.time()-start_time)/60, lr=lr))\n",
    "\n",
    "    #valid_accus = []\n",
    "    valid_losses = []\n",
    "    for epoch_i in range(opt.epoch):\n",
    "        print('[ Epoch', epoch_i, ']')\n",
    "\n",
    "        start = time.time()\n",
    "        train_loss, train_accu = train_epoch(\n",
    "            model, training_data, optimizer, opt, device, smoothing=opt.label_smoothing)\n",
    "        train_ppl = math.exp(min(train_loss, 100))\n",
    "        # Current learning rate\n",
    "        lr = optimizer._optimizer.param_groups[0]['lr']\n",
    "        print_performances('Training', train_ppl, train_accu, start, lr)\n",
    "\n",
    "        start = time.time()\n",
    "        valid_loss, valid_accu = eval_epoch(model, validation_data, device, opt)\n",
    "        valid_ppl = math.exp(min(valid_loss, 100))\n",
    "        print_performances('Validation', valid_ppl, valid_accu, start, lr)\n",
    "\n",
    "        valid_losses += [valid_loss]\n",
    "\n",
    "        checkpoint = {'epoch': epoch_i, 'settings': opt, 'model': model.state_dict()}\n",
    "\n",
    "        if opt.save_mode == 'all':\n",
    "            model_name = 'model_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n",
    "            torch.save(checkpoint, model_name)\n",
    "        elif opt.save_mode == 'best':\n",
    "            model_name = 'model.chkpt'\n",
    "            if valid_loss <= min(valid_losses):\n",
    "                torch.save(checkpoint, os.path.join(opt.output_dir, model_name))\n",
    "                print('    - [Info] The checkpoint file has been updated.')\n",
    "\n",
    "        with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n",
    "            log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                epoch=epoch_i, loss=train_loss,\n",
    "                ppl=train_ppl, accu=100*train_accu))\n",
    "            log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                epoch=epoch_i, loss=valid_loss,\n",
    "                ppl=valid_ppl, accu=100*valid_accu))\n",
    "\n",
    "        if opt.use_tb:\n",
    "            tb_writer.add_scalars('ppl', {'train': train_ppl, 'val': valid_ppl}, epoch_i)\n",
    "            tb_writer.add_scalars('accuracy', {'train': train_accu*100, 'val': valid_accu*100}, epoch_i)\n",
    "            tb_writer.add_scalar('learning_rate', lr, epoch_i)\n",
    "\n",
    "def main():\n",
    "    ''' \n",
    "    Usage:\n",
    "    python train.py -data_pkl m30k_deen_shr.pkl -log m30k_deen_shr -embs_share_weight -proj_share_weight -label_smoothing -output_dir output -b 256 -warmup 128000\n",
    "    '''\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('-data_pkl',  default='m30k_deen_shr.pkl')     # all-in-1 data pickle or bpe field\n",
    "\n",
    "    parser.add_argument('-train_path', default=None)   # bpe encoded data\n",
    "    parser.add_argument('-val_path', default=None)     # bpe encoded data\n",
    "\n",
    "    parser.add_argument('-epoch', type=int, default=3)\n",
    "    parser.add_argument('-b', '--batch_size', type=int, default=32)\n",
    "\n",
    "    parser.add_argument('-d_model', type=int, default=512)\n",
    "    parser.add_argument('-d_inner_hid', type=int, default=2048)\n",
    "    parser.add_argument('-d_k', type=int, default=64)\n",
    "    parser.add_argument('-d_v', type=int, default=64)\n",
    "\n",
    "    parser.add_argument('-n_head', type=int, default=8)\n",
    "    parser.add_argument('-n_layers', type=int, default=6)\n",
    "    parser.add_argument('-warmup','--n_warmup_steps', type=int, default=4000)\n",
    "    parser.add_argument('-lr_mul', type=float, default=2.0)\n",
    "    parser.add_argument('-seed', type=int, default=None)\n",
    "\n",
    "    parser.add_argument('-dropout', type=float, default=0.1)\n",
    "    parser.add_argument('-embs_share_weight', action='store_true')\n",
    "    parser.add_argument('-proj_share_weight', action='store_true')\n",
    "    parser.add_argument('-scale_emb_or_prj', type=str, default='prj')\n",
    "\n",
    "    parser.add_argument('-output_dir', type=str, default=\"output\")\n",
    "    parser.add_argument('-use_tb', action='store_true')\n",
    "    parser.add_argument('-save_mode', type=str, choices=['all', 'best'], default='best')\n",
    "\n",
    "    parser.add_argument('-no_cuda', action='store_true')\n",
    "    parser.add_argument('-label_smoothing', action='store_true')\n",
    "\n",
    "    opt = parser.parse_args([])\n",
    "    opt.cuda = not opt.no_cuda\n",
    "    opt.d_word_vec = opt.d_model\n",
    "\n",
    "    # https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    # For reproducibility\n",
    "    if opt.seed is not None:\n",
    "        torch.manual_seed(opt.seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # torch.set_deterministic(True)\n",
    "        np.random.seed(opt.seed)\n",
    "        random.seed(opt.seed)\n",
    "\n",
    "    if not opt.output_dir:\n",
    "        print('No experiment result will be saved.')\n",
    "        raise\n",
    "\n",
    "    if not os.path.exists(opt.output_dir):\n",
    "        os.makedirs(opt.output_dir)\n",
    "\n",
    "    if opt.batch_size < 2048 and opt.n_warmup_steps <= 4000:\n",
    "        print('[Warning] The warmup steps may be not enough.\\n'\\\n",
    "              '(sz_b, warmup) = (2048, 4000) is the official setting.\\n'\\\n",
    "              'Using smaller batch w/o longer warmup may cause '\\\n",
    "              'the warmup stage ends with only little data trained.')\n",
    "\n",
    "    device = torch.device('cuda' if opt.cuda else 'cpu')\n",
    "\n",
    "    #========= Loading Dataset =========#\n",
    "\n",
    "    if all((opt.train_path, opt.val_path)):\n",
    "        training_data, validation_data = prepare_dataloaders_from_bpe_files(opt, device)\n",
    "    elif opt.data_pkl:\n",
    "        training_data, validation_data = prepare_dataloaders(opt, device)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    print(opt)\n",
    "\n",
    "    transformer = Transformer(\n",
    "        opt.src_vocab_size,\n",
    "        opt.trg_vocab_size,\n",
    "        src_pad_idx=opt.src_pad_idx,\n",
    "        trg_pad_idx=opt.trg_pad_idx,\n",
    "        trg_emb_prj_weight_sharing=opt.proj_share_weight,\n",
    "        emb_src_trg_weight_sharing=opt.embs_share_weight,\n",
    "        d_k=opt.d_k,\n",
    "        d_v=opt.d_v,\n",
    "        d_model=opt.d_model,\n",
    "        d_word_vec=opt.d_word_vec,\n",
    "        d_inner=opt.d_inner_hid,\n",
    "        n_layers=opt.n_layers,\n",
    "        n_head=opt.n_head,\n",
    "        dropout=opt.dropout,\n",
    "        scale_emb_or_prj=opt.scale_emb_or_prj).to(device)\n",
    "\n",
    "    optimizer = ScheduledOptim(\n",
    "        optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "        opt.lr_mul, opt.d_model, opt.n_warmup_steps)\n",
    "\n",
    "    train(transformer, training_data, validation_data, optimizer, device, opt)\n",
    "\n",
    "\n",
    "def prepare_dataloaders_from_bpe_files(opt, device):\n",
    "    batch_size = opt.batch_size\n",
    "    MIN_FREQ = 2\n",
    "    if not opt.embs_share_weight:\n",
    "        raise\n",
    "\n",
    "    data = pickle.load(open(opt.data_pkl, 'rb'))\n",
    "    MAX_LEN = data['settings'].max_len\n",
    "    field = data['vocab']\n",
    "    fields = (field, field)\n",
    "\n",
    "    def filter_examples_with_length(x):\n",
    "        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "    train = TranslationDataset(\n",
    "        fields=fields,\n",
    "        path=opt.train_path, \n",
    "        exts=('.src', '.trg'),\n",
    "        filter_pred=filter_examples_with_length)\n",
    "    val = TranslationDataset(\n",
    "        fields=fields,\n",
    "        path=opt.val_path, \n",
    "        exts=('.src', '.trg'),\n",
    "        filter_pred=filter_examples_with_length)\n",
    "\n",
    "    opt.max_token_seq_len = MAX_LEN + 2\n",
    "    opt.src_pad_idx = opt.trg_pad_idx = field.vocab.stoi[PAD_WORD]\n",
    "    opt.src_vocab_size = opt.trg_vocab_size = len(field.vocab)\n",
    "\n",
    "    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n",
    "    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n",
    "    return train_iterator, val_iterator\n",
    "\n",
    "\n",
    "def prepare_dataloaders(opt, device):\n",
    "    batch_size = opt.batch_size\n",
    "    data = pickle.load(open(opt.data_pkl, 'rb'))\n",
    "\n",
    "    opt.max_token_seq_len = data['settings'].max_len\n",
    "    opt.src_pad_idx = data['vocab']['src'].vocab.stoi[PAD_WORD]\n",
    "    opt.trg_pad_idx = data['vocab']['trg'].vocab.stoi[PAD_WORD]\n",
    "\n",
    "    opt.src_vocab_size = len(data['vocab']['src'].vocab)\n",
    "    opt.trg_vocab_size = len(data['vocab']['trg'].vocab)\n",
    "\n",
    "    #========= Preparing Model =========#\n",
    "    if opt.embs_share_weight:\n",
    "        assert data['vocab']['src'].vocab.stoi == data['vocab']['trg'].vocab.stoi, \\\n",
    "            'To sharing word embedding the src/trg word2idx table shall be the same.'\n",
    "\n",
    "    fields = {'src': data['vocab']['src'], 'trg':data['vocab']['trg']}\n",
    "\n",
    "    train = Dataset(examples=data['train'], fields=fields)\n",
    "    val = Dataset(examples=data['valid'], fields=fields)\n",
    "\n",
    "    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n",
    "    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n",
    "\n",
    "    return train_iterator, val_iterator\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a67c6-ec78-4b21-a4b5-a03202142bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c3fdfdf-132c-4ffe-87eb-717652b327b6",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b5b32-9a72-47bb-a371-f1df99c3f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE(object):\n",
    "\n",
    "    def __init__(self, codes, merges=-1, separator='@@', vocab=None, glossaries=None):\n",
    "\n",
    "        codes.seek(0)\n",
    "        offset=1\n",
    "\n",
    "        # check version information\n",
    "        firstline = codes.readline()\n",
    "        if firstline.startswith('#version:'):\n",
    "            self.version = tuple([int(x) for x in re.sub(r'(\\.0+)*$','', firstline.split()[-1]).split(\".\")])\n",
    "            offset += 1\n",
    "        else:\n",
    "            self.version = (0, 1)\n",
    "            codes.seek(0)\n",
    "\n",
    "        self.bpe_codes = [tuple(item.strip('\\r\\n ').split(' ')) for (n, item) in enumerate(codes) if (n < merges or merges == -1)]\n",
    "\n",
    "        for i, item in enumerate(self.bpe_codes):\n",
    "            if len(item) != 2:\n",
    "                sys.stderr.write('Error: invalid line {0} in BPE codes file: {1}\\n'.format(i+offset, ' '.join(item)))\n",
    "                sys.stderr.write('The line should exist of exactly two subword units, separated by whitespace\\n')\n",
    "                sys.exit(1)\n",
    "\n",
    "        # some hacking to deal with duplicates (only consider first instance)\n",
    "        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n",
    "\n",
    "        self.bpe_codes_reverse = dict([(pair[0] + pair[1], pair) for pair,i in self.bpe_codes.items()])\n",
    "\n",
    "        self.separator = separator\n",
    "\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.glossaries = glossaries if glossaries else []\n",
    "\n",
    "        self.glossaries_regex = re.compile('^({})$'.format('|'.join(glossaries))) if glossaries else None\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def process_line(self, line, dropout=0):\n",
    "        \"\"\"segment line, dealing with leading and trailing whitespace\"\"\"\n",
    "\n",
    "        out = \"\"\n",
    "\n",
    "        leading_whitespace = len(line)-len(line.lstrip('\\r\\n '))\n",
    "        if leading_whitespace:\n",
    "            out += line[:leading_whitespace]\n",
    "\n",
    "        out += self.segment(line, dropout)\n",
    "\n",
    "        trailing_whitespace = len(line)-len(line.rstrip('\\r\\n '))\n",
    "        if trailing_whitespace and trailing_whitespace != len(line):\n",
    "            out += line[-trailing_whitespace:]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def segment(self, sentence, dropout=0):\n",
    "        \"\"\"segment single sentence (whitespace-tokenized string) with BPE encoding\"\"\"\n",
    "        segments = self.segment_tokens(sentence.strip('\\r\\n ').split(' '), dropout)\n",
    "        return ' '.join(segments)\n",
    "\n",
    "    def segment_tokens(self, tokens, dropout=0):\n",
    "        \"\"\"segment a sequence of tokens with BPE encoding\"\"\"\n",
    "        output = []\n",
    "        for word in tokens:\n",
    "            # eliminate double spaces\n",
    "            if not word:\n",
    "                continue\n",
    "            new_word = [out for segment in self._isolate_glossaries(word)\n",
    "                        for out in encode(segment,\n",
    "                                          self.bpe_codes,\n",
    "                                          self.bpe_codes_reverse,\n",
    "                                          self.vocab,\n",
    "                                          self.separator,\n",
    "                                          self.version,\n",
    "                                          self.cache,\n",
    "                                          self.glossaries_regex,\n",
    "                                          dropout)]\n",
    "\n",
    "            for item in new_word[:-1]:\n",
    "                output.append(item + self.separator)\n",
    "            output.append(new_word[-1])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _isolate_glossaries(self, word):\n",
    "        word_segments = [word]\n",
    "        for gloss in self.glossaries:\n",
    "            word_segments = [out_segments for segment in word_segments\n",
    "                                 for out_segments in isolate_glossary(segment, gloss)]\n",
    "        return word_segments\n",
    "\n",
    "def encode(orig, bpe_codes, bpe_codes_reverse, vocab, separator, version, cache, glossaries_regex=None, dropout=0):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\n",
    "    \"\"\"\n",
    "\n",
    "    if not dropout and orig in cache:\n",
    "        return cache[orig]\n",
    "\n",
    "    if glossaries_regex and glossaries_regex.match(orig):\n",
    "        cache[orig] = (orig,)\n",
    "        return (orig,)\n",
    "\n",
    "    if len(orig) == 1:\n",
    "        return orig\n",
    "\n",
    "    if version == (0, 1):\n",
    "        word = list(orig) + ['</w>']\n",
    "    elif version == (0, 2): # more consistent handling of word-final segments\n",
    "        word = list(orig[:-1]) + [orig[-1] + '</w>']\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    while len(word) > 1:\n",
    "\n",
    "        # get list of symbol pairs; optionally apply dropout\n",
    "        pairs = [(bpe_codes[pair],i,pair) for (i,pair) in enumerate(zip(word, word[1:])) if (not dropout or random.random() > dropout) and pair in bpe_codes]\n",
    "\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        #get first merge operation in list of BPE codes\n",
    "        bigram = min(pairs)[2]\n",
    "\n",
    "        # find start position of all pairs that we want to merge\n",
    "        positions = [i for (rank,i,pair) in pairs if pair == bigram]\n",
    "\n",
    "        i = 0\n",
    "        new_word = []\n",
    "        bigram = ''.join(bigram)\n",
    "        for j in positions:\n",
    "            # merges are invalid if they start before current position. This can happen if there are overlapping pairs: (x x x -> xx x)\n",
    "            if j < i:\n",
    "                continue\n",
    "            new_word.extend(word[i:j]) # all symbols before merged pair\n",
    "            new_word.append(bigram) # merged pair\n",
    "            i = j+2 # continue after merged pair\n",
    "        new_word.extend(word[i:]) # add all symbols until end of word\n",
    "        word = new_word\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word[-1] = word[-1][:-4]\n",
    "\n",
    "    word = tuple(word)\n",
    "    if vocab:\n",
    "        word = check_vocab_and_split(word, bpe_codes_reverse, vocab, separator)\n",
    "\n",
    "    cache[orig] = word\n",
    "    return word\n",
    "\n",
    "def recursive_split(segment, bpe_codes, vocab, separator, final=False):\n",
    "    \"\"\"Recursively split segment into smaller units (by reversing BPE merges)\n",
    "    until all units are either in-vocabulary, or cannot be split futher.\"\"\"\n",
    "\n",
    "    try:\n",
    "        if final:\n",
    "            left, right = bpe_codes[segment + '</w>']\n",
    "            right = right[:-4]\n",
    "        else:\n",
    "            left, right = bpe_codes[segment]\n",
    "    except:\n",
    "        #sys.stderr.write('cannot split {0} further.\\n'.format(segment))\n",
    "        yield segment\n",
    "        return\n",
    "\n",
    "    if left + separator in vocab:\n",
    "        yield left\n",
    "    else:\n",
    "        for item in recursive_split(left, bpe_codes, vocab, separator, False):\n",
    "            yield item\n",
    "\n",
    "    if (final and right in vocab) or (not final and right + separator in vocab):\n",
    "        yield right\n",
    "    else:\n",
    "        for item in recursive_split(right, bpe_codes, vocab, separator, final):\n",
    "            yield item\n",
    "\n",
    "def check_vocab_and_split(orig, bpe_codes, vocab, separator):\n",
    "    \"\"\"Check for each segment in word if it is in-vocabulary,\n",
    "    and segment OOV segments into smaller units by reversing the BPE merge operations\"\"\"\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for segment in orig[:-1]:\n",
    "        if segment + separator in vocab:\n",
    "            out.append(segment)\n",
    "        else:\n",
    "            #sys.stderr.write('OOV: {0}\\n'.format(segment))\n",
    "            for item in recursive_split(segment, bpe_codes, vocab, separator, False):\n",
    "                out.append(item)\n",
    "\n",
    "    segment = orig[-1]\n",
    "    if segment in vocab:\n",
    "        out.append(segment)\n",
    "    else:\n",
    "        #sys.stderr.write('OOV: {0}\\n'.format(segment))\n",
    "        for item in recursive_split(segment, bpe_codes, vocab, separator, True):\n",
    "            out.append(item)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def read_vocabulary(vocab_file, threshold):\n",
    "    \"\"\"read vocabulary file produced by get_vocab.py, and filter according to frequency threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set()\n",
    "\n",
    "    for line in vocab_file:\n",
    "        word, freq = line.strip('\\r\\n ').split(' ')\n",
    "        freq = int(freq)\n",
    "        if threshold == None or freq >= threshold:\n",
    "            vocabulary.add(word)\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "def isolate_glossary(word, glossary):\n",
    "    \"\"\"\n",
    "    Isolate a glossary present inside a word.\n",
    "    Returns a list of subwords. In which all 'glossary' glossaries are isolated \n",
    "    For example, if 'USA' is the glossary and '1934USABUSA' the word, the return value is:\n",
    "        ['1934', 'USA', 'B', 'USA']\n",
    "    \"\"\"\n",
    "    # regex equivalent of (if word == glossary or glossary not in word)\n",
    "    if re.match('^'+glossary+'$', word) or not re.search(glossary, word):\n",
    "        return [word]\n",
    "    else:\n",
    "        segments = re.split(r'({})'.format(glossary), word)\n",
    "        segments, ending = segments[:-1], segments[-1]\n",
    "        segments = list(filter(None, segments)) # Remove empty strings in regex group.\n",
    "        return segments + [ending.strip('\\r\\n ')] if ending != '' else segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e0299-495c-4508-afac-6d604b47cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocabulary(vocab, file_name, is_dict=False):\n",
    "    \"\"\"Read text and return dictionary that encodes vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    #vocab = Counter()\n",
    "    with codecs.open(file_name, encoding='utf-8') as fobj:\n",
    "        for i, line in enumerate(fobj):\n",
    "            if is_dict:\n",
    "                try:\n",
    "                    word, count = line.strip('\\r\\n ').split(' ')\n",
    "                except:\n",
    "                    print('Failed reading vocabulary file at line {0}: {1}'.format(i, line))\n",
    "                    sys.exit(1)\n",
    "                vocab[word] += int(count)\n",
    "            else:\n",
    "                for word in line.strip('\\r\\n ').split(' '):\n",
    "                    if word:\n",
    "                        vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def update_pair_statistics(pair, changed, stats, indices):\n",
    "    \"\"\"Minimally update the indices and frequency of symbol pairs\n",
    "    if we merge a pair of symbols, only pairs that overlap with occurrences\n",
    "    of this pair are affected, and need to be updated.\n",
    "    \"\"\"\n",
    "    stats[pair] = 0\n",
    "    indices[pair] = defaultdict(int)\n",
    "    first, second = pair\n",
    "    new_pair = first+second\n",
    "    for j, word, old_word, freq in changed:\n",
    "\n",
    "        # find all instances of pair, and update frequency/indices around it\n",
    "        i = 0\n",
    "        while True:\n",
    "            # find first symbol\n",
    "            try:\n",
    "                i = old_word.index(first, i)\n",
    "            except ValueError:\n",
    "                break\n",
    "            # if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])\n",
    "            if i < len(old_word)-1 and old_word[i+1] == second:\n",
    "                # assuming a symbol sequence \"A B C\", if \"B C\" is merged, reduce the frequency of \"A B\"\n",
    "                if i:\n",
    "                    prev = old_word[i-1:i+1]\n",
    "                    stats[prev] -= freq\n",
    "                    indices[prev][j] -= 1\n",
    "                if i < len(old_word)-2:\n",
    "                    # assuming a symbol sequence \"A B C B\", if \"B C\" is merged, reduce the frequency of \"C B\".\n",
    "                    # however, skip this if the sequence is A B C B C, because the frequency of \"C B\" will be reduced by the previous code block\n",
    "                    if old_word[i+2] != first or i >= len(old_word)-3 or old_word[i+3] != second:\n",
    "                        nex = old_word[i+1:i+3]\n",
    "                        stats[nex] -= freq\n",
    "                        indices[nex][j] -= 1\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                # find new pair\n",
    "                i = word.index(new_pair, i)\n",
    "            except ValueError:\n",
    "                break\n",
    "            # assuming a symbol sequence \"A BC D\", if \"B C\" is merged, increase the frequency of \"A BC\"\n",
    "            if i:\n",
    "                prev = word[i-1:i+1]\n",
    "                stats[prev] += freq\n",
    "                indices[prev][j] += 1\n",
    "            # assuming a symbol sequence \"A BC B\", if \"B C\" is merged, increase the frequency of \"BC B\"\n",
    "            # however, if the sequence is A BC BC, skip this step because the count of \"BC BC\" will be incremented by the previous code block\n",
    "            if i < len(word)-1 and word[i+1] != new_pair:\n",
    "                nex = word[i:i+2]\n",
    "                stats[nex] += freq\n",
    "                indices[nex][j] += 1\n",
    "            i += 1\n",
    "\n",
    "\n",
    "def get_pair_statistics(vocab):\n",
    "    \"\"\"Count frequency of all symbol pairs, and create index\"\"\"\n",
    "\n",
    "    # data structure of pair frequencies\n",
    "    stats = defaultdict(int)\n",
    "\n",
    "    #index from pairs to words\n",
    "    indices = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for i, (word, freq) in enumerate(vocab):\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            stats[prev_char, char] += freq\n",
    "            indices[prev_char, char][i] += 1\n",
    "            prev_char = char\n",
    "\n",
    "    return stats, indices\n",
    "\n",
    "\n",
    "def replace_pair(pair, vocab, indices):\n",
    "    \"\"\"Replace all occurrences of a symbol pair ('A', 'B') with a new symbol 'AB'\"\"\"\n",
    "    first, second = pair\n",
    "    pair_str = ''.join(pair)\n",
    "    pair_str = pair_str.replace('\\\\','\\\\\\\\')\n",
    "    changes = []\n",
    "    pattern = re.compile(r'(?<!\\S)' + re.escape(first + ' ' + second) + r'(?!\\S)')\n",
    "    if sys.version_info < (3, 0):\n",
    "        iterator = indices[pair].iteritems()\n",
    "    else:\n",
    "        iterator = indices[pair].items()\n",
    "    for j, freq in iterator:\n",
    "        if freq < 1:\n",
    "            continue\n",
    "        word, freq = vocab[j]\n",
    "        new_word = ' '.join(word)\n",
    "        new_word = pattern.sub(pair_str, new_word)\n",
    "        new_word = tuple(new_word.split(' '))\n",
    "\n",
    "        vocab[j] = (new_word, freq)\n",
    "        changes.append((j, new_word, word, freq))\n",
    "\n",
    "    return changes\n",
    "\n",
    "def prune_stats(stats, big_stats, threshold):\n",
    "    \"\"\"Prune statistics dict for efficiency of max()\n",
    "    The frequency of a symbol pair never increases, so pruning is generally safe\n",
    "    (until we the most frequent pair is less frequent than a pair we previously pruned)\n",
    "    big_stats keeps full statistics for when we need to access pruned items\n",
    "    \"\"\"\n",
    "    for item,freq in list(stats.items()):\n",
    "        if freq < threshold:\n",
    "            del stats[item]\n",
    "            if freq < 0:\n",
    "                big_stats[item] += freq\n",
    "            else:\n",
    "                big_stats[item] = freq\n",
    "\n",
    "\n",
    "def learn_bpe(infile_names, outfile_name, num_symbols, min_frequency=2, verbose=False, is_dict=False, total_symbols=False):\n",
    "    \"\"\"Learn num_symbols BPE operations from vocabulary, and write to outfile.\n",
    "    \"\"\"\n",
    "    sys.stderr = codecs.getwriter('UTF-8')(sys.stderr.buffer)\n",
    "    sys.stdout = codecs.getwriter('UTF-8')(sys.stdout.buffer)\n",
    "    sys.stdin = codecs.getreader('UTF-8')(sys.stdin.buffer)\n",
    "\n",
    "    #vocab = get_vocabulary(infile, is_dict)\n",
    "    vocab = Counter()\n",
    "    for f in infile_names:\n",
    "        sys.stderr.write(f'Collecting vocab from {f}\\n')\n",
    "        vocab = update_vocabulary(vocab, f, is_dict)\n",
    "\n",
    "    vocab = dict([(tuple(x[:-1])+(x[-1]+'</w>',) ,y) for (x,y) in vocab.items()])\n",
    "    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    stats, indices = get_pair_statistics(sorted_vocab)\n",
    "    big_stats = copy.deepcopy(stats)\n",
    "\n",
    "    if total_symbols:\n",
    "        uniq_char_internal = set()\n",
    "        uniq_char_final = set()\n",
    "        for word in vocab:\n",
    "            for char in word[:-1]:\n",
    "                uniq_char_internal.add(char)\n",
    "            uniq_char_final.add(word[-1])\n",
    "        sys.stderr.write('Number of word-internal characters: {0}\\n'.format(len(uniq_char_internal)))\n",
    "        sys.stderr.write('Number of word-final characters: {0}\\n'.format(len(uniq_char_final)))\n",
    "        sys.stderr.write('Reducing number of merge operations by {0}\\n'.format(len(uniq_char_internal) + len(uniq_char_final)))\n",
    "        num_symbols -= len(uniq_char_internal) + len(uniq_char_final)\n",
    "\n",
    "\n",
    "    sys.stderr.write(f'Write vocab file to {outfile_name}')\n",
    "    with codecs.open(outfile_name, 'w', encoding='utf-8') as outfile:\n",
    "        # version 0.2 changes the handling of the end-of-word token ('</w>');\n",
    "        # version numbering allows bckward compatibility\n",
    "\n",
    "        outfile.write('#version: 0.2\\n')\n",
    "        # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "        threshold = max(stats.values()) / 10\n",
    "        for i in range(num_symbols):\n",
    "            if stats:\n",
    "                most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "\n",
    "            # we probably missed the best pair because of pruning; go back to full statistics\n",
    "            if not stats or (i and stats[most_frequent] < threshold):\n",
    "                prune_stats(stats, big_stats, threshold)\n",
    "                stats = copy.deepcopy(big_stats)\n",
    "                most_frequent = max(stats, key=lambda x: (stats[x], x))\n",
    "                # threshold is inspired by Zipfian assumption, but should only affect speed\n",
    "                threshold = stats[most_frequent] * i/(i+10000.0)\n",
    "                prune_stats(stats, big_stats, threshold)\n",
    "\n",
    "            if stats[most_frequent] < min_frequency:\n",
    "                sys.stderr.write(f'no pair has frequency >= {min_frequency}. Stopping\\n')\n",
    "                break\n",
    "\n",
    "            if verbose:\n",
    "                sys.stderr.write('pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n'.format(\n",
    "                    i, most_frequent[0], most_frequent[1], stats[most_frequent]))\n",
    "            outfile.write('{0} {1}\\n'.format(*most_frequent))\n",
    "            changes = replace_pair(most_frequent, sorted_vocab, indices)\n",
    "            update_pair_statistics(most_frequent, changes, stats, indices)\n",
    "            stats[most_frequent] = 0\n",
    "            if not i % 100:\n",
    "                prune_stats(stats, big_stats, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5903bb-053b-4c11-92c9-af36901ea8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import dill as pickle\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import codecs\n",
    "import spacy\n",
    "import torch\n",
    "import tarfile\n",
    "import torchtext.data\n",
    "import torchtext.datasets\n",
    "from torchtext.datasets import TranslationDataset\n",
    "#import transformer.Constants as Constants\n",
    "#from learn_bpe import learn_bpe\n",
    "#from apply_bpe import BPE\n",
    "\n",
    "\n",
    "__author__ = \"Yu-Hsiang Huang\"\n",
    "\n",
    "\n",
    "_TRAIN_DATA_SOURCES = [\n",
    "    {\"url\": \"http://data.statmt.org/wmt17/translation-task/\" \\\n",
    "             \"training-parallel-nc-v12.tgz\",\n",
    "     \"trg\": \"news-commentary-v12.de-en.en\",\n",
    "     \"src\": \"news-commentary-v12.de-en.de\"},\n",
    "    #{\"url\": \"http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz\",\n",
    "    # \"trg\": \"commoncrawl.de-en.en\",\n",
    "    # \"src\": \"commoncrawl.de-en.de\"},\n",
    "    #{\"url\": \"http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz\",\n",
    "    # \"trg\": \"europarl-v7.de-en.en\",\n",
    "    # \"src\": \"europarl-v7.de-en.de\"}\n",
    "    ]\n",
    "\n",
    "_VAL_DATA_SOURCES = [\n",
    "    {\"url\": \"http://data.statmt.org/wmt17/translation-task/dev.tgz\",\n",
    "     \"trg\": \"newstest2013.en\",\n",
    "     \"src\": \"newstest2013.de\"}]\n",
    "\n",
    "_TEST_DATA_SOURCES = [\n",
    "    {\"url\": \"https://storage.googleapis.com/tf-perf-public/\" \\\n",
    "                \"official_transformer/test_data/newstest2014.tgz\",\n",
    "     \"trg\": \"newstest2014.en\",\n",
    "     \"src\": \"newstest2014.de\"}]\n",
    "\n",
    "\n",
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def file_exist(dir_name, file_name):\n",
    "    for sub_dir, _, files in os.walk(dir_name):\n",
    "        if file_name in files:\n",
    "            return os.path.join(sub_dir, file_name)\n",
    "    return None\n",
    "\n",
    "\n",
    "def download_and_extract(download_dir, url, src_filename, trg_filename):\n",
    "    src_path = file_exist(download_dir, src_filename)\n",
    "    trg_path = file_exist(download_dir, trg_filename)\n",
    "\n",
    "    if src_path and trg_path:\n",
    "        sys.stderr.write(f\"Already downloaded and extracted {url}.\\n\")\n",
    "        return src_path, trg_path\n",
    "\n",
    "    compressed_file = _download_file(download_dir, url)\n",
    "\n",
    "    sys.stderr.write(f\"Extracting {compressed_file}.\\n\")\n",
    "    with tarfile.open(compressed_file, \"r:gz\") as corpus_tar:\n",
    "        corpus_tar.extractall(download_dir)\n",
    "\n",
    "    src_path = file_exist(download_dir, src_filename)\n",
    "    trg_path = file_exist(download_dir, trg_filename)\n",
    "\n",
    "    if src_path and trg_path:\n",
    "        return src_path, trg_path\n",
    "\n",
    "    raise OSError(f\"Download/extraction failed for url {url} to path {download_dir}\")\n",
    "\n",
    "\n",
    "def _download_file(download_dir, url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if file_exist(download_dir, filename):\n",
    "        sys.stderr.write(f\"Already downloaded: {url} (at {filename}).\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(f\"Downloading from {url} to {filename}.\\n\")\n",
    "        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
    "            urllib.request.urlretrieve(url, filename=filename, reporthook=t.update_to)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def get_raw_files(raw_dir, sources):\n",
    "    raw_files = { \"src\": [], \"trg\": [], }\n",
    "    for d in sources:\n",
    "        src_file, trg_file = download_and_extract(raw_dir, d[\"url\"], d[\"src\"], d[\"trg\"])\n",
    "        raw_files[\"src\"].append(src_file)\n",
    "        raw_files[\"trg\"].append(trg_file)\n",
    "    return raw_files\n",
    "\n",
    "\n",
    "def mkdir_if_needed(dir_name):\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "\n",
    "def compile_files(raw_dir, raw_files, prefix):\n",
    "    src_fpath = os.path.join(raw_dir, f\"raw-{prefix}.src\")\n",
    "    trg_fpath = os.path.join(raw_dir, f\"raw-{prefix}.trg\")\n",
    "\n",
    "    if os.path.isfile(src_fpath) and os.path.isfile(trg_fpath):\n",
    "        sys.stderr.write(f\"Merged files found, skip the merging process.\\n\")\n",
    "        return src_fpath, trg_fpath\n",
    "\n",
    "    sys.stderr.write(f\"Merge files into two files: {src_fpath} and {trg_fpath}.\\n\")\n",
    "\n",
    "    with open(src_fpath, 'w') as src_outf, open(trg_fpath, 'w') as trg_outf:\n",
    "        for src_inf, trg_inf in zip(raw_files['src'], raw_files['trg']):\n",
    "            sys.stderr.write(f'  Input files: \\n'\\\n",
    "                    f'    - SRC: {src_inf}, and\\n' \\\n",
    "                    f'    - TRG: {trg_inf}.\\n')\n",
    "            with open(src_inf, newline='\\n') as src_inf, open(trg_inf, newline='\\n') as trg_inf:\n",
    "                cntr = 0\n",
    "                for i, line in enumerate(src_inf):\n",
    "                    cntr += 1\n",
    "                    src_outf.write(line.replace('\\r', ' ').strip() + '\\n')\n",
    "                for j, line in enumerate(trg_inf):\n",
    "                    cntr -= 1\n",
    "                    trg_outf.write(line.replace('\\r', ' ').strip() + '\\n')\n",
    "                assert cntr == 0, 'Number of lines in two files are inconsistent.'\n",
    "    return src_fpath, trg_fpath\n",
    "\n",
    "\n",
    "def encode_file(bpe, in_file, out_file):\n",
    "    sys.stderr.write(f\"Read raw content from {in_file} and \\n\"\\\n",
    "            f\"Write encoded content to {out_file}\\n\")\n",
    "    \n",
    "    with codecs.open(in_file, encoding='utf-8') as in_f:\n",
    "        with codecs.open(out_file, 'w', encoding='utf-8') as out_f:\n",
    "            for line in in_f:\n",
    "                out_f.write(bpe.process_line(line))\n",
    "\n",
    "\n",
    "def encode_files(bpe, src_in_file, trg_in_file, data_dir, prefix):\n",
    "    src_out_file = os.path.join(data_dir, f\"{prefix}.src\")\n",
    "    trg_out_file = os.path.join(data_dir, f\"{prefix}.trg\")\n",
    "\n",
    "    if os.path.isfile(src_out_file) and os.path.isfile(trg_out_file):\n",
    "        sys.stderr.write(f\"Encoded files found, skip the encoding process ...\\n\")\n",
    "\n",
    "    encode_file(bpe, src_in_file, src_out_file)\n",
    "    encode_file(bpe, trg_in_file, trg_out_file)\n",
    "    return src_out_file, trg_out_file\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-raw_dir', required=True)\n",
    "    parser.add_argument('-data_dir', required=True)\n",
    "    parser.add_argument('-codes', required=True)\n",
    "    parser.add_argument('-save_data', required=True)\n",
    "    parser.add_argument('-prefix', required=True)\n",
    "    parser.add_argument('-max_len', type=int, default=100)\n",
    "    parser.add_argument('--symbols', '-s', type=int, default=32000, help=\"Vocabulary size\")\n",
    "    parser.add_argument(\n",
    "        '--min-frequency', type=int, default=6, metavar='FREQ',\n",
    "        help='Stop if no symbol pair has frequency >= FREQ (default: %(default)s))')\n",
    "    parser.add_argument('--dict-input', action=\"store_true\",\n",
    "        help=\"If set, input file is interpreted as a dictionary where each line contains a word-count pair\")\n",
    "    parser.add_argument(\n",
    "        '--separator', type=str, default='@@', metavar='STR',\n",
    "        help=\"Separator between non-final subword units (default: '%(default)s'))\")\n",
    "    parser.add_argument('--total-symbols', '-t', action=\"store_true\")\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    # Create folder if needed.\n",
    "    mkdir_if_needed(opt.raw_dir)\n",
    "    mkdir_if_needed(opt.data_dir)\n",
    "\n",
    "    # Download and extract raw data.\n",
    "    raw_train = get_raw_files(opt.raw_dir, _TRAIN_DATA_SOURCES)\n",
    "    raw_val = get_raw_files(opt.raw_dir, _VAL_DATA_SOURCES)\n",
    "    raw_test = get_raw_files(opt.raw_dir, _TEST_DATA_SOURCES)\n",
    "\n",
    "    # Merge files into one.\n",
    "    train_src, train_trg = compile_files(opt.raw_dir, raw_train, opt.prefix + '-train')\n",
    "    val_src, val_trg = compile_files(opt.raw_dir, raw_val, opt.prefix + '-val')\n",
    "    test_src, test_trg = compile_files(opt.raw_dir, raw_test, opt.prefix + '-test')\n",
    "\n",
    "    # Build up the code from training files if not exist\n",
    "    opt.codes = os.path.join(opt.data_dir, opt.codes)\n",
    "    if not os.path.isfile(opt.codes):\n",
    "        sys.stderr.write(f\"Collect codes from training data and save to {opt.codes}.\\n\")\n",
    "        learn_bpe(raw_train['src'] + raw_train['trg'], opt.codes, opt.symbols, opt.min_frequency, True)\n",
    "    sys.stderr.write(f\"BPE codes prepared.\\n\")\n",
    "\n",
    "    sys.stderr.write(f\"Build up the tokenizer.\\n\")\n",
    "    with codecs.open(opt.codes, encoding='utf-8') as codes: \n",
    "        bpe = BPE(codes, separator=opt.separator)\n",
    "\n",
    "    sys.stderr.write(f\"Encoding ...\\n\")\n",
    "    encode_files(bpe, train_src, train_trg, opt.data_dir, opt.prefix + '-train')\n",
    "    encode_files(bpe, val_src, val_trg, opt.data_dir, opt.prefix + '-val')\n",
    "    encode_files(bpe, test_src, test_trg, opt.data_dir, opt.prefix + '-test')\n",
    "    sys.stderr.write(f\"Done.\\n\")\n",
    "\n",
    "\n",
    "    field = torchtext.data.Field(\n",
    "        tokenize=str.split,\n",
    "        lower=True,\n",
    "        pad_token=Constants.PAD_WORD,\n",
    "        init_token=Constants.BOS_WORD,\n",
    "        eos_token=Constants.EOS_WORD)\n",
    "\n",
    "    fields = (field, field)\n",
    "\n",
    "    MAX_LEN = opt.max_len\n",
    "\n",
    "    def filter_examples_with_length(x):\n",
    "        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "    enc_train_files_prefix = opt.prefix + '-train'\n",
    "    train = TranslationDataset(\n",
    "        fields=fields,\n",
    "        path=os.path.join(opt.data_dir, enc_train_files_prefix),\n",
    "        exts=('.src', '.trg'),\n",
    "        filter_pred=filter_examples_with_length)\n",
    "\n",
    "    from itertools import chain\n",
    "    field.build_vocab(chain(train.src, train.trg), min_freq=2)\n",
    "\n",
    "    data = { 'settings': opt, 'vocab': field, }\n",
    "    opt.save_data = os.path.join(opt.data_dir, opt.save_data)\n",
    "\n",
    "    print('[Info] Dumping the processed data to pickle file', opt.save_data)\n",
    "    pickle.dump(data, open(opt.save_data, 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "def main_wo_bpe():\n",
    "    '''\n",
    "    Usage: python preprocess.py -lang_src de -lang_trg en -save_data multi30k_de_en.pkl -share_vocab\n",
    "    '''\n",
    "\n",
    "    #spacy_support_langs = ['de', 'el', 'en', 'es', 'fr', 'it', 'lt', 'nb', 'nl', 'pt']\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-lang_src', default='de')\n",
    "    parser.add_argument('-lang_trg', default='en')\n",
    "    parser.add_argument('-save_data', default='m30k_deen_shr.pkl')\n",
    "    parser.add_argument('-data_src', type=str, default=None)\n",
    "    parser.add_argument('-data_trg', type=str, default=None)\n",
    "\n",
    "    parser.add_argument('-max_len', type=int, default=100)\n",
    "    parser.add_argument('-min_word_count', type=int, default=3)\n",
    "    parser.add_argument('-keep_case', action='store_true')\n",
    "    parser.add_argument('-share_vocab', action='store_true')\n",
    "    #parser.add_argument('-ratio', '--train_valid_test_ratio', type=int, nargs=3, metavar=(8,1,1))\n",
    "    #parser.add_argument('-vocab', default=None)\n",
    "\n",
    "    opt = parser.parse_args([])\n",
    "    assert not any([opt.data_src, opt.data_trg]), 'Custom data input is not support now.'\n",
    "    assert not any([opt.data_src, opt.data_trg]) or all([opt.data_src, opt.data_trg])\n",
    "    print(opt)\n",
    "\n",
    "    src_lang_model = spacy.load('de_core_news_sm')\n",
    "    trg_lang_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize_src(text):\n",
    "        return [tok.text for tok in src_lang_model.tokenizer(text)]\n",
    "\n",
    "    def tokenize_trg(text):\n",
    "        return [tok.text for tok in trg_lang_model.tokenizer(text)]\n",
    "\n",
    "    SRC = torchtext.data.Field(\n",
    "        tokenize=tokenize_src, lower=not opt.keep_case,\n",
    "        pad_token=PAD_WORD, init_token=BOS_WORD, eos_token=EOS_WORD)\n",
    "\n",
    "    TRG = torchtext.data.Field(\n",
    "        tokenize=tokenize_trg, lower=not opt.keep_case,\n",
    "        pad_token=PAD_WORD, init_token=BOS_WORD, eos_token=EOS_WORD)\n",
    "\n",
    "    MAX_LEN = opt.max_len\n",
    "    MIN_FREQ = opt.min_word_count\n",
    "\n",
    "    if not all([opt.data_src, opt.data_trg]):\n",
    "        assert {opt.lang_src, opt.lang_trg} == {'de', 'en'}\n",
    "    else:\n",
    "        # Pack custom txt file into example datasets\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def filter_examples_with_length(x):\n",
    "        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "    train, val, test = torchtext.datasets.Multi30k.splits(\n",
    "            exts = ('.' + opt.lang_src, '.' + opt.lang_trg),\n",
    "            fields = (SRC, TRG),\n",
    "            filter_pred=filter_examples_with_length)\n",
    "\n",
    "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "    print('[Info] Get source language vocabulary size:', len(SRC.vocab))\n",
    "    TRG.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "    print('[Info] Get target language vocabulary size:', len(TRG.vocab))\n",
    "\n",
    "    if opt.share_vocab:\n",
    "        print('[Info] Merging two vocabulary ...')\n",
    "        for w, _ in SRC.vocab.stoi.items():\n",
    "            # TODO: Also update the `freq`, although it is not likely to be used.\n",
    "            if w not in TRG.vocab.stoi:\n",
    "                TRG.vocab.stoi[w] = len(TRG.vocab.stoi)\n",
    "        TRG.vocab.itos = [None] * len(TRG.vocab.stoi)\n",
    "        for w, i in TRG.vocab.stoi.items():\n",
    "            TRG.vocab.itos[i] = w\n",
    "        SRC.vocab.stoi = TRG.vocab.stoi\n",
    "        SRC.vocab.itos = TRG.vocab.itos\n",
    "        print('[Info] Get merged vocabulary size:', len(TRG.vocab))\n",
    "\n",
    "\n",
    "    data = {\n",
    "        'settings': opt,\n",
    "        'vocab': {'src': SRC, 'trg': TRG},\n",
    "        'train': train.examples,\n",
    "        'valid': val.examples,\n",
    "        'test': test.examples}\n",
    "\n",
    "    print('[Info] Dumping the processed data to pickle file', opt.save_data)\n",
    "    pickle.dump(data, open(opt.save_data, 'wb'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_wo_bpe()\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919963b-bee1-4fed-a67e-2ed0b2d93a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb699f-ac62-42bc-a48d-785d346235af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24394e1-4856-4e1f-beb5-1229988ab60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51747955-32e7-4f5e-a05a-8d6aecd71a9e",
   "metadata": {},
   "source": [
    "## 8.https://github.com/sushant097/Deep-Learning-Paper-Scratch-Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954e4e5-6b95-49f1-bbf0-6c16382927eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads ==\n",
    "                embed_size), \"Embed size needs to be div by heads\"\n",
    "\n",
    "        # - We will break the embedding into `heads` chunks and feed each to a different attention head\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        # This unifies the outputs of the different heads into\n",
    "        # a single head_dim-vector\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split embedding into self.heads pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # after reshape, send it through linear layer\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # fold heads into the batch dimension\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # ->outputshape\n",
    "        # queries shape: (N, query_len, heads, heads_dim)\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy shape: (N, heads, query_len, key_len)\n",
    "        # or you can use\n",
    "        # torch.bmm()\n",
    "\n",
    "        # change matrix to lower traingular matrix by making upper half is zero i.e softmax(-inf) = 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(\n",
    "                mask == 0, float(\"-1e20\"))  # set to - infinity\n",
    "\n",
    "        # Attention(Q,K,V) = softmax(((QK^T) / sqrt(dk)) * V)\n",
    "        # dim=3 --> normalizing accross key_len\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # (N, query_len, heads, head_dim)\n",
    "        # after einsum (N, query_len, heads, head_dim) then flatten last two dimensions\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion) -> None:\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "        # normalize across every single example\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # not do anything, extra computation for mapping back\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)  # dropout layer\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(\n",
    "            value, key, query, mask)  # multi-head attention\n",
    "        attention = attention + query  # adding skip connection\n",
    "        x = self.dropout(self.norm1(attention))\n",
    "        forward = self.feed_forward(x)\n",
    "        x = forward + x  # adding skip connection\n",
    "        out = self.dropout(self.norm2(x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length  # how long max sentence is\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(\n",
    "            N, seq_length).to(self.device)\n",
    "\n",
    "        # positional embedding find out structure of text\n",
    "        out = self.dropout(self.word_embedding(\n",
    "            x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # all inputs are same in encoder\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device) -> None:\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        attention = attention + x  # adding skip connection\n",
    "        query = self.dropout(self.norm(attention))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 trg_vocab_size,\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length\n",
    "                 ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.postion_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(\n",
    "            N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(\n",
    "            x) + self.postion_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=256,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cuda\",\n",
    "        max_length=100\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )  # expand so that it is available to each training example\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [\n",
    "                     1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
    "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [\n",
    "                       1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "    src_pad_idx = 0\n",
    "    trg_pad_idx = 0\n",
    "    src_vocab_size = 10\n",
    "    trg_vocab_size = 10\n",
    "    model = Transformer(src_vocab_size, trg_vocab_size,\n",
    "                        src_pad_idx, trg_pad_idx, device=device).to(device)\n",
    "    out = model(x, trg[:, :-1])\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6decec-6b02-4ab9-b3fb-5a0c4e91d68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Optimization-Python",
   "language": "python",
   "name": "optimization-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
