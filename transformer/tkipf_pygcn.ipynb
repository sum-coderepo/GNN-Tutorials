{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30574533-a868-4ab8-a1ad-10f5523f4299",
   "metadata": {},
   "source": [
    "## 1. https://github.com/tkipf/pygcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6151e5ea-99ab-43f7-ac05-8b582942adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "062899e5-ed3e-4d40-bf72-212626624eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9776234d-e661-4150-85ca-58af20d938ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "818710dc-7749-4bd5-afe2-3c90491a6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37422d3-b791-4cc9-9cea-c619d79f537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0773262b-7381-4642-a190-9c6c06e6d9dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9803 acc_train: 0.0857 loss_val: 1.9716 acc_val: 0.0667 time: 2.8904s\n",
      "Epoch: 0002 loss_train: 1.9597 acc_train: 0.0929 loss_val: 1.9575 acc_val: 0.0667 time: 0.0126s\n",
      "Epoch: 0003 loss_train: 1.9486 acc_train: 0.0929 loss_val: 1.9443 acc_val: 0.1100 time: 0.0130s\n",
      "Epoch: 0004 loss_train: 1.9419 acc_train: 0.1786 loss_val: 1.9318 acc_val: 0.1567 time: 0.0100s\n",
      "Epoch: 0005 loss_train: 1.9326 acc_train: 0.2000 loss_val: 1.9198 acc_val: 0.1567 time: 0.0110s\n",
      "Epoch: 0006 loss_train: 1.9085 acc_train: 0.2143 loss_val: 1.9085 acc_val: 0.1567 time: 0.0080s\n",
      "Epoch: 0007 loss_train: 1.9062 acc_train: 0.2786 loss_val: 1.8979 acc_val: 0.1567 time: 0.0107s\n",
      "Epoch: 0008 loss_train: 1.8959 acc_train: 0.2500 loss_val: 1.8880 acc_val: 0.1567 time: 0.0091s\n",
      "Epoch: 0009 loss_train: 1.8901 acc_train: 0.2714 loss_val: 1.8787 acc_val: 0.1567 time: 0.0100s\n",
      "Epoch: 0010 loss_train: 1.8723 acc_train: 0.2357 loss_val: 1.8697 acc_val: 0.1700 time: 0.0110s\n",
      "Epoch: 0011 loss_train: 1.8695 acc_train: 0.2571 loss_val: 1.8611 acc_val: 0.2767 time: 0.0089s\n",
      "Epoch: 0012 loss_train: 1.8407 acc_train: 0.3643 loss_val: 1.8525 acc_val: 0.4033 time: 0.0080s\n",
      "Epoch: 0013 loss_train: 1.8545 acc_train: 0.2786 loss_val: 1.8441 acc_val: 0.4433 time: 0.0086s\n",
      "Epoch: 0014 loss_train: 1.8433 acc_train: 0.3214 loss_val: 1.8358 acc_val: 0.4333 time: 0.0092s\n",
      "Epoch: 0015 loss_train: 1.8420 acc_train: 0.3429 loss_val: 1.8278 acc_val: 0.4033 time: 0.0092s\n",
      "Epoch: 0016 loss_train: 1.8396 acc_train: 0.2786 loss_val: 1.8201 acc_val: 0.3733 time: 0.0101s\n",
      "Epoch: 0017 loss_train: 1.8199 acc_train: 0.3214 loss_val: 1.8123 acc_val: 0.3700 time: 0.0085s\n",
      "Epoch: 0018 loss_train: 1.8000 acc_train: 0.3714 loss_val: 1.8042 acc_val: 0.3633 time: 0.0088s\n",
      "Epoch: 0019 loss_train: 1.7902 acc_train: 0.3143 loss_val: 1.7962 acc_val: 0.3633 time: 0.0080s\n",
      "Epoch: 0020 loss_train: 1.7905 acc_train: 0.3357 loss_val: 1.7882 acc_val: 0.3500 time: 0.0073s\n",
      "Epoch: 0021 loss_train: 1.7875 acc_train: 0.3000 loss_val: 1.7804 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0022 loss_train: 1.7860 acc_train: 0.2929 loss_val: 1.7731 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0023 loss_train: 1.7778 acc_train: 0.3000 loss_val: 1.7661 acc_val: 0.3500 time: 0.0090s\n",
      "Epoch: 0024 loss_train: 1.7447 acc_train: 0.3071 loss_val: 1.7592 acc_val: 0.3500 time: 0.0107s\n",
      "Epoch: 0025 loss_train: 1.7727 acc_train: 0.3214 loss_val: 1.7525 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0026 loss_train: 1.7294 acc_train: 0.3786 loss_val: 1.7460 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0027 loss_train: 1.7445 acc_train: 0.3000 loss_val: 1.7394 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0028 loss_train: 1.7318 acc_train: 0.3071 loss_val: 1.7329 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0029 loss_train: 1.7159 acc_train: 0.3429 loss_val: 1.7264 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0030 loss_train: 1.6984 acc_train: 0.3429 loss_val: 1.7200 acc_val: 0.3500 time: 0.0081s\n",
      "Epoch: 0031 loss_train: 1.7147 acc_train: 0.3643 loss_val: 1.7137 acc_val: 0.3500 time: 0.0079s\n",
      "Epoch: 0032 loss_train: 1.6822 acc_train: 0.3857 loss_val: 1.7075 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0033 loss_train: 1.7081 acc_train: 0.3214 loss_val: 1.7013 acc_val: 0.3500 time: 0.0071s\n",
      "Epoch: 0034 loss_train: 1.7010 acc_train: 0.3643 loss_val: 1.6948 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0035 loss_train: 1.6576 acc_train: 0.3357 loss_val: 1.6880 acc_val: 0.3600 time: 0.0069s\n",
      "Epoch: 0036 loss_train: 1.6960 acc_train: 0.3571 loss_val: 1.6809 acc_val: 0.3600 time: 0.0070s\n",
      "Epoch: 0037 loss_train: 1.6338 acc_train: 0.3857 loss_val: 1.6736 acc_val: 0.3600 time: 0.0070s\n",
      "Epoch: 0038 loss_train: 1.6748 acc_train: 0.3929 loss_val: 1.6662 acc_val: 0.3600 time: 0.0070s\n",
      "Epoch: 0039 loss_train: 1.6380 acc_train: 0.3786 loss_val: 1.6583 acc_val: 0.3800 time: 0.0070s\n",
      "Epoch: 0040 loss_train: 1.6084 acc_train: 0.4143 loss_val: 1.6499 acc_val: 0.3800 time: 0.0070s\n",
      "Epoch: 0041 loss_train: 1.6104 acc_train: 0.3786 loss_val: 1.6412 acc_val: 0.3900 time: 0.0070s\n",
      "Epoch: 0042 loss_train: 1.5887 acc_train: 0.4071 loss_val: 1.6318 acc_val: 0.4033 time: 0.0070s\n",
      "Epoch: 0043 loss_train: 1.5508 acc_train: 0.4357 loss_val: 1.6224 acc_val: 0.4100 time: 0.0070s\n",
      "Epoch: 0044 loss_train: 1.5425 acc_train: 0.4929 loss_val: 1.6127 acc_val: 0.4300 time: 0.0081s\n",
      "Epoch: 0045 loss_train: 1.5169 acc_train: 0.4857 loss_val: 1.6030 acc_val: 0.4433 time: 0.0069s\n",
      "Epoch: 0046 loss_train: 1.5223 acc_train: 0.5071 loss_val: 1.5930 acc_val: 0.4667 time: 0.0070s\n",
      "Epoch: 0047 loss_train: 1.5611 acc_train: 0.4857 loss_val: 1.5826 acc_val: 0.4900 time: 0.0080s\n",
      "Epoch: 0048 loss_train: 1.4972 acc_train: 0.4929 loss_val: 1.5719 acc_val: 0.4967 time: 0.0070s\n",
      "Epoch: 0049 loss_train: 1.5000 acc_train: 0.4929 loss_val: 1.5606 acc_val: 0.5033 time: 0.0070s\n",
      "Epoch: 0050 loss_train: 1.4670 acc_train: 0.5429 loss_val: 1.5491 acc_val: 0.5233 time: 0.0070s\n",
      "Epoch: 0051 loss_train: 1.4389 acc_train: 0.5143 loss_val: 1.5372 acc_val: 0.5400 time: 0.0066s\n",
      "Epoch: 0052 loss_train: 1.4495 acc_train: 0.5357 loss_val: 1.5247 acc_val: 0.5500 time: 0.0070s\n",
      "Epoch: 0053 loss_train: 1.4258 acc_train: 0.5643 loss_val: 1.5126 acc_val: 0.5667 time: 0.0100s\n",
      "Epoch: 0054 loss_train: 1.3683 acc_train: 0.6071 loss_val: 1.4998 acc_val: 0.5700 time: 0.0070s\n",
      "Epoch: 0055 loss_train: 1.3894 acc_train: 0.5714 loss_val: 1.4864 acc_val: 0.5700 time: 0.0070s\n",
      "Epoch: 0056 loss_train: 1.3699 acc_train: 0.6000 loss_val: 1.4728 acc_val: 0.5767 time: 0.0070s\n",
      "Epoch: 0057 loss_train: 1.3591 acc_train: 0.5929 loss_val: 1.4590 acc_val: 0.5800 time: 0.0070s\n",
      "Epoch: 0058 loss_train: 1.3179 acc_train: 0.5857 loss_val: 1.4453 acc_val: 0.5867 time: 0.0060s\n",
      "Epoch: 0059 loss_train: 1.3284 acc_train: 0.5714 loss_val: 1.4314 acc_val: 0.5900 time: 0.0077s\n",
      "Epoch: 0060 loss_train: 1.3002 acc_train: 0.6143 loss_val: 1.4178 acc_val: 0.5933 time: 0.0070s\n",
      "Epoch: 0061 loss_train: 1.2937 acc_train: 0.6429 loss_val: 1.4039 acc_val: 0.5933 time: 0.0060s\n",
      "Epoch: 0062 loss_train: 1.2585 acc_train: 0.6143 loss_val: 1.3901 acc_val: 0.5933 time: 0.0060s\n",
      "Epoch: 0063 loss_train: 1.2617 acc_train: 0.6214 loss_val: 1.3765 acc_val: 0.5900 time: 0.0060s\n",
      "Epoch: 0064 loss_train: 1.2628 acc_train: 0.6357 loss_val: 1.3630 acc_val: 0.5900 time: 0.0070s\n",
      "Epoch: 0065 loss_train: 1.1809 acc_train: 0.6429 loss_val: 1.3496 acc_val: 0.6033 time: 0.0070s\n",
      "Epoch: 0066 loss_train: 1.1976 acc_train: 0.6500 loss_val: 1.3358 acc_val: 0.6033 time: 0.0060s\n",
      "Epoch: 0067 loss_train: 1.1497 acc_train: 0.6714 loss_val: 1.3225 acc_val: 0.6200 time: 0.0090s\n",
      "Epoch: 0068 loss_train: 1.1638 acc_train: 0.6214 loss_val: 1.3088 acc_val: 0.6333 time: 0.0070s\n",
      "Epoch: 0069 loss_train: 1.1448 acc_train: 0.7214 loss_val: 1.2949 acc_val: 0.6367 time: 0.0060s\n",
      "Epoch: 0070 loss_train: 1.1411 acc_train: 0.6786 loss_val: 1.2815 acc_val: 0.6400 time: 0.0070s\n",
      "Epoch: 0071 loss_train: 1.0834 acc_train: 0.7357 loss_val: 1.2682 acc_val: 0.6533 time: 0.0060s\n",
      "Epoch: 0072 loss_train: 1.0955 acc_train: 0.7143 loss_val: 1.2548 acc_val: 0.6567 time: 0.0070s\n",
      "Epoch: 0073 loss_train: 1.0758 acc_train: 0.7357 loss_val: 1.2419 acc_val: 0.6567 time: 0.0070s\n",
      "Epoch: 0074 loss_train: 1.0508 acc_train: 0.7357 loss_val: 1.2294 acc_val: 0.6633 time: 0.0080s\n",
      "Epoch: 0075 loss_train: 1.0695 acc_train: 0.7000 loss_val: 1.2173 acc_val: 0.6700 time: 0.0070s\n",
      "Epoch: 0076 loss_train: 1.0452 acc_train: 0.7000 loss_val: 1.2056 acc_val: 0.6733 time: 0.0070s\n",
      "Epoch: 0077 loss_train: 1.0216 acc_train: 0.7071 loss_val: 1.1943 acc_val: 0.6800 time: 0.0070s\n",
      "Epoch: 0078 loss_train: 0.9462 acc_train: 0.7571 loss_val: 1.1834 acc_val: 0.6933 time: 0.0070s\n",
      "Epoch: 0079 loss_train: 1.0251 acc_train: 0.7286 loss_val: 1.1735 acc_val: 0.6967 time: 0.0070s\n",
      "Epoch: 0080 loss_train: 0.9742 acc_train: 0.7571 loss_val: 1.1638 acc_val: 0.7033 time: 0.0062s\n",
      "Epoch: 0081 loss_train: 0.9546 acc_train: 0.7571 loss_val: 1.1536 acc_val: 0.7133 time: 0.0090s\n",
      "Epoch: 0082 loss_train: 0.9514 acc_train: 0.7357 loss_val: 1.1430 acc_val: 0.7167 time: 0.0081s\n",
      "Epoch: 0083 loss_train: 0.9468 acc_train: 0.7786 loss_val: 1.1326 acc_val: 0.7167 time: 0.0090s\n",
      "Epoch: 0084 loss_train: 0.9702 acc_train: 0.7357 loss_val: 1.1228 acc_val: 0.7167 time: 0.0115s\n",
      "Epoch: 0085 loss_train: 0.9305 acc_train: 0.7500 loss_val: 1.1124 acc_val: 0.7167 time: 0.0090s\n",
      "Epoch: 0086 loss_train: 0.9014 acc_train: 0.7643 loss_val: 1.1015 acc_val: 0.7200 time: 0.0080s\n",
      "Epoch: 0087 loss_train: 0.9550 acc_train: 0.7429 loss_val: 1.0911 acc_val: 0.7167 time: 0.0080s\n",
      "Epoch: 0088 loss_train: 0.9202 acc_train: 0.7286 loss_val: 1.0812 acc_val: 0.7167 time: 0.0096s\n",
      "Epoch: 0089 loss_train: 0.8978 acc_train: 0.7571 loss_val: 1.0717 acc_val: 0.7200 time: 0.0090s\n",
      "Epoch: 0090 loss_train: 0.8547 acc_train: 0.8071 loss_val: 1.0627 acc_val: 0.7200 time: 0.0080s\n",
      "Epoch: 0091 loss_train: 0.8769 acc_train: 0.7714 loss_val: 1.0543 acc_val: 0.7233 time: 0.0090s\n",
      "Epoch: 0092 loss_train: 0.8604 acc_train: 0.7643 loss_val: 1.0464 acc_val: 0.7267 time: 0.0101s\n",
      "Epoch: 0093 loss_train: 0.8037 acc_train: 0.8071 loss_val: 1.0390 acc_val: 0.7333 time: 0.0119s\n",
      "Epoch: 0094 loss_train: 0.8491 acc_train: 0.7643 loss_val: 1.0324 acc_val: 0.7333 time: 0.0110s\n",
      "Epoch: 0095 loss_train: 0.8597 acc_train: 0.7429 loss_val: 1.0260 acc_val: 0.7400 time: 0.0090s\n",
      "Epoch: 0096 loss_train: 0.8632 acc_train: 0.7929 loss_val: 1.0195 acc_val: 0.7400 time: 0.0090s\n",
      "Epoch: 0097 loss_train: 0.8137 acc_train: 0.8500 loss_val: 1.0131 acc_val: 0.7433 time: 0.0080s\n",
      "Epoch: 0098 loss_train: 0.8039 acc_train: 0.8143 loss_val: 1.0064 acc_val: 0.7467 time: 0.0090s\n",
      "Epoch: 0099 loss_train: 0.8042 acc_train: 0.7786 loss_val: 1.0001 acc_val: 0.7467 time: 0.0085s\n",
      "Epoch: 0100 loss_train: 0.7794 acc_train: 0.8286 loss_val: 0.9945 acc_val: 0.7467 time: 0.0096s\n",
      "Epoch: 0101 loss_train: 0.8017 acc_train: 0.8286 loss_val: 0.9879 acc_val: 0.7533 time: 0.0101s\n",
      "Epoch: 0102 loss_train: 0.7539 acc_train: 0.8071 loss_val: 0.9820 acc_val: 0.7533 time: 0.0100s\n",
      "Epoch: 0103 loss_train: 0.7845 acc_train: 0.8000 loss_val: 0.9757 acc_val: 0.7567 time: 0.0106s\n",
      "Epoch: 0104 loss_train: 0.7592 acc_train: 0.8429 loss_val: 0.9704 acc_val: 0.7567 time: 0.0110s\n",
      "Epoch: 0105 loss_train: 0.7796 acc_train: 0.7857 loss_val: 0.9652 acc_val: 0.7567 time: 0.0115s\n",
      "Epoch: 0106 loss_train: 0.7476 acc_train: 0.8000 loss_val: 0.9608 acc_val: 0.7600 time: 0.0111s\n",
      "Epoch: 0107 loss_train: 0.7275 acc_train: 0.8429 loss_val: 0.9566 acc_val: 0.7600 time: 0.0116s\n",
      "Epoch: 0108 loss_train: 0.6824 acc_train: 0.8571 loss_val: 0.9514 acc_val: 0.7600 time: 0.0146s\n",
      "Epoch: 0109 loss_train: 0.6991 acc_train: 0.8500 loss_val: 0.9447 acc_val: 0.7633 time: 0.0137s\n",
      "Epoch: 0110 loss_train: 0.7099 acc_train: 0.9071 loss_val: 0.9384 acc_val: 0.7700 time: 0.0124s\n",
      "Epoch: 0111 loss_train: 0.7465 acc_train: 0.8143 loss_val: 0.9329 acc_val: 0.7800 time: 0.0109s\n",
      "Epoch: 0112 loss_train: 0.7206 acc_train: 0.8286 loss_val: 0.9281 acc_val: 0.7833 time: 0.0090s\n",
      "Epoch: 0113 loss_train: 0.7281 acc_train: 0.8214 loss_val: 0.9254 acc_val: 0.7833 time: 0.0080s\n",
      "Epoch: 0114 loss_train: 0.7458 acc_train: 0.8357 loss_val: 0.9229 acc_val: 0.7733 time: 0.0090s\n",
      "Epoch: 0115 loss_train: 0.6883 acc_train: 0.8571 loss_val: 0.9195 acc_val: 0.7733 time: 0.0103s\n",
      "Epoch: 0116 loss_train: 0.6321 acc_train: 0.8643 loss_val: 0.9142 acc_val: 0.7767 time: 0.0114s\n",
      "Epoch: 0117 loss_train: 0.6760 acc_train: 0.8571 loss_val: 0.9081 acc_val: 0.7733 time: 0.0118s\n",
      "Epoch: 0118 loss_train: 0.6373 acc_train: 0.8786 loss_val: 0.9025 acc_val: 0.7767 time: 0.0104s\n",
      "Epoch: 0119 loss_train: 0.6624 acc_train: 0.8714 loss_val: 0.8958 acc_val: 0.7800 time: 0.0090s\n",
      "Epoch: 0120 loss_train: 0.6750 acc_train: 0.8500 loss_val: 0.8888 acc_val: 0.7867 time: 0.0090s\n",
      "Epoch: 0121 loss_train: 0.6802 acc_train: 0.8357 loss_val: 0.8824 acc_val: 0.7900 time: 0.0110s\n",
      "Epoch: 0122 loss_train: 0.6538 acc_train: 0.8643 loss_val: 0.8766 acc_val: 0.7933 time: 0.0110s\n",
      "Epoch: 0123 loss_train: 0.6663 acc_train: 0.8714 loss_val: 0.8709 acc_val: 0.7933 time: 0.0130s\n",
      "Epoch: 0124 loss_train: 0.6796 acc_train: 0.8357 loss_val: 0.8666 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0125 loss_train: 0.6332 acc_train: 0.8714 loss_val: 0.8635 acc_val: 0.8033 time: 0.0076s\n",
      "Epoch: 0126 loss_train: 0.6467 acc_train: 0.8714 loss_val: 0.8613 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0127 loss_train: 0.5726 acc_train: 0.9214 loss_val: 0.8590 acc_val: 0.8033 time: 0.0108s\n",
      "Epoch: 0128 loss_train: 0.6545 acc_train: 0.8714 loss_val: 0.8549 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0129 loss_train: 0.6469 acc_train: 0.8786 loss_val: 0.8493 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0130 loss_train: 0.6354 acc_train: 0.8786 loss_val: 0.8436 acc_val: 0.8100 time: 0.0090s\n",
      "Epoch: 0131 loss_train: 0.6064 acc_train: 0.9143 loss_val: 0.8385 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0132 loss_train: 0.6304 acc_train: 0.8786 loss_val: 0.8344 acc_val: 0.8200 time: 0.0088s\n",
      "Epoch: 0133 loss_train: 0.5802 acc_train: 0.9286 loss_val: 0.8306 acc_val: 0.8200 time: 0.0109s\n",
      "Epoch: 0134 loss_train: 0.5804 acc_train: 0.8786 loss_val: 0.8280 acc_val: 0.8167 time: 0.0090s\n",
      "Epoch: 0135 loss_train: 0.6203 acc_train: 0.8714 loss_val: 0.8256 acc_val: 0.8167 time: 0.0095s\n",
      "Epoch: 0136 loss_train: 0.5888 acc_train: 0.8929 loss_val: 0.8235 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0137 loss_train: 0.5411 acc_train: 0.9000 loss_val: 0.8216 acc_val: 0.8000 time: 0.0087s\n",
      "Epoch: 0138 loss_train: 0.5843 acc_train: 0.8714 loss_val: 0.8188 acc_val: 0.7967 time: 0.0106s\n",
      "Epoch: 0139 loss_train: 0.5277 acc_train: 0.9214 loss_val: 0.8157 acc_val: 0.7967 time: 0.0081s\n",
      "Epoch: 0140 loss_train: 0.5731 acc_train: 0.8786 loss_val: 0.8124 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0141 loss_train: 0.5487 acc_train: 0.9143 loss_val: 0.8104 acc_val: 0.8033 time: 0.0091s\n",
      "Epoch: 0142 loss_train: 0.5482 acc_train: 0.8643 loss_val: 0.8083 acc_val: 0.8100 time: 0.0069s\n",
      "Epoch: 0143 loss_train: 0.5773 acc_train: 0.8929 loss_val: 0.8052 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0144 loss_train: 0.5885 acc_train: 0.8714 loss_val: 0.8022 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0145 loss_train: 0.5555 acc_train: 0.9286 loss_val: 0.7989 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0146 loss_train: 0.5609 acc_train: 0.8929 loss_val: 0.7962 acc_val: 0.8133 time: 0.0080s\n",
      "Epoch: 0147 loss_train: 0.5043 acc_train: 0.9071 loss_val: 0.7928 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0148 loss_train: 0.5122 acc_train: 0.9286 loss_val: 0.7894 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0149 loss_train: 0.5725 acc_train: 0.9143 loss_val: 0.7860 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0150 loss_train: 0.5660 acc_train: 0.9286 loss_val: 0.7832 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0151 loss_train: 0.5314 acc_train: 0.9214 loss_val: 0.7810 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0152 loss_train: 0.5515 acc_train: 0.8929 loss_val: 0.7799 acc_val: 0.8133 time: 0.0060s\n",
      "Epoch: 0153 loss_train: 0.5033 acc_train: 0.9071 loss_val: 0.7795 acc_val: 0.8067 time: 0.0080s\n",
      "Epoch: 0154 loss_train: 0.5034 acc_train: 0.9214 loss_val: 0.7786 acc_val: 0.8033 time: 0.0060s\n",
      "Epoch: 0155 loss_train: 0.4972 acc_train: 0.9214 loss_val: 0.7769 acc_val: 0.8033 time: 0.0091s\n",
      "Epoch: 0156 loss_train: 0.4829 acc_train: 0.9143 loss_val: 0.7745 acc_val: 0.8100 time: 0.0069s\n",
      "Epoch: 0157 loss_train: 0.5007 acc_train: 0.9357 loss_val: 0.7724 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0158 loss_train: 0.4881 acc_train: 0.9286 loss_val: 0.7699 acc_val: 0.8167 time: 0.0070s\n",
      "Epoch: 0159 loss_train: 0.5390 acc_train: 0.9000 loss_val: 0.7660 acc_val: 0.8167 time: 0.0062s\n",
      "Epoch: 0160 loss_train: 0.5242 acc_train: 0.9143 loss_val: 0.7624 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0161 loss_train: 0.4966 acc_train: 0.9071 loss_val: 0.7598 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0162 loss_train: 0.4624 acc_train: 0.9286 loss_val: 0.7583 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0163 loss_train: 0.4956 acc_train: 0.9071 loss_val: 0.7578 acc_val: 0.8200 time: 0.0060s\n",
      "Epoch: 0164 loss_train: 0.4999 acc_train: 0.9286 loss_val: 0.7561 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0165 loss_train: 0.4804 acc_train: 0.9429 loss_val: 0.7539 acc_val: 0.8167 time: 0.0060s\n",
      "Epoch: 0166 loss_train: 0.4471 acc_train: 0.9357 loss_val: 0.7509 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0167 loss_train: 0.4566 acc_train: 0.9357 loss_val: 0.7472 acc_val: 0.8167 time: 0.0080s\n",
      "Epoch: 0168 loss_train: 0.4981 acc_train: 0.9286 loss_val: 0.7440 acc_val: 0.8200 time: 0.0080s\n",
      "Epoch: 0169 loss_train: 0.4775 acc_train: 0.9286 loss_val: 0.7412 acc_val: 0.8200 time: 0.0070s\n",
      "Epoch: 0170 loss_train: 0.4826 acc_train: 0.9286 loss_val: 0.7395 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0171 loss_train: 0.4609 acc_train: 0.9143 loss_val: 0.7391 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0172 loss_train: 0.4960 acc_train: 0.9500 loss_val: 0.7389 acc_val: 0.8267 time: 0.0060s\n",
      "Epoch: 0173 loss_train: 0.5006 acc_train: 0.9143 loss_val: 0.7389 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0174 loss_train: 0.4547 acc_train: 0.9143 loss_val: 0.7397 acc_val: 0.8100 time: 0.0115s\n",
      "Epoch: 0175 loss_train: 0.4512 acc_train: 0.9357 loss_val: 0.7411 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0176 loss_train: 0.4507 acc_train: 0.9429 loss_val: 0.7431 acc_val: 0.8100 time: 0.0080s\n",
      "Epoch: 0177 loss_train: 0.4711 acc_train: 0.9357 loss_val: 0.7450 acc_val: 0.8067 time: 0.0070s\n",
      "Epoch: 0178 loss_train: 0.4676 acc_train: 0.9214 loss_val: 0.7460 acc_val: 0.8000 time: 0.0072s\n",
      "Epoch: 0179 loss_train: 0.4972 acc_train: 0.8929 loss_val: 0.7438 acc_val: 0.8067 time: 0.0081s\n",
      "Epoch: 0180 loss_train: 0.4480 acc_train: 0.9357 loss_val: 0.7390 acc_val: 0.8033 time: 0.0079s\n",
      "Epoch: 0181 loss_train: 0.4483 acc_train: 0.9500 loss_val: 0.7344 acc_val: 0.8067 time: 0.0072s\n",
      "Epoch: 0182 loss_train: 0.4454 acc_train: 0.9643 loss_val: 0.7304 acc_val: 0.8067 time: 0.0072s\n",
      "Epoch: 0183 loss_train: 0.4028 acc_train: 0.9571 loss_val: 0.7263 acc_val: 0.8267 time: 0.0070s\n",
      "Epoch: 0184 loss_train: 0.4650 acc_train: 0.9286 loss_val: 0.7231 acc_val: 0.8267 time: 0.0077s\n",
      "Epoch: 0185 loss_train: 0.4717 acc_train: 0.9000 loss_val: 0.7208 acc_val: 0.8267 time: 0.0085s\n",
      "Epoch: 0186 loss_train: 0.4302 acc_train: 0.9214 loss_val: 0.7194 acc_val: 0.8233 time: 0.0072s\n",
      "Epoch: 0187 loss_train: 0.4398 acc_train: 0.9286 loss_val: 0.7196 acc_val: 0.8200 time: 0.0078s\n",
      "Epoch: 0188 loss_train: 0.4362 acc_train: 0.9500 loss_val: 0.7220 acc_val: 0.8133 time: 0.0083s\n",
      "Epoch: 0189 loss_train: 0.4556 acc_train: 0.9500 loss_val: 0.7257 acc_val: 0.8100 time: 0.0071s\n",
      "Epoch: 0190 loss_train: 0.4200 acc_train: 0.9714 loss_val: 0.7279 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0191 loss_train: 0.4108 acc_train: 0.9571 loss_val: 0.7267 acc_val: 0.8000 time: 0.0071s\n",
      "Epoch: 0192 loss_train: 0.4335 acc_train: 0.9286 loss_val: 0.7249 acc_val: 0.7933 time: 0.0076s\n",
      "Epoch: 0193 loss_train: 0.4304 acc_train: 0.9500 loss_val: 0.7216 acc_val: 0.8000 time: 0.0070s\n",
      "Epoch: 0194 loss_train: 0.4298 acc_train: 0.9500 loss_val: 0.7179 acc_val: 0.8000 time: 0.0080s\n",
      "Epoch: 0195 loss_train: 0.4074 acc_train: 0.9500 loss_val: 0.7130 acc_val: 0.8067 time: 0.0093s\n",
      "Epoch: 0196 loss_train: 0.3957 acc_train: 0.9286 loss_val: 0.7107 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0197 loss_train: 0.4360 acc_train: 0.9500 loss_val: 0.7069 acc_val: 0.8133 time: 0.0099s\n",
      "Epoch: 0198 loss_train: 0.3862 acc_train: 0.9429 loss_val: 0.7038 acc_val: 0.8133 time: 0.0082s\n",
      "Epoch: 0199 loss_train: 0.4234 acc_train: 0.9286 loss_val: 0.7029 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0200 loss_train: 0.4164 acc_train: 0.9286 loss_val: 0.7015 acc_val: 0.8133 time: 0.0099s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 4.5762s\n",
      "Test set results: loss= 0.7341 accuracy= 0.8270\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a1f56-ab81-4aff-85d5-148c6f2a303f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79605ccf-13cc-4ad3-849f-c0263262bd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7378f8f2-9eb8-45a3-b4b6-f8c6b6cec84e",
   "metadata": {},
   "source": [
    "## 2. https://pytorch-geometric.readthedocs.io/en/1.3.0/notes/create_gnn.html\n",
    "\n",
    "Slight different variation in latest doc\n",
    "#### https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d947f963-c25e-4ccd-9f6f-6e2314430250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c610bd-ef8c-43b5-afc1-3a80b5d38336",
   "metadata": {},
   "source": [
    "#### Aproach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bb4617f-824e-4f2b-93a6-952d5bc71250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        # Step 6: Apply a final bias vector.\n",
    "        out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d6ed4-2252-4595-8c73-e62809aa9d7b",
   "metadata": {},
   "source": [
    "#### Aproach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cb6a61a-17be-4b85-ab27-38b78cbd43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3-5: Start propagating messages.\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    def message(self, x_j, edge_index, size):\n",
    "        print(\"Called message\")\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 3: Normalize node features.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22e71355-779e-42bb-9c02-87364b6886f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2708, 1433]), torch.Size([2, 10556]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = dataset[0].x\n",
    "edge_index = dataset[0].edge_index\n",
    "x.size(), edge_index.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92bb8982-defe-4a22-a2cb-78c7f33a2c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 7])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = GCNConv(1433, 7)\n",
    "x = conv(x, edge_index)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aca398ad-6f4e-467d-91ca-ab4b2586dcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2708, 1433]), torch.Size([2, 10556]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((2708, 1433))\n",
    "edge_index = torch.randint(0, 1433,(2, 10556))\n",
    "x.size(), edge_index.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e9f52c4-92a9-4564-9662-721f068c7684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 7])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = GCNConv(1433, 7)\n",
    "x = conv(x, edge_index)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de7284-2b5b-4be2-ba0b-2d900190a641",
   "metadata": {},
   "source": [
    "## 3. https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21767bcb-adca-45f6-83e9-29e18a64834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
    "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. \n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1663400d-98c8-4426-aa44-b601fd9a89f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjacency matrix:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])\n",
    "\n",
    "print(\"Node features:\\n\", node_feats)\n",
    "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36003f4b-df54-4b79-9e05-c1891a2502c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.],\n",
       "         [4.],\n",
       "         [3.],\n",
       "         [3.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "num_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7523b10-157f-4895-9509-ea04c0f97b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444bfca1-4cbd-481f-862b-0959a86e1acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Optimization-Python",
   "language": "python",
   "name": "optimization-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
