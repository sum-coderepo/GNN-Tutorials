{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4414cc84-5960-4cd1-880d-8b4bc07eb1a2",
   "metadata": {},
   "source": [
    "## 1. https://github.com/joshchang1112/bert_gnn_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747f3663-d132-43c5-a5dd-67b213a80106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version:  1.13.1\n",
      "GPU 0 is available!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from ogb.nodeproppred.dataset_pyg import PygNodePropPredDataset\n",
    "from ogb.nodeproppred import Evaluator\n",
    "#from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(\"Pytorch Version: \",  torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "  print(\"GPU {} is available!\".format(torch.cuda.current_device()))\n",
    "else:\n",
    "  print(\"Only CPU is available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c468e-a5ed-4166-8d3f-b981ed9d94ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {    \n",
    "    \"encoder\": \"bert\",\n",
    "    \"node2paper\": \"dataset/ogbn_arxiv/mapping/node2paper.pkl\",\n",
    "    \"paper2node\": \"dataset/ogbn_arxiv/mapping/paper2node.pkl\",\n",
    "    \"raw_text_path\": \"dataset/ogbn_arxiv/raw/titleabs.tsv\",\n",
    "    \"train\": \"dataset/ogbn_arxiv/bert/train.pkl\",\n",
    "    \"valid\": \"dataset/ogbn_arxiv/bert/valid.pkl\",\n",
    "    \"test\": \"dataset/ogbn_arxiv/bert/test.pkl\",\n",
    "    \"bert_models\": \"models/fine-tuned_bert_{}.pkl\",\n",
    "    \"node_features\": \"node_feat/bert_feat_{}.pkl\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1032f-b802-4635-b63e-c254183fe990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred.dataset_pyg import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "dataset = PygNodePropPredDataset(name='ogbn-arxiv', transform=T.ToSparseTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6eb64-9039-4f3e-b03b-1447c2907980",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ID_PATH = 'dataset/ogbn_arxiv/split/time/train.csv.gz'\n",
    "VALID_ID_PATH = 'dataset/ogbn_arxiv/split/time/valid.csv.gz'\n",
    "TEST_ID_PATH = 'dataset/ogbn_arxiv/split/time/test.csv.gz'\n",
    "LABEL_PATH = 'dataset/ogbn_arxiv/raw/node-label.csv.gz'\n",
    "NODE2PAPER_PATH = 'dataset/ogbn_arxiv/mapping/nodeidx2paperid.csv.gz'\n",
    "RAW_DATA_PATH = 'dataset/ogbn_arxiv/raw/titleabs.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654a0d1-01c7-4921-8ef3-929f4aa11283",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(RAW_DATA_PATH, sep='\\t', header=None)\n",
    "raw_data.columns = ['Id', 'Title', 'Abstract']\n",
    "raw_data.iloc[0, 0] = 200971\n",
    "raw_data = raw_data.drop(len(raw_data)-1)\n",
    "\n",
    "node2paper = pd.read_csv(NODE2PAPER_PATH)\n",
    "train_idx = pd.read_csv(TRAIN_ID_PATH, header=None)\n",
    "val_idx = pd.read_csv(VALID_ID_PATH, header=None)\n",
    "test_idx = pd.read_csv(TEST_ID_PATH, header=None)\n",
    "label = pd.read_csv(LABEL_PATH, header=None)\n",
    "\n",
    "train_idx = train_idx.iloc[:, 0].tolist()\n",
    "val_idx = val_idx.iloc[:, 0].tolist()\n",
    "test_idx = test_idx.iloc[:, 0].tolist()\n",
    "label = label.iloc[:, 0].tolist()\n",
    "\n",
    "paper2node_dict = {}\n",
    "node2paper_dict = {}\n",
    "\n",
    "for i, row in tqdm(node2paper.iterrows()):\n",
    "    paper2node_dict[int(row[1])] = int(row[0])\n",
    "    node2paper_dict[int(row[0])] = int(row[1])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ddd426-f46a-4e1e-aab6-075aae3b989e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "for i, row in tqdm(raw_data.iterrows()):\n",
    "    if int(row['Id']) not in paper2node_dict:\n",
    "        continue\n",
    "    processed = {}\n",
    "    processed['context'] = tokenizer.tokenize(text=row['Title']+row['Abstract'])\n",
    "    processed['context'] = tokenizer.convert_tokens_to_ids(processed['context'])\n",
    "    processed['length'] = len(processed['context'])\n",
    "    processed['id'] = paper2node_dict[int(row['Id'])]\n",
    "    processed['label'] = label[int(paper2node_dict[int(row['Id'])])]\n",
    "    \n",
    "    if processed['id'] in train_idx:\n",
    "        train.append(processed)\n",
    "    elif processed['id'] in val_idx:\n",
    "        val.append(processed)\n",
    "    elif processed['id'] in test_idx:\n",
    "        test.append(processed)\n",
    "    else:\n",
    "        print(\"NOT MATCH!!!!!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955234b-db93-4e94-a0a6-688d8a9e6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "  \"\"\"Hyperparameters used for training BERT.\"\"\"\n",
    "  def __init__(self):\n",
    "    ### dataset parameters\n",
    "    self.num_classes = 40\n",
    "    self.max_seq_length = 500\n",
    "    ### training parameters\n",
    "    self.train_epochs = 2\n",
    "    self.batch_size = 8\n",
    "    self.learning_rate = 2e-5\n",
    "    self.dropout_rate = 0.5\n",
    "    ### eval parameters\n",
    "    self.eval_steps = 4000\n",
    "\n",
    "args = Args()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05fb34-5555-4f7d-97d9-417ca40e6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def pad_to_len(arr, padded_len, padding=0):\n",
    "    length_arr = len(arr)\n",
    "    new_arr = arr\n",
    "    if length_arr < padded_len:\n",
    "        for i in range(padded_len - length_arr):\n",
    "            new_arr.append(padding)\n",
    "    else:\n",
    "        for i in range(length_arr - padded_len):\n",
    "            del new_arr[-2]\n",
    "    return new_arr\n",
    "\n",
    "class CitationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, max_seq_len, padding=0):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = dict(self.data[index])\n",
    "        if len(data['context']) > self.max_seq_len:\n",
    "            data['context'] = data['context'][:self.max_seq_len]\n",
    "        return data\n",
    "\n",
    "    def collate_fn(self, datas):\n",
    "        batch = {}\n",
    "        batch['length'] = torch.LongTensor([data['length'] for data in datas])\n",
    "        padded_len = min(self.max_seq_len, max(batch['length']))\n",
    "        batch['context'] = torch.tensor(\n",
    "            [pad_to_len(data['context'], padded_len, self.padding)\n",
    "             for data in datas]\n",
    "        )\n",
    "        batch['label'] = torch.LongTensor([data['label'] for data in datas])\n",
    "        return batch\n",
    "\n",
    "train_dataset = CitationDataset(train, max_seq_len=args.max_seq_length)\n",
    "valid_dataset = CitationDataset(val, max_seq_len=args.max_seq_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, \n",
    "    collate_fn=train_dataset.collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "    collate_fn=valid_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a8821-3d1a-4097-9b6f-594aa0d5ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.name = 'Metric Name'\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def update(self, predicts, batch):\n",
    "        pass\n",
    "\n",
    "    def get_score(self):\n",
    "        pass\n",
    "\n",
    "class Accuracy(Metrics):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "         ats (int): @ to eval.\n",
    "         rank_na (bool): whether to consider no answer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.name = 'Accuracy'\n",
    "        self.match = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.n = 0\n",
    "        self.match = 0\n",
    "        \n",
    "    def update(self, predicts, label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predicts (FloatTensor): with size (batch, n_samples).\n",
    "            batch (dict): batch.\n",
    "        \"\"\"\n",
    "        predicts, label = predicts.cpu(), label.cpu()\n",
    "        batch_size = list(predicts.size())[0]\n",
    "        _, y_pred = torch.max(predicts, dim=1)\n",
    "        self.match += accuracy_score(label, y_pred, normalize=False)\n",
    "        self.n += batch_size\n",
    "    \n",
    "    def print_score(self):\n",
    "        acc = self.match / self.n\n",
    "        #self.get_category_f1()\n",
    "        return '{:.4f}'.format(acc)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f5a31-2036-4298-8a90-07768c858889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_iter(batch, model, device, training):\n",
    "    context, context_lens = batch['context'].to(device), batch['length'].to(device)\n",
    "    batch_size = context.size()[0]\n",
    "    max_context_len = context.size()[1]\n",
    "    padding_mask = []\n",
    "    for j in range(batch_size):\n",
    "        if context_lens[j] < max_context_len:\n",
    "            tmp = [1] * context_lens[j] + [0] * (max_context_len - context_lens[j])\n",
    "        else:\n",
    "            tmp = [1] * max_context_len\n",
    "        padding_mask.append(tmp)\n",
    "\n",
    "    padding_mask = torch.Tensor(padding_mask).to(device)\n",
    "    if training:\n",
    "        prob = model(context, attention_mask=padding_mask)[0]\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            prob = model(context, attention_mask=padding_mask)[0]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3178c-c284-4fe2-b887-aecb8a1bf413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_loader, valid_loader, model, optimizer, epochs, eval_steps, device):\n",
    "    train_metrics = Accuracy()\n",
    "    best_valid_acc = 0\n",
    "    total_iter = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        train_trange = tqdm(enumerate(train_loader), total=len(train_loader), desc='training')\n",
    "        train_loss = 0\n",
    "        train_metrics.reset()\n",
    "        for i, batch in train_trange:\n",
    "            model.train()\n",
    "            prob = run_iter(batch, model, device, training=True)\n",
    "            answer = batch['label'].to(device)\n",
    "            loss = criterion(prob, answer)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_iter += 1\n",
    "            train_loss += loss.item()\n",
    "            train_metrics.update(prob, answer)\n",
    "            train_trange.set_postfix(loss= train_loss/(i+1),\n",
    "                                     **{train_metrics.name: train_metrics.print_score()})\n",
    "            \n",
    "            if total_iter % eval_steps == 0:\n",
    "                valid_acc = testing(valid_loader, model, device, valid=True)\n",
    "                if valid_acc > best_valid_acc:\n",
    "                    best_valid_acc = valid_acc\n",
    "                    torch.save(model, 'best_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158944c-d6b8-4b42-aa31-8e2cda595b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(dataloader, model, device, valid):\n",
    "    metrics = Accuracy()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='validation' if valid else 'testing')\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    metrics.reset()\n",
    "    for k, batch in trange:\n",
    "        model.eval()\n",
    "        prob = run_iter(batch, model, device, training=False)\n",
    "        answer = batch['label'].to(device)\n",
    "        loss = criterion(prob, batch['label'].to(device))\n",
    "        total_loss += loss.item()\n",
    "        metrics.update(prob, answer)\n",
    "        trange.set_postfix(loss= total_loss/(k+1),\n",
    "                           **{metrics.name: metrics.print_score()})\n",
    "    acc = metrics.match / metrics.n\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011395b-927b-493d-a849-0a1cc90a41d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim import Adam\n",
    "device = torch.device('cuda:{}'.format(torch.cuda.current_device()) \n",
    "                       if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                      num_labels=args.num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=args.learning_rate)\n",
    "training(train_loader, valid_loader, model, optimizer, args.train_epochs, args.eval_steps, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b90ee-3261-4d77-89a6-0ba4e48992ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089987f0-a4c7-4938-8038-c81bc4d9a2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554e08e-d881-4cfa-aa52-856e3150ac80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aed2e2-3e2e-4e80-9d8d-c8dbd7f35e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ogb\n",
    "# pip install transformers\n",
    "#Utils\n",
    "\n",
    "\n",
    "def set_seed(SEED=0):\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_data(config_path):\n",
    "    \"\"\" Load dataset preprocess by tokenizer.\"\"\"\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    with open(config['train'], 'rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(config['valid'], 'rb') as f:\n",
    "        valid = pickle.load(f)\n",
    "    with open(config['test'], 'rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    return train, valid, test\n",
    "\n",
    "def pad_to_len(arr, padded_len, padding=0):\n",
    "    length_arr = len(arr)\n",
    "    new_arr = arr\n",
    "    if length_arr < padded_len:\n",
    "        for i in range(padded_len - length_arr):\n",
    "            new_arr.append(padding)\n",
    "    else:\n",
    "        for i in range(length_arr - padded_len):\n",
    "            del new_arr[-2]\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57c7f6-6a7b-455e-ab40-9176bbefb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CitationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, max_length, padding=0):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_length\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = dict(self.data[index])\n",
    "        if len(data['context']) > self.max_seq_len:\n",
    "            data['context'] = data['context'][:self.max_seq_len]\n",
    "        return data\n",
    "\n",
    "    def collate_fn(self, datas):\n",
    "        batch = {}\n",
    "        batch['length'] = torch.LongTensor([data['length'] for data in datas])\n",
    "        padded_len = min(self.max_seq_len, max(batch['length']))\n",
    "        batch['context'] = torch.tensor(\n",
    "            [pad_to_len(data['context'], padded_len, self.padding)\n",
    "             for data in datas]\n",
    "        )\n",
    "        batch['label'] = torch.LongTensor([data['label'] for data in datas])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d24fc-113f-434d-89f7-b8cc94437eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make_bert_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def tokenize(data, paper2node, idx, label):\n",
    "    \"\"\"Tokenize and convert word token to ids\"\"\"\n",
    "    train, valid, test = [], [], []\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    for i, row in tqdm(data.iterrows()):\n",
    "        if int(row['Id']) not in paper2node:\n",
    "            continue\n",
    "        processed = {}\n",
    "        processed['context'] = tokenizer.tokenize(text=\"[CLS] \" + row['Title']+row['Abstract'] + \" [SEP]\")\n",
    "        processed['context'] = tokenizer.convert_tokens_to_ids(processed['context'])\n",
    "        processed['length'] = len(processed['context'])\n",
    "        processed['id'] = paper2node[int(row['Id'])]\n",
    "        processed['label'] = label[int(paper2node[int(row['Id'])])]\n",
    "    \n",
    "        if processed['id'] in idx['train']:\n",
    "            train.append(processed)\n",
    "        elif processed['id'] in idx['valid']:\n",
    "            valid.append(processed)\n",
    "        elif processed['id'] in idx['test']:\n",
    "            test.append(processed)\n",
    "        else:\n",
    "            print(\"NOT MATCH!!!!!\")\n",
    "            break\n",
    "\n",
    "    return train, valid, test\n",
    "\n",
    "def main():\n",
    "\n",
    "    #with open('config.json') as f:\n",
    "    #    config = json.load(f)\n",
    "    if os.path.isdir(os.path.join('dataset/ogbn_arxiv', config['encoder'])) == False:\n",
    "        os.makedirs(os.path.join('dataset/ogbn_arxiv', config['encoder']))\n",
    "        print('Create folder: dataset/ogbn_arxiv/{}'.format(config['encoder']))\n",
    "    else:\n",
    "        print('dataset/ogbn_arxiv/{} exists!'.format(config['encoder']))\n",
    "    \n",
    "    # Load raw ogbn-arxiv data\n",
    "    raw_data = pd.read_csv(config['raw_text_path'], sep='\\t')\n",
    "    node2paper = pd.read_csv('dataset/ogbn_arxiv/mapping/nodeidx2paperid.csv.gz')\n",
    "    train_idx = pd.read_csv('dataset/ogbn_arxiv/split/time/train.csv.gz', header=None)\n",
    "    valid_idx = pd.read_csv('dataset/ogbn_arxiv/split/time/valid.csv.gz', header=None)\n",
    "    test_idx = pd.read_csv('dataset/ogbn_arxiv/split/time/test.csv.gz', header=None)\n",
    "    label = pd.read_csv('dataset/ogbn_arxiv/raw/node-label.csv.gz', header=None)\n",
    "    \n",
    "    # Preprocess & modify csv error\n",
    "    raw_data.columns = ['Id', 'Title', 'Abstract']\n",
    "    raw_data.iloc[0, 0] = 200971\n",
    "    raw_data = raw_data.drop(len(raw_data)-1)\n",
    "\n",
    "    train_idx = train_idx.iloc[:, 0].tolist()\n",
    "    valid_idx = valid_idx.iloc[:, 0].tolist()\n",
    "    test_idx = test_idx.iloc[:, 0].tolist()\n",
    "    idx = {'train': train_idx, 'valid': valid_idx, 'test': test_idx}\n",
    "    label = label.iloc[:, 0].tolist()\n",
    "\n",
    "    # Create node_id->paper_id dict, paper_id->node_id dict\n",
    "    paper2node_dict = {}\n",
    "    node2paper_dict = {}\n",
    "    for i, row in tqdm(node2paper.iterrows()):\n",
    "        paper2node_dict[int(row[1])] = int(row[0])\n",
    "        node2paper_dict[int(row[0])] = int(row[1])\n",
    "    \n",
    "    train, valid, test = tokenize(raw_data, paper2node_dict, idx, label)\n",
    "\n",
    "    with open(config['train'], 'wb') as f:\n",
    "        pickle.dump(train, f)\n",
    "    with open(config['valid'], 'wb') as f:\n",
    "        pickle.dump(valid, f)\n",
    "    with open(config['test'], 'wb') as f:\n",
    "        pickle.dump(test, f)\n",
    "    with open(config['paper2node'], 'wb') as f:\n",
    "        pickle.dump(paper2node_dict, f)\n",
    "    with open(config['node2paper'], 'wb') as f:\n",
    "        pickle.dump(node2paper_dict, f)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bdf69f-d524-4c93-98af-94bf9d7c507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encode Features\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def freeze_bert_layers(model):\n",
    "    \"\"\"Freeze all bert layers to release GPU memory\"\"\"\n",
    "    freeze_layers = 12\n",
    "    for p in model.bert.embeddings.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.bert.embeddings.dropout.p = 0.0\n",
    "    for p in model.bert.pooler.parameters():\n",
    "        p.requires_grad = False\n",
    "    for idx in range(freeze_layers):\n",
    "        for p in model.bert.encoder.layer[idx].parameters():\n",
    "            p.requires_grad = False\n",
    "        model.bert.encoder.layer[idx].attention.self.dropout.p = 0.0\n",
    "        model.bert.encoder.layer[idx].attention.output.dropout.p = 0.0\n",
    "        model.bert.encoder.layer[idx].output.dropout.p = 0.0\n",
    "    return model\n",
    "\n",
    "def encode_features(data, data_len, paper2node_dict, model):\n",
    "    node_feats = torch.zeros((data_len, 768)).cuda()\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = freeze_bert_layers(model)\n",
    "    for i, row in tqdm(data.iterrows()):\n",
    "        if row['Id'] not in paper2node_dict:\n",
    "            continue\n",
    "        context = \"[CLS] \" + row['Title'] + row['Abstract'] + \" [SEP]\"\n",
    "        tokenize_context = tokenizer.tokenize(context)\n",
    "        context_len = len(tokenize_context)\n",
    "\n",
    "        if context_len > 512:\n",
    "            tokenize_context = tokenize_context[:512]\n",
    "    \n",
    "        context_id = tokenizer.convert_tokens_to_ids(tokenize_context)\n",
    "        context_id = torch.LongTensor(context_id).unsqueeze(0).cuda()\n",
    "        feat = model.bert(context_id)[0].squeeze(0)[0]\n",
    "        node_id = paper2node_dict[row['Id']]\n",
    "        node_feats[node_id, :] = feat\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return node_feats\n",
    "    \n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Encode Node Features')\n",
    "    parser.add_argument('--device', type=int, default=0)\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--node_feat_dir', type=str, default='node_feat',\n",
    "                        help='Directory to the fine-tuned node features.')\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    device = torch.device('cuda:{}'.format(args.device) if torch.cuda.is_available()\n",
    "                          else 'cpu')\n",
    "\n",
    "    with open('config.json') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load data & model\n",
    "    MODEL_PATH = config['bert_models'].format(args.seed)\n",
    "    data = pd.read_csv(config['raw_text_path'], sep='\\t')\n",
    "    with open(config['node2paper'], 'rb') as f:\n",
    "        node2paper_dict = pickle.load(f)\n",
    "    with open(config['paper2node'], 'rb') as f:\n",
    "        paper2node_dict = pickle.load(f)\n",
    "    \n",
    "    data.columns = ['Id', 'Title', 'Abstract']\n",
    "    data.iloc[0, 0] = 200971\n",
    "    data = data.drop(len(data)-1)\n",
    "    model = torch.load(MODEL_PATH).to(device)\n",
    "    \n",
    "    # Create or check directory\n",
    "    if os.path.isdir(args.node_feat_dir) == False:\n",
    "        os.makedirs(args.node_feat_dir)\n",
    "        print('Create folder: {}'.format(args.node_feat_dir))\n",
    "    else:\n",
    "        print('{} exists!'.format(args.node_feat_dir))\n",
    "\n",
    "    node_feats = encode_features(data, len(node2paper_dict),\n",
    "                                 paper2node_dict, model)\n",
    "\n",
    "    torch.save(node_feats, config['node_features'].format(args.seed))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e336a-ecab-4df9-953b-0aa108355bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9edc94-e601-470f-b268-360020fe5313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a722784-5bcb-4d7b-9c23-7870a3a580a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e1058b1-236f-4fe7-8c12-93a7e9425ca7",
   "metadata": {},
   "source": [
    "## 2. https://github.com/mpcrlab/MolecularTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b22bc1-81ea-4c95-8edd-e760c4ea19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "import argparse\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0ae193-eafd-4f49-9e01-a6dd9414ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINTABLE_ASCII_CHARS = 95\n",
    "\n",
    "_extra_chars = [\"seq_start\", \"seq_end\", \"pad\"]\n",
    "EXTRA_CHARS = {key: chr(PRINTABLE_ASCII_CHARS + i) for i, key in enumerate(_extra_chars)}\n",
    "ALPHABET_SIZE = PRINTABLE_ASCII_CHARS + len(EXTRA_CHARS)\n",
    "\n",
    "def encode_char(c):\n",
    "    return ord(c) - 32\n",
    "\n",
    "def decode_char(n):\n",
    "    return chr(n + 32)\n",
    "\n",
    "def smiles_iupac_batch(instances):\n",
    "    smiles_lens = torch.tensor([s[0].shape[0] + 1 for s in instances], dtype=torch.long)\n",
    "    iupac_lens = torch.tensor([s[1].shape[0] + 1 for s in instances], dtype=torch.long)\n",
    "    \n",
    "    max_len_smiles = smiles_lens.max().item()\n",
    "    max_len_iupac = iupac_lens.max().item()\n",
    "    \n",
    "    batch_smiles = torch.full((len(instances), max_len_smiles), ord(EXTRA_CHARS['pad']), dtype=torch.long)\n",
    "    batch_iupac_in = torch.full((len(instances), max_len_iupac), ord(EXTRA_CHARS['pad']), dtype=torch.long)\n",
    "    batch_iupac_out = torch.full((len(instances), max_len_iupac), ord(EXTRA_CHARS['pad']), dtype=torch.long)\n",
    "\n",
    "    for i, instance in enumerate(instances):\n",
    "        batch_smiles[i, 0] = ord(EXTRA_CHARS['seq_start'])\n",
    "        batch_smiles[i, 1:smiles_lens[i]] = instance[0]\n",
    "\n",
    "        batch_iupac_in[i, 0] = ord(EXTRA_CHARS['seq_start'])\n",
    "        batch_iupac_in[i, 1:iupac_lens[i]] = instance[1]\n",
    "\n",
    "        batch_iupac_out[i, iupac_lens[i]-1] = ord(EXTRA_CHARS['seq_end'])\n",
    "        batch_iupac_out[i, 0:iupac_lens[i]-1] = instance[1]\n",
    "    \n",
    "    return batch_smiles, batch_iupac_in, batch_iupac_out, smiles_lens, iupac_lens\n",
    "\n",
    "class SmilesIupacDataset(Dataset):\n",
    "    def __init__(self, data_path, max_len=None):\n",
    "        self.pairs = [line.strip(\"\\n\").split(\"\\t\") for line in open(data_path, \"r\")]\n",
    "        self.max_len = max_len - 1 if max_len else 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def string_to_tensor(self, string):\n",
    "        tensor = torch.tensor(list(map(encode_char, string)), dtype=torch.uint8)\n",
    "        \n",
    "        if self.max_len > 0:\n",
    "            tensor = tensor[:self.max_len]\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        smiles, iupac = self.pairs[index]\n",
    "        return self.string_to_tensor(smiles), self.string_to_tensor(iupac)\n",
    "\n",
    "def get_dataloader(batch_size, data_path, max_len=256):\n",
    "    dataset = SmilesIupacDataset(data_path, max_len=max_len)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=smiles_iupac_batch), dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca860398-d7c2-4135-9e30-65c815013c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "class OneHotEmbedding(nn.Module):\n",
    "    def __init__(self, alphabet_size):\n",
    "        super().__init__()\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.eye(alphabet_size))\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "    \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, alphabet_size, d_model):\n",
    "        super().__init__()\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(alphabet_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 6000, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        pe = self.pe[:,:seq_len]\n",
    "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = x + pe\n",
    "        #print(x.mean(), x)\n",
    "        x = self.dropout(x)\n",
    "        #x = F.dropout(x, p=0.1, training=self.training)\n",
    "        #print(x.mean(), x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into N heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * N * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "    \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, \\\n",
    "        src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, alphabet_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedding(alphabet_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedding(alphabet_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, alphabet_size, d_model, N, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(alphabet_size, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(alphabet_size, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, alphabet_size)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        #print(\"DECODER\")\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "def nopeak_mask(size, device):\n",
    "    np_mask = torch.triu(torch.ones((size, size), dtype=torch.uint8), diagonal=1).unsqueeze(0)\n",
    "    \n",
    "    np_mask = np_mask == 0\n",
    "    np_mask = np_mask.to(device)\n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg=None, pad_idx=ord(EXTRA_CHARS['pad']), device=None):\n",
    "    src_mask = (src != pad_idx).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != pad_idx).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, device)\n",
    "        np_mask.to(device)\n",
    "        trg_mask = trg_mask & np_mask\n",
    "        return src_mask, trg_mask\n",
    "    return src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b04c217-0c47-4aab-a63f-acd3b47987a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine annealing with restarts.\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer : torch.optim.Optimizer\n",
    "    T_max : int\n",
    "        The maximum number of iterations within the first cycle.\n",
    "    eta_min : float, optional (default: 0)\n",
    "        The minimum learning rate.\n",
    "    last_epoch : int, optional (default: -1)\n",
    "        The index of the last epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 optimizer,\n",
    "                 T_max,\n",
    "                 eta_min = 0.,\n",
    "                 last_epoch = -1,\n",
    "                 factor = 1.):\n",
    "        # pylint: disable=invalid-name\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.factor = factor\n",
    "        self._last_restart = 0\n",
    "        self._cycle_counter = 0\n",
    "        self._cycle_factor = 1.\n",
    "        self._updated_cycle_len = T_max\n",
    "        self._initialized = False\n",
    "        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Get updated learning rate.\"\"\"\n",
    "        # HACK: We need to check if this is the first time get_lr() was called, since\n",
    "        # we want to start with step = 0, but _LRScheduler calls get_lr with\n",
    "        # last_epoch + 1 when initialized.\n",
    "        if not self._initialized:\n",
    "            self._initialized = True\n",
    "            return self.base_lrs\n",
    "\n",
    "        step = self.last_epoch + 1\n",
    "        self._cycle_counter = step - self._last_restart\n",
    "\n",
    "        lrs = [\n",
    "            (\n",
    "                self.eta_min + ((lr - self.eta_min) / 2) *\n",
    "                (\n",
    "                    np.cos(\n",
    "                        np.pi *\n",
    "                        ((self._cycle_counter) % self._updated_cycle_len) /\n",
    "                        self._updated_cycle_len\n",
    "                    ) + 1\n",
    "                )\n",
    "            ) for lr in self.base_lrs\n",
    "        ]\n",
    "\n",
    "        if self._cycle_counter % self._updated_cycle_len == 0:\n",
    "            # Adjust the cycle length.\n",
    "            self._cycle_factor *= self.factor\n",
    "            self._cycle_counter = 0\n",
    "            self._updated_cycle_len = int(self._cycle_factor * self.T_max)\n",
    "            self._last_restart = step\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a22b286-d274-4e7e-a3a7-9a2f849163af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path='data/amino_acids.txt', checkpoint_path='checkpoints/pretrained.ckpt', max_length=256, embedding_size=512, num_layers=6)\n",
      "Loaded 20 SMILES strings from data/amino_acids.txt\n",
      "Initializing Transformer...\n",
      "Transformer Initialized.\n",
      "Loading pretrained weights from checkpoints/pretrained.ckpt\n",
      "Pretrained weights loaded\n",
      "embedded C(CC(C(=O)O)N)CN=C(N)N into (23, 512) matrix.\n",
      "embedded C1=C(NC=N1)CC(C(=O)O)N into (23, 512) matrix.\n",
      "embedded CCC(C)C(C(=O)O)N into (17, 512) matrix.\n",
      "embedded CC(C)CC(C(=O)O)N into (17, 512) matrix.\n",
      "embedded C(CCN)CC(C(=O)O)N into (18, 512) matrix.\n",
      "embedded CSCCC(C(=O)O)N into (15, 512) matrix.\n",
      "embedded C1=CC=C(C=C1)CC(C(=O)O)N into (25, 512) matrix.\n",
      "embedded CC(C(C(=O)O)N)O into (16, 512) matrix.\n",
      "embedded C1=CC=C2C(=C1)C(=CN2)CC(C(=O)O)N into (33, 512) matrix.\n",
      "embedded CC(C)C(C(=O)O)N into (16, 512) matrix.\n",
      "embedded CC(C(=O)O)N into (12, 512) matrix.\n",
      "embedded C(C(C(=O)O)N)C(=O)N into (20, 512) matrix.\n",
      "embedded C(C(C(=O)O)N)C(=O)O into (20, 512) matrix.\n",
      "embedded C(CC(=O)O)C(C(=O)O)N into (21, 512) matrix.\n",
      "embedded C(C(=O)O)N into (11, 512) matrix.\n",
      "embedded C(C(C(=O)O)N)O into (15, 512) matrix.\n",
      "embedded C1=CC(=CC=C1CC(C(=O)O)N)O into (26, 512) matrix.\n",
      "embedded C(C(C(=O)O)N)S into (15, 512) matrix.\n",
      "embedded C(CC(=O)N)C(C(=O)O)N into (21, 512) matrix.\n",
      "embedded C1CC(NC1)C(=O)O into (16, 512) matrix.\n",
      "All SMILES strings embedded. Saving...\n",
      "Saved embeddings to embeddings/amino_acids.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from transformer import Transformer, create_masks\n",
    "#from load_data import ALPHABET_SIZE, EXTRA_CHARS\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"data/amino_acids.txt\", help=\"Path to a text file with one SMILES string per line. These strings will be embedded.\")\n",
    "parser.add_argument(\"--checkpoint_path\", type=str, default=\"checkpoints/pretrained.ckpt\", help=\"Path to a binary file containing pretrained model weights.\")\n",
    "parser.add_argument(\"--max_length\", type=int, default=256, help=\"Strings in the data longer than this length will be truncated.\")\n",
    "parser.add_argument(\"--embedding_size\", type=int, default=512, help=\"Embedding size used in the pretrained Transformer.\")\n",
    "parser.add_argument(\"--num_layers\", type=int, default=6, help=\"Number of layers used in the Encoder and Decoder of the pretrained Transformer.\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "print(args)\n",
    "\n",
    "def encode_char(c):\n",
    "    return ord(c) - 32\n",
    "\n",
    "def encode_smiles(string, start_char=EXTRA_CHARS['seq_start']):\n",
    "    return torch.tensor([ord(start_char)] + [encode_char(c) for c in string], dtype=torch.long)[:args.max_length].unsqueeze(0)\n",
    "\n",
    "\n",
    "smiles_strings = [line.strip(\"\\n\") for line in open(args.data_path, \"r\")]\n",
    "print(\"Loaded {0} SMILES strings from {1}\".format(len(smiles_strings), args.data_path))\n",
    "\n",
    "print(\"Initializing Transformer...\")\n",
    "model = Transformer(ALPHABET_SIZE, args.embedding_size, args.num_layers).eval()\n",
    "model = torch.nn.DataParallel(model)\n",
    "print(\"Transformer Initialized.\")\n",
    "\n",
    "print(\"Loading pretrained weights from\", args.checkpoint_path)\n",
    "checkpoint = torch.load(args.checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"Pretrained weights loaded\")\n",
    "model = model.module.cpu()\n",
    "encoder = model.encoder.cpu()\n",
    "\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for smiles in smiles_strings:\n",
    "        encoded = encode_smiles(smiles)\n",
    "        mask = create_masks(encoded)\n",
    "        embedding = encoder(encoded, mask)[0].numpy()\n",
    "        embeddings.append(embedding)\n",
    "        print(\"embedded {0} into {1} matrix.\".format(smiles, str(embedding.shape)))\n",
    "        \n",
    "print(\"All SMILES strings embedded. Saving...\")\n",
    "filename = os.path.splitext(os.path.basename(args.data_path))[0]\n",
    "out_dir = \"embeddings/\"\n",
    "out_file = os.path.join(out_dir, filename + \".npz\")\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "out_dict = {smiles: matrix for smiles, matrix in zip(smiles_strings, embeddings)}\n",
    "np.savez(out_file, **out_dict)\n",
    "print(\"Saved embeddings to\", out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb0639-ce54-4626-a524-a7529be43a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"data/smiles_iupac_train_15k.tsv\", help=\"Path to a csv containing pairs of strings for training.\")\n",
    "parser.add_argument(\"--checkpoint_path\", type=str, default=None, help=\"Path to a binary file containing pretrained model weights. If not supplied, a random initialization will be used.\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=24, help=\"How many samples to average in each training step. If more than one GPU is available, samples will be split across devices.\")\n",
    "parser.add_argument(\"--learning_rate\", type=int, default=1e-4, help=\"Weight updates calculated during gradient descent will be multiplied by this factor before they are added to the weights.\")\n",
    "parser.add_argument(\"--max_length\", type=int, default=256, help=\"Strings in the data longer than this length will be truncated.\")\n",
    "parser.add_argument(\"--embedding_size\", type=int, default=512, help=\"Each SMILES string character will be embedded to a vector with this many elements.\")\n",
    "parser.add_argument(\"--num_layers\", type=int, default=6, help=\"The Encoder and Decoder modules of the Transformer network will each have this many sequential layers.\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=10, help=\"In each epoch, every training sample will be used once.\")\n",
    "parser.add_argument(\"--cpu\", action=\"store_true\", help=\"Set this flag to run only on the CPU (no cuda needed).\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "print(args)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if args.cpu:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"{0} GPUs available. Training with {1}.\".format(torch.cuda.device_count(), DEVICE))\n",
    "\n",
    "def print_progress(time, epoch, iters, loss):\n",
    "    print(str(time), \"minutes : epoch\", str(epoch), \": batch\", str(iters), \": loss =\", str(loss))\n",
    "    \n",
    "def save(epoch, model, optimizer):\n",
    "    checkpoint_name = \"checkpoints/epoch_{0}.ckpt\".format(epoch+1)\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "                }, checkpoint_name)\n",
    "    print(\"saved checkpoint at\", checkpoint_name)\n",
    "    \n",
    "def train_epoch(epoch, model, dataloader, optimizer, sched=None):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    print_every = max(1, int(len(dataloader) / 100.0))\n",
    "    \n",
    "    for i, (smiles, iupac_in, iupac_out, smiles_lens, iupac_lens) in enumerate(dataloader):\n",
    "        smiles = smiles.to(DEVICE)\n",
    "        iupac_in = iupac_in.to(DEVICE)\n",
    "        iupac_out = iupac_out.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        smiles_mask, iupac_mask = create_masks(smiles, iupac_in, device=DEVICE)\n",
    "        preds = model(smiles, iupac_in, smiles_mask, iupac_mask)\n",
    "        \n",
    "        loss = torch.nn.functional.cross_entropy(preds.view(-1, preds.size(-1)), iupac_out.view(-1), ignore_index=ord(EXTRA_CHARS['pad']))\n",
    "        #print(loss, preds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if sched:\n",
    "            sched.step()\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i+1) % print_every == 0:\n",
    "            avg_loss = total_loss / float(print_every)\n",
    "            print_progress((time.time() - start)//60, epoch+1, i+1, avg_loss)\n",
    "            total_loss = 0\n",
    "            \n",
    "        #if (i+1) % SAVE_ITERS == 0:\n",
    "        #    save(epoch, i+1, NAME, model, optimizer)\n",
    "       \n",
    "    avg_loss = total_loss / max(1, (i+1) % print_every)\n",
    "    print_progress((time.time() - start)//60, epoch+1, i+1, avg_loss)\n",
    "    save(epoch, model, optimizer)\n",
    "    \n",
    "    \n",
    "dataloader, dataset = get_dataloader(args.batch_size, args.data_path, max_len=args.max_length)\n",
    "\n",
    "print(\"Loaded {0} samples from {1}\".format(len(dataset), args.data_path))\n",
    "\n",
    "print(\"Initializing Transformer...\")\n",
    "model = Transformer(ALPHABET_SIZE, args.embedding_size, args.num_layers)\n",
    "if torch.cuda.is_available() and not args.cpu:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.to(DEVICE)\n",
    "print(\"Transformer Initialized on device(s):\", DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "sched = CosineWithRestarts(optimizer, T_max=len(dataloader))\n",
    "epoch = 0\n",
    "\n",
    "if args.checkpoint_path is not None:\n",
    "    print(\"Loading pretrained weights from\", args.checkpoint_path)\n",
    "    checkpoint = torch.load(args.checkpoint_path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    assert optimizer.param_groups[0]['lr'] == checkpoint['lr']\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    print(\"Pretrained weights loaded. Resuming training at epoch\", epoch)\n",
    "\n",
    "for i in range(epoch, epoch + args.num_epochs):\n",
    "    print(\"Starting epoch\", i+1)\n",
    "    train_epoch(i, model, dataloader, optimizer, sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173bfa3-59ba-46d7-9e00-328ef7e1f48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8920789-4744-4b4c-a26d-6f8885f92587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Optimization-Python",
   "language": "python",
   "name": "optimization-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
