{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30574533-a868-4ab8-a1ad-10f5523f4299",
   "metadata": {},
   "source": [
    "## https://github.com/tkipf/pygcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6151e5ea-99ab-43f7-ac05-8b582942adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "062899e5-ed3e-4d40-bf72-212626624eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9776234d-e661-4150-85ca-58af20d938ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "818710dc-7749-4bd5-afe2-3c90491a6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e37422d3-b791-4cc9-9cea-c619d79f537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0773262b-7381-4642-a190-9c6c06e6d9dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.0078 acc_train: 0.1000 loss_val: 2.0051 acc_val: 0.0833 time: 1.1933s\n",
      "Epoch: 0002 loss_train: 1.9876 acc_train: 0.1071 loss_val: 1.9917 acc_val: 0.0867 time: 0.0406s\n",
      "Epoch: 0003 loss_train: 1.9740 acc_train: 0.1143 loss_val: 1.9784 acc_val: 0.1133 time: 0.0454s\n",
      "Epoch: 0004 loss_train: 1.9651 acc_train: 0.1429 loss_val: 1.9653 acc_val: 0.1700 time: 0.0464s\n",
      "Epoch: 0005 loss_train: 1.9505 acc_train: 0.1429 loss_val: 1.9523 acc_val: 0.1900 time: 0.0296s\n",
      "Epoch: 0006 loss_train: 1.9355 acc_train: 0.2000 loss_val: 1.9396 acc_val: 0.2500 time: 0.0366s\n",
      "Epoch: 0007 loss_train: 1.9301 acc_train: 0.1071 loss_val: 1.9274 acc_val: 0.1567 time: 0.0384s\n",
      "Epoch: 0008 loss_train: 1.9099 acc_train: 0.2143 loss_val: 1.9153 acc_val: 0.1567 time: 0.0343s\n",
      "Epoch: 0009 loss_train: 1.9098 acc_train: 0.2071 loss_val: 1.9034 acc_val: 0.1567 time: 0.0289s\n",
      "Epoch: 0010 loss_train: 1.8873 acc_train: 0.2071 loss_val: 1.8918 acc_val: 0.1567 time: 0.0277s\n",
      "Epoch: 0011 loss_train: 1.8714 acc_train: 0.2000 loss_val: 1.8806 acc_val: 0.1567 time: 0.0278s\n",
      "Epoch: 0012 loss_train: 1.8644 acc_train: 0.2000 loss_val: 1.8696 acc_val: 0.1567 time: 0.0293s\n",
      "Epoch: 0013 loss_train: 1.8493 acc_train: 0.2143 loss_val: 1.8590 acc_val: 0.1567 time: 0.0307s\n",
      "Epoch: 0014 loss_train: 1.8383 acc_train: 0.2000 loss_val: 1.8487 acc_val: 0.1567 time: 0.0325s\n",
      "Epoch: 0015 loss_train: 1.8373 acc_train: 0.1929 loss_val: 1.8386 acc_val: 0.1567 time: 0.0290s\n",
      "Epoch: 0016 loss_train: 1.8405 acc_train: 0.1929 loss_val: 1.8290 acc_val: 0.1567 time: 0.0312s\n",
      "Epoch: 0017 loss_train: 1.8035 acc_train: 0.2071 loss_val: 1.8197 acc_val: 0.1567 time: 0.0293s\n",
      "Epoch: 0018 loss_train: 1.8037 acc_train: 0.1929 loss_val: 1.8102 acc_val: 0.1567 time: 0.0302s\n",
      "Epoch: 0019 loss_train: 1.7712 acc_train: 0.2000 loss_val: 1.8005 acc_val: 0.1567 time: 0.0292s\n",
      "Epoch: 0020 loss_train: 1.8129 acc_train: 0.2000 loss_val: 1.7911 acc_val: 0.1567 time: 0.0344s\n",
      "Epoch: 0021 loss_train: 1.7679 acc_train: 0.2143 loss_val: 1.7815 acc_val: 0.1567 time: 0.0339s\n",
      "Epoch: 0022 loss_train: 1.7643 acc_train: 0.2429 loss_val: 1.7722 acc_val: 0.1567 time: 0.0301s\n",
      "Epoch: 0023 loss_train: 1.7512 acc_train: 0.2857 loss_val: 1.7630 acc_val: 0.3267 time: 0.0311s\n",
      "Epoch: 0024 loss_train: 1.7382 acc_train: 0.2929 loss_val: 1.7538 acc_val: 0.4233 time: 0.0373s\n",
      "Epoch: 0025 loss_train: 1.7285 acc_train: 0.3500 loss_val: 1.7446 acc_val: 0.3600 time: 0.0342s\n",
      "Epoch: 0026 loss_train: 1.7141 acc_train: 0.3714 loss_val: 1.7356 acc_val: 0.3467 time: 0.0270s\n",
      "Epoch: 0027 loss_train: 1.7138 acc_train: 0.3286 loss_val: 1.7265 acc_val: 0.3467 time: 0.0347s\n",
      "Epoch: 0028 loss_train: 1.7112 acc_train: 0.3214 loss_val: 1.7176 acc_val: 0.3467 time: 0.0347s\n",
      "Epoch: 0029 loss_train: 1.6910 acc_train: 0.3000 loss_val: 1.7087 acc_val: 0.3500 time: 0.0355s\n",
      "Epoch: 0030 loss_train: 1.6963 acc_train: 0.2929 loss_val: 1.6999 acc_val: 0.3500 time: 0.0272s\n",
      "Epoch: 0031 loss_train: 1.7059 acc_train: 0.3000 loss_val: 1.6911 acc_val: 0.3500 time: 0.0233s\n",
      "Epoch: 0032 loss_train: 1.6526 acc_train: 0.3000 loss_val: 1.6823 acc_val: 0.3500 time: 0.0252s\n",
      "Epoch: 0033 loss_train: 1.6731 acc_train: 0.3214 loss_val: 1.6736 acc_val: 0.3500 time: 0.0275s\n",
      "Epoch: 0034 loss_train: 1.6362 acc_train: 0.3357 loss_val: 1.6649 acc_val: 0.3500 time: 0.0344s\n",
      "Epoch: 0035 loss_train: 1.6173 acc_train: 0.3071 loss_val: 1.6562 acc_val: 0.3467 time: 0.0353s\n",
      "Epoch: 0036 loss_train: 1.6480 acc_train: 0.3071 loss_val: 1.6474 acc_val: 0.3467 time: 0.0335s\n",
      "Epoch: 0037 loss_train: 1.6154 acc_train: 0.3286 loss_val: 1.6386 acc_val: 0.3467 time: 0.0365s\n",
      "Epoch: 0038 loss_train: 1.6052 acc_train: 0.3143 loss_val: 1.6297 acc_val: 0.3467 time: 0.0302s\n",
      "Epoch: 0039 loss_train: 1.5820 acc_train: 0.3571 loss_val: 1.6206 acc_val: 0.3533 time: 0.0283s\n",
      "Epoch: 0040 loss_train: 1.5736 acc_train: 0.3500 loss_val: 1.6111 acc_val: 0.3567 time: 0.0246s\n",
      "Epoch: 0041 loss_train: 1.5550 acc_train: 0.3857 loss_val: 1.6014 acc_val: 0.3633 time: 0.0272s\n",
      "Epoch: 0042 loss_train: 1.5403 acc_train: 0.3571 loss_val: 1.5914 acc_val: 0.3733 time: 0.0246s\n",
      "Epoch: 0043 loss_train: 1.5382 acc_train: 0.4286 loss_val: 1.5812 acc_val: 0.3800 time: 0.0249s\n",
      "Epoch: 0044 loss_train: 1.5058 acc_train: 0.4000 loss_val: 1.5707 acc_val: 0.4000 time: 0.0276s\n",
      "Epoch: 0045 loss_train: 1.4807 acc_train: 0.4786 loss_val: 1.5599 acc_val: 0.4200 time: 0.0277s\n",
      "Epoch: 0046 loss_train: 1.4919 acc_train: 0.4214 loss_val: 1.5488 acc_val: 0.4233 time: 0.0292s\n",
      "Epoch: 0047 loss_train: 1.4762 acc_train: 0.4071 loss_val: 1.5377 acc_val: 0.4367 time: 0.0241s\n",
      "Epoch: 0048 loss_train: 1.4392 acc_train: 0.4429 loss_val: 1.5265 acc_val: 0.4533 time: 0.0236s\n",
      "Epoch: 0049 loss_train: 1.4407 acc_train: 0.5143 loss_val: 1.5147 acc_val: 0.4767 time: 0.0295s\n",
      "Epoch: 0050 loss_train: 1.4206 acc_train: 0.5429 loss_val: 1.5028 acc_val: 0.4867 time: 0.0277s\n",
      "Epoch: 0051 loss_train: 1.4129 acc_train: 0.5571 loss_val: 1.4910 acc_val: 0.5067 time: 0.0262s\n",
      "Epoch: 0052 loss_train: 1.3988 acc_train: 0.5643 loss_val: 1.4792 acc_val: 0.5200 time: 0.0246s\n",
      "Epoch: 0053 loss_train: 1.3736 acc_train: 0.5429 loss_val: 1.4674 acc_val: 0.5467 time: 0.0246s\n",
      "Epoch: 0054 loss_train: 1.3552 acc_train: 0.6571 loss_val: 1.4549 acc_val: 0.5600 time: 0.0246s\n",
      "Epoch: 0055 loss_train: 1.3416 acc_train: 0.5857 loss_val: 1.4418 acc_val: 0.5700 time: 0.0248s\n",
      "Epoch: 0056 loss_train: 1.2973 acc_train: 0.6214 loss_val: 1.4285 acc_val: 0.5767 time: 0.0247s\n",
      "Epoch: 0057 loss_train: 1.3197 acc_train: 0.6429 loss_val: 1.4151 acc_val: 0.5800 time: 0.0263s\n",
      "Epoch: 0058 loss_train: 1.2915 acc_train: 0.6214 loss_val: 1.4015 acc_val: 0.5867 time: 0.0268s\n",
      "Epoch: 0059 loss_train: 1.2773 acc_train: 0.6786 loss_val: 1.3878 acc_val: 0.5967 time: 0.0237s\n",
      "Epoch: 0060 loss_train: 1.2337 acc_train: 0.6786 loss_val: 1.3747 acc_val: 0.6000 time: 0.0237s\n",
      "Epoch: 0061 loss_train: 1.2550 acc_train: 0.6714 loss_val: 1.3613 acc_val: 0.6100 time: 0.0256s\n",
      "Epoch: 0062 loss_train: 1.2482 acc_train: 0.6643 loss_val: 1.3478 acc_val: 0.6133 time: 0.0276s\n",
      "Epoch: 0063 loss_train: 1.2232 acc_train: 0.6571 loss_val: 1.3348 acc_val: 0.6233 time: 0.0259s\n",
      "Epoch: 0064 loss_train: 1.2064 acc_train: 0.7286 loss_val: 1.3218 acc_val: 0.6333 time: 0.0275s\n",
      "Epoch: 0065 loss_train: 1.1700 acc_train: 0.7286 loss_val: 1.3091 acc_val: 0.6400 time: 0.0262s\n",
      "Epoch: 0066 loss_train: 1.1615 acc_train: 0.7500 loss_val: 1.2962 acc_val: 0.6500 time: 0.0248s\n",
      "Epoch: 0067 loss_train: 1.1258 acc_train: 0.7143 loss_val: 1.2834 acc_val: 0.6633 time: 0.0260s\n",
      "Epoch: 0068 loss_train: 1.1324 acc_train: 0.7143 loss_val: 1.2703 acc_val: 0.6733 time: 0.0250s\n",
      "Epoch: 0069 loss_train: 1.1026 acc_train: 0.7214 loss_val: 1.2574 acc_val: 0.6767 time: 0.0281s\n",
      "Epoch: 0070 loss_train: 1.0779 acc_train: 0.7571 loss_val: 1.2445 acc_val: 0.6867 time: 0.0257s\n",
      "Epoch: 0071 loss_train: 1.0676 acc_train: 0.7429 loss_val: 1.2321 acc_val: 0.6967 time: 0.0256s\n",
      "Epoch: 0072 loss_train: 1.0665 acc_train: 0.7571 loss_val: 1.2195 acc_val: 0.6967 time: 0.0252s\n",
      "Epoch: 0073 loss_train: 1.0472 acc_train: 0.7286 loss_val: 1.2074 acc_val: 0.6933 time: 0.0259s\n",
      "Epoch: 0074 loss_train: 1.0385 acc_train: 0.7500 loss_val: 1.1949 acc_val: 0.7000 time: 0.0263s\n",
      "Epoch: 0075 loss_train: 1.0043 acc_train: 0.7571 loss_val: 1.1822 acc_val: 0.7067 time: 0.0247s\n",
      "Epoch: 0076 loss_train: 0.9989 acc_train: 0.7571 loss_val: 1.1691 acc_val: 0.7167 time: 0.0253s\n",
      "Epoch: 0077 loss_train: 1.0002 acc_train: 0.7500 loss_val: 1.1564 acc_val: 0.7300 time: 0.0259s\n",
      "Epoch: 0078 loss_train: 0.9324 acc_train: 0.7571 loss_val: 1.1446 acc_val: 0.7467 time: 0.0266s\n",
      "Epoch: 0079 loss_train: 0.9729 acc_train: 0.7929 loss_val: 1.1337 acc_val: 0.7533 time: 0.0248s\n",
      "Epoch: 0080 loss_train: 0.9391 acc_train: 0.7786 loss_val: 1.1237 acc_val: 0.7633 time: 0.0261s\n",
      "Epoch: 0081 loss_train: 0.9640 acc_train: 0.7571 loss_val: 1.1143 acc_val: 0.7700 time: 0.0271s\n",
      "Epoch: 0082 loss_train: 0.9464 acc_train: 0.7714 loss_val: 1.1052 acc_val: 0.7800 time: 0.0256s\n",
      "Epoch: 0083 loss_train: 0.9222 acc_train: 0.8143 loss_val: 1.0963 acc_val: 0.7900 time: 0.0223s\n",
      "Epoch: 0084 loss_train: 0.9366 acc_train: 0.7929 loss_val: 1.0868 acc_val: 0.7933 time: 0.0257s\n",
      "Epoch: 0085 loss_train: 0.8984 acc_train: 0.8500 loss_val: 1.0768 acc_val: 0.7967 time: 0.0256s\n",
      "Epoch: 0086 loss_train: 0.8575 acc_train: 0.8143 loss_val: 1.0656 acc_val: 0.7967 time: 0.0250s\n",
      "Epoch: 0087 loss_train: 0.9432 acc_train: 0.7857 loss_val: 1.0551 acc_val: 0.7967 time: 0.0241s\n",
      "Epoch: 0088 loss_train: 0.8449 acc_train: 0.8429 loss_val: 1.0453 acc_val: 0.7933 time: 0.0266s\n",
      "Epoch: 0089 loss_train: 0.8266 acc_train: 0.8071 loss_val: 1.0358 acc_val: 0.7900 time: 0.0259s\n",
      "Epoch: 0090 loss_train: 0.8361 acc_train: 0.8286 loss_val: 1.0267 acc_val: 0.7933 time: 0.0240s\n",
      "Epoch: 0091 loss_train: 0.8523 acc_train: 0.8357 loss_val: 1.0177 acc_val: 0.7967 time: 0.0233s\n",
      "Epoch: 0092 loss_train: 0.8311 acc_train: 0.8214 loss_val: 1.0096 acc_val: 0.8000 time: 0.0250s\n",
      "Epoch: 0093 loss_train: 0.7794 acc_train: 0.8071 loss_val: 1.0021 acc_val: 0.8067 time: 0.0317s\n",
      "Epoch: 0094 loss_train: 0.8028 acc_train: 0.8214 loss_val: 0.9958 acc_val: 0.8067 time: 0.0306s\n",
      "Epoch: 0095 loss_train: 0.8148 acc_train: 0.8214 loss_val: 0.9901 acc_val: 0.8067 time: 0.0276s\n",
      "Epoch: 0096 loss_train: 0.8021 acc_train: 0.8143 loss_val: 0.9838 acc_val: 0.8067 time: 0.0351s\n",
      "Epoch: 0097 loss_train: 0.7652 acc_train: 0.8571 loss_val: 0.9785 acc_val: 0.8033 time: 0.0340s\n",
      "Epoch: 0098 loss_train: 0.7419 acc_train: 0.8714 loss_val: 0.9725 acc_val: 0.8067 time: 0.0303s\n",
      "Epoch: 0099 loss_train: 0.7393 acc_train: 0.8786 loss_val: 0.9662 acc_val: 0.8067 time: 0.0338s\n",
      "Epoch: 0100 loss_train: 0.7327 acc_train: 0.8714 loss_val: 0.9585 acc_val: 0.8067 time: 0.0331s\n",
      "Epoch: 0101 loss_train: 0.7725 acc_train: 0.8214 loss_val: 0.9502 acc_val: 0.8100 time: 0.0331s\n",
      "Epoch: 0102 loss_train: 0.7524 acc_train: 0.8714 loss_val: 0.9431 acc_val: 0.8100 time: 0.0391s\n",
      "Epoch: 0103 loss_train: 0.6957 acc_train: 0.8929 loss_val: 0.9351 acc_val: 0.8133 time: 0.0349s\n",
      "Epoch: 0104 loss_train: 0.6853 acc_train: 0.8643 loss_val: 0.9277 acc_val: 0.8133 time: 0.0292s\n",
      "Epoch: 0105 loss_train: 0.7007 acc_train: 0.8571 loss_val: 0.9205 acc_val: 0.8133 time: 0.0259s\n",
      "Epoch: 0106 loss_train: 0.7379 acc_train: 0.8357 loss_val: 0.9145 acc_val: 0.8100 time: 0.0252s\n",
      "Epoch: 0107 loss_train: 0.6996 acc_train: 0.8286 loss_val: 0.9091 acc_val: 0.8100 time: 0.0238s\n",
      "Epoch: 0108 loss_train: 0.7029 acc_train: 0.8643 loss_val: 0.9038 acc_val: 0.8133 time: 0.0275s\n",
      "Epoch: 0109 loss_train: 0.6755 acc_train: 0.8643 loss_val: 0.8985 acc_val: 0.8133 time: 0.0343s\n",
      "Epoch: 0110 loss_train: 0.7165 acc_train: 0.8143 loss_val: 0.8946 acc_val: 0.8133 time: 0.0376s\n",
      "Epoch: 0111 loss_train: 0.6430 acc_train: 0.8714 loss_val: 0.8912 acc_val: 0.8133 time: 0.0278s\n",
      "Epoch: 0112 loss_train: 0.6440 acc_train: 0.8857 loss_val: 0.8884 acc_val: 0.8167 time: 0.0275s\n",
      "Epoch: 0113 loss_train: 0.6862 acc_train: 0.8714 loss_val: 0.8851 acc_val: 0.8233 time: 0.0272s\n",
      "Epoch: 0114 loss_train: 0.6350 acc_train: 0.9214 loss_val: 0.8812 acc_val: 0.8233 time: 0.0301s\n",
      "Epoch: 0115 loss_train: 0.6777 acc_train: 0.8714 loss_val: 0.8765 acc_val: 0.8233 time: 0.0258s\n",
      "Epoch: 0116 loss_train: 0.6320 acc_train: 0.8571 loss_val: 0.8717 acc_val: 0.8233 time: 0.0322s\n",
      "Epoch: 0117 loss_train: 0.6545 acc_train: 0.8643 loss_val: 0.8660 acc_val: 0.8233 time: 0.0254s\n",
      "Epoch: 0118 loss_train: 0.5880 acc_train: 0.9000 loss_val: 0.8606 acc_val: 0.8267 time: 0.0249s\n",
      "Epoch: 0119 loss_train: 0.6692 acc_train: 0.8857 loss_val: 0.8555 acc_val: 0.8233 time: 0.0263s\n",
      "Epoch: 0120 loss_train: 0.6477 acc_train: 0.8786 loss_val: 0.8507 acc_val: 0.8267 time: 0.0264s\n",
      "Epoch: 0121 loss_train: 0.6632 acc_train: 0.8929 loss_val: 0.8465 acc_val: 0.8233 time: 0.0289s\n",
      "Epoch: 0122 loss_train: 0.5753 acc_train: 0.9071 loss_val: 0.8427 acc_val: 0.8267 time: 0.0315s\n",
      "Epoch: 0123 loss_train: 0.5902 acc_train: 0.9143 loss_val: 0.8389 acc_val: 0.8267 time: 0.0254s\n",
      "Epoch: 0124 loss_train: 0.6052 acc_train: 0.9143 loss_val: 0.8352 acc_val: 0.8267 time: 0.0266s\n",
      "Epoch: 0125 loss_train: 0.5635 acc_train: 0.9071 loss_val: 0.8324 acc_val: 0.8233 time: 0.0333s\n",
      "Epoch: 0126 loss_train: 0.6296 acc_train: 0.9000 loss_val: 0.8299 acc_val: 0.8233 time: 0.0307s\n",
      "Epoch: 0127 loss_train: 0.5872 acc_train: 0.8929 loss_val: 0.8283 acc_val: 0.8133 time: 0.0249s\n",
      "Epoch: 0128 loss_train: 0.5825 acc_train: 0.8929 loss_val: 0.8262 acc_val: 0.8133 time: 0.0249s\n",
      "Epoch: 0129 loss_train: 0.6073 acc_train: 0.8643 loss_val: 0.8241 acc_val: 0.8100 time: 0.0257s\n",
      "Epoch: 0130 loss_train: 0.6150 acc_train: 0.8714 loss_val: 0.8214 acc_val: 0.8233 time: 0.0258s\n",
      "Epoch: 0131 loss_train: 0.5503 acc_train: 0.8929 loss_val: 0.8174 acc_val: 0.8233 time: 0.0276s\n",
      "Epoch: 0132 loss_train: 0.5491 acc_train: 0.9214 loss_val: 0.8124 acc_val: 0.8267 time: 0.0296s\n",
      "Epoch: 0133 loss_train: 0.5807 acc_train: 0.8929 loss_val: 0.8069 acc_val: 0.8300 time: 0.0269s\n",
      "Epoch: 0134 loss_train: 0.6075 acc_train: 0.8786 loss_val: 0.8022 acc_val: 0.8300 time: 0.0260s\n",
      "Epoch: 0135 loss_train: 0.5667 acc_train: 0.8929 loss_val: 0.7981 acc_val: 0.8300 time: 0.0240s\n",
      "Epoch: 0136 loss_train: 0.5228 acc_train: 0.8786 loss_val: 0.7949 acc_val: 0.8300 time: 0.0250s\n",
      "Epoch: 0137 loss_train: 0.5208 acc_train: 0.9000 loss_val: 0.7915 acc_val: 0.8300 time: 0.0255s\n",
      "Epoch: 0138 loss_train: 0.5638 acc_train: 0.8786 loss_val: 0.7887 acc_val: 0.8300 time: 0.0283s\n",
      "Epoch: 0139 loss_train: 0.5966 acc_train: 0.9071 loss_val: 0.7858 acc_val: 0.8300 time: 0.0301s\n",
      "Epoch: 0140 loss_train: 0.5410 acc_train: 0.9000 loss_val: 0.7832 acc_val: 0.8233 time: 0.0275s\n",
      "Epoch: 0141 loss_train: 0.5003 acc_train: 0.9286 loss_val: 0.7816 acc_val: 0.8300 time: 0.0277s\n",
      "Epoch: 0142 loss_train: 0.5134 acc_train: 0.9214 loss_val: 0.7801 acc_val: 0.8333 time: 0.0315s\n",
      "Epoch: 0143 loss_train: 0.5635 acc_train: 0.8714 loss_val: 0.7792 acc_val: 0.8333 time: 0.0353s\n",
      "Epoch: 0144 loss_train: 0.6059 acc_train: 0.8929 loss_val: 0.7786 acc_val: 0.8400 time: 0.0327s\n",
      "Epoch: 0145 loss_train: 0.5495 acc_train: 0.9286 loss_val: 0.7763 acc_val: 0.8433 time: 0.0336s\n",
      "Epoch: 0146 loss_train: 0.5624 acc_train: 0.9214 loss_val: 0.7733 acc_val: 0.8400 time: 0.0443s\n",
      "Epoch: 0147 loss_train: 0.5113 acc_train: 0.8857 loss_val: 0.7701 acc_val: 0.8367 time: 0.0333s\n",
      "Epoch: 0148 loss_train: 0.5368 acc_train: 0.9500 loss_val: 0.7669 acc_val: 0.8367 time: 0.0322s\n",
      "Epoch: 0149 loss_train: 0.4913 acc_train: 0.9071 loss_val: 0.7639 acc_val: 0.8333 time: 0.0293s\n",
      "Epoch: 0150 loss_train: 0.5109 acc_train: 0.9143 loss_val: 0.7616 acc_val: 0.8300 time: 0.0255s\n",
      "Epoch: 0151 loss_train: 0.4959 acc_train: 0.9143 loss_val: 0.7590 acc_val: 0.8267 time: 0.0255s\n",
      "Epoch: 0152 loss_train: 0.5336 acc_train: 0.8786 loss_val: 0.7571 acc_val: 0.8233 time: 0.0249s\n",
      "Epoch: 0153 loss_train: 0.4818 acc_train: 0.9000 loss_val: 0.7555 acc_val: 0.8233 time: 0.0241s\n",
      "Epoch: 0154 loss_train: 0.4891 acc_train: 0.9071 loss_val: 0.7539 acc_val: 0.8233 time: 0.0275s\n",
      "Epoch: 0155 loss_train: 0.5153 acc_train: 0.9143 loss_val: 0.7525 acc_val: 0.8233 time: 0.0314s\n",
      "Epoch: 0156 loss_train: 0.4911 acc_train: 0.9143 loss_val: 0.7509 acc_val: 0.8267 time: 0.0344s\n",
      "Epoch: 0157 loss_train: 0.5082 acc_train: 0.9143 loss_val: 0.7497 acc_val: 0.8367 time: 0.0288s\n",
      "Epoch: 0158 loss_train: 0.4953 acc_train: 0.9214 loss_val: 0.7476 acc_val: 0.8367 time: 0.0306s\n",
      "Epoch: 0159 loss_train: 0.5047 acc_train: 0.9357 loss_val: 0.7450 acc_val: 0.8367 time: 0.0384s\n",
      "Epoch: 0160 loss_train: 0.5190 acc_train: 0.9071 loss_val: 0.7434 acc_val: 0.8367 time: 0.0361s\n",
      "Epoch: 0161 loss_train: 0.5095 acc_train: 0.9071 loss_val: 0.7420 acc_val: 0.8367 time: 0.0313s\n",
      "Epoch: 0162 loss_train: 0.4792 acc_train: 0.9214 loss_val: 0.7402 acc_val: 0.8267 time: 0.0264s\n",
      "Epoch: 0163 loss_train: 0.5131 acc_train: 0.8857 loss_val: 0.7375 acc_val: 0.8400 time: 0.0238s\n",
      "Epoch: 0164 loss_train: 0.4860 acc_train: 0.9357 loss_val: 0.7344 acc_val: 0.8333 time: 0.0242s\n",
      "Epoch: 0165 loss_train: 0.4765 acc_train: 0.9143 loss_val: 0.7318 acc_val: 0.8333 time: 0.0256s\n",
      "Epoch: 0166 loss_train: 0.4384 acc_train: 0.9500 loss_val: 0.7304 acc_val: 0.8333 time: 0.0232s\n",
      "Epoch: 0167 loss_train: 0.4597 acc_train: 0.9357 loss_val: 0.7288 acc_val: 0.8300 time: 0.0243s\n",
      "Epoch: 0168 loss_train: 0.4679 acc_train: 0.9143 loss_val: 0.7276 acc_val: 0.8200 time: 0.0261s\n",
      "Epoch: 0169 loss_train: 0.4424 acc_train: 0.9357 loss_val: 0.7261 acc_val: 0.8200 time: 0.0299s\n",
      "Epoch: 0170 loss_train: 0.4646 acc_train: 0.9214 loss_val: 0.7244 acc_val: 0.8233 time: 0.0282s\n",
      "Epoch: 0171 loss_train: 0.4501 acc_train: 0.9143 loss_val: 0.7225 acc_val: 0.8333 time: 0.0272s\n",
      "Epoch: 0172 loss_train: 0.4412 acc_train: 0.9143 loss_val: 0.7212 acc_val: 0.8300 time: 0.0248s\n",
      "Epoch: 0173 loss_train: 0.4356 acc_train: 0.9500 loss_val: 0.7200 acc_val: 0.8267 time: 0.0251s\n",
      "Epoch: 0174 loss_train: 0.4429 acc_train: 0.9429 loss_val: 0.7194 acc_val: 0.8333 time: 0.0279s\n",
      "Epoch: 0175 loss_train: 0.4518 acc_train: 0.9286 loss_val: 0.7187 acc_val: 0.8367 time: 0.0292s\n",
      "Epoch: 0176 loss_train: 0.4988 acc_train: 0.9071 loss_val: 0.7186 acc_val: 0.8267 time: 0.0260s\n",
      "Epoch: 0177 loss_train: 0.4625 acc_train: 0.9500 loss_val: 0.7179 acc_val: 0.8267 time: 0.0281s\n",
      "Epoch: 0178 loss_train: 0.4668 acc_train: 0.9143 loss_val: 0.7172 acc_val: 0.8267 time: 0.0257s\n",
      "Epoch: 0179 loss_train: 0.4594 acc_train: 0.9214 loss_val: 0.7157 acc_val: 0.8300 time: 0.0239s\n",
      "Epoch: 0180 loss_train: 0.4254 acc_train: 0.9286 loss_val: 0.7142 acc_val: 0.8267 time: 0.0250s\n",
      "Epoch: 0181 loss_train: 0.4353 acc_train: 0.9500 loss_val: 0.7131 acc_val: 0.8267 time: 0.0246s\n",
      "Epoch: 0182 loss_train: 0.4541 acc_train: 0.9143 loss_val: 0.7119 acc_val: 0.8233 time: 0.0252s\n",
      "Epoch: 0183 loss_train: 0.4591 acc_train: 0.9214 loss_val: 0.7118 acc_val: 0.8200 time: 0.0233s\n",
      "Epoch: 0184 loss_train: 0.3860 acc_train: 0.9214 loss_val: 0.7118 acc_val: 0.8100 time: 0.0252s\n",
      "Epoch: 0185 loss_train: 0.4296 acc_train: 0.9500 loss_val: 0.7104 acc_val: 0.8100 time: 0.0254s\n",
      "Epoch: 0186 loss_train: 0.4105 acc_train: 0.9643 loss_val: 0.7085 acc_val: 0.8167 time: 0.0258s\n",
      "Epoch: 0187 loss_train: 0.4858 acc_train: 0.9143 loss_val: 0.7058 acc_val: 0.8233 time: 0.0289s\n",
      "Epoch: 0188 loss_train: 0.4710 acc_train: 0.8929 loss_val: 0.7041 acc_val: 0.8233 time: 0.0227s\n",
      "Epoch: 0189 loss_train: 0.4267 acc_train: 0.9286 loss_val: 0.7029 acc_val: 0.8300 time: 0.0228s\n",
      "Epoch: 0190 loss_train: 0.4514 acc_train: 0.9429 loss_val: 0.7015 acc_val: 0.8233 time: 0.0244s\n",
      "Epoch: 0191 loss_train: 0.4094 acc_train: 0.9214 loss_val: 0.7000 acc_val: 0.8267 time: 0.0275s\n",
      "Epoch: 0192 loss_train: 0.4389 acc_train: 0.9143 loss_val: 0.6989 acc_val: 0.8267 time: 0.0343s\n",
      "Epoch: 0193 loss_train: 0.4283 acc_train: 0.9500 loss_val: 0.6980 acc_val: 0.8267 time: 0.0311s\n",
      "Epoch: 0194 loss_train: 0.4218 acc_train: 0.9071 loss_val: 0.6985 acc_val: 0.8233 time: 0.0273s\n",
      "Epoch: 0195 loss_train: 0.3842 acc_train: 0.9571 loss_val: 0.6988 acc_val: 0.8300 time: 0.0249s\n",
      "Epoch: 0196 loss_train: 0.4130 acc_train: 0.9214 loss_val: 0.6993 acc_val: 0.8233 time: 0.0286s\n",
      "Epoch: 0197 loss_train: 0.4693 acc_train: 0.9214 loss_val: 0.6994 acc_val: 0.8167 time: 0.0372s\n",
      "Epoch: 0198 loss_train: 0.4058 acc_train: 0.9286 loss_val: 0.6992 acc_val: 0.8133 time: 0.0291s\n",
      "Epoch: 0199 loss_train: 0.3921 acc_train: 0.9214 loss_val: 0.6993 acc_val: 0.8133 time: 0.0289s\n",
      "Epoch: 0200 loss_train: 0.3832 acc_train: 0.9500 loss_val: 0.6983 acc_val: 0.8100 time: 0.0241s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 6.9907s\n",
      "Test set results: loss= 0.7147 accuracy= 0.8380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a1f56-ab81-4aff-85d5-148c6f2a303f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Optimization-Python] *",
   "language": "python",
   "name": "conda-env-Optimization-Python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
